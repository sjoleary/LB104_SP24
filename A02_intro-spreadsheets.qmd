# Introduction to Data Management using spreadsheets {#sec-introduction-to-GoogleSheets}

```{r}
#| label: set-up
#| include: false

# set options
knitr::opts_chunk$set(tidy = FALSE)
options(htmltools.dir.version = FALSE)

```

Learning Objectives

**After completing this activity you should be able to**

-   implement good data entry practices to organize data in spreadsheets to create data sets for straightforward analysis using computational tools such as `R`.
-   arrange variables and observations in a spreadsheet in a way conducive to be both human and machine readable for further processing.
-   identify, avoid, and address common formatting mistakes.
-   perform basic quality control and data manipulation in spreadsheets to minimize incorrect data entry identify errors.
-   effectively export data from spreadsheet programs as text files for use in computational tools such as `R`.

## Spreadsheets for data entry and management

The foundation of any research project is good data organization. This not only includes your actual data points (observations) but also things like keeping track of specimen and other samples along with all the **metadata**[^a02_intro-spreadsheets-1]. Additionally, you should keep good records of how the data was produced (your methods). Thinking through ahead of time what measurements are important, i.e. what data you will record and how you will store your data is really important to make sure you are keeping track of the entire process. Good data management and clean data sets will make sharing and analyzing data a lot more straightforward.

[^a02_intro-spreadsheets-1]: Metadata is data about your data. It helps describe, categorize, organize, and provide context to the main content or primary data it is associated with. It's commonly used to provide additional information that helps users, systems, or processes understand and interpret the main data.

While we are going to use `R` to wrangle, manipulate, analyze, and visualize data in `R` for a more efficient and reproducible approach compared to what can be using spreadsheets, spreadsheets are the better tool for data entry, data management/organization, and simple **quality control** (QC) and **quality assurance** (QA). Thinking ahead to how you want to organize and format your data in spreadsheets will prevent a lot of extra work and headaches down the line, especially when we plan ahead as to how we should organize it to make it more efficient to use with command-line computational tools such as `R`.

So, before we dive deep into manipulating data with `R`, we'll take a small step back and think through a few fundamental rules for managing data in spreadsheets before learning how to do these and more advanced data wrangling and manipulation using `R`.

While you can do some statistics and plotting using spreadsheet programs we will not be learning how to do this in this class[^a02_intro-spreadsheets-2]. Data analysis in spreadsheets requires a lot of manual work and generally any time you want to change one parameter or if you have to update your spreadsheet with new entries or you need to apply the same analysis to another data set you end up having to redo everything by hand. The more things you do by hand, the more likely you are to make a mistake. Even if you do apply some sophisticated coding in spreadsheets and/or use it for analysis or plotting[^a02_intro-spreadsheets-3] it is very difficult to track the exact steps or document them in a way that makes it fully reproducible for another person.

[^a02_intro-spreadsheets-2]: If you are interested, you can also take courses to learn how to leverage spreadsheets efficiently for a wide rang of tasks in the Computer Science department, depending on what quantitative reasoning course you take or if you get involved in a research lab, you will be exposed to various other programs for data analysis and a lot of the key principles you will learn this semester will still apply even if they are implemented a bit differently.

[^a02_intro-spreadsheets-3]: It can be helpful to know the fundamentals for simple plots in spreadsheets for quick and dirty plotting to get a quick look at your data to get an idea of what it looks like and spot potential mistakes during data entry without having to export data and fire up `R` or a another command-line program.

## Best practices for formatting data in spreadsheets {#sec-best-practices-for-formatting-data-in-spreadsheets}

### Format your data set for the tool you will use to analyze it with {#sec-format-your-data-set-for-the-tool-you-will-use-to-analyze-it-with}

Our brains don't work the same way as computers. Your spreadsheet is not a lab notebook and while a layout where there are notes in the margin, context of the experiment, or a specific layout of data might be something that you can interpret, it will more difficult for another person to follow your through process and understand your records/notes. Another person might have the opportunity to ask follow up questions and get the clarifications they need, but a computer cannot.

Occasionally, you might use a spreadsheet instead of a lab notebook where it is a way of keeping track of an experiment and various people interacting with samples, completing different steps etc. However, if you are using a spreadsheet for data entry and management, then you want to ensure that you have set up your spreadsheet in a way where a computer is going to be able to correctly interpret it as intended. This means that we need to think through how we want to set up spreadsheets. There are generally a few different ways you can set things up and some of them will limit how easy it is for you and/or a future collaborator to work with it down the line[^a02_intro-spreadsheets-4].

[^a02_intro-spreadsheets-4]: The optimum software/interface for data input and layout/formatting may differ depending on your intended analysis, so keep in mind *how* you want to be able to analyze your data and whether that will require specific formats. In general, try to pick a format that will give you the advantage of being able to easily convert it between different formats - which is something we will learn to do with `R` specifically in the `tidyverse` which centers on a specific concept of what makes data tidy.

This semester you will be gathering data for three research projects, take the time to think through why we are organizing data in a certain way and how the principles we discuss today apply.

### Never touch your raw data {#sec-never-touch-your-raw-data}

**Raw data** is the original, unaltered data that is collected or generated before any manipulation, transformation, or analysis takes place. It's the most fundamental form of data, directly obtained from observations, measurements, or data sources. Raw data is often in its most unstructured and basic state, and it serves as the foundation for subsequent data processing and analysis.

In the biological and environmental sciences typical sources include direct observations made in experiments, field studies, or natural phenomena or measurements from sensors or other instruments measuring physical and chemical parameters such as temperature, GPS coordinate, pH etc.

::: {.callout-warning title="Pay Attention"}
**Never touch your raw data. Always keep a copy of your raw data that you never modify directly**.

For any kind of data related work it is important that you preserve the original, unaltered version of your data when conducting data analysis. Avoid making changes directly to the original data files or data set. Instead, You should work with copies of the data or use a structured workflow that ensures the integrity and reproducibility of your analysis.
:::

Keeping your raw data as a separate file that is never altered is important for

-   **Data Integrity**: Modifying the raw data directly could lead to unintended changes or loss of information. By keeping the raw data untouched, you ensure that you have a reliable source of truth to refer back to if needed.
-   **Reproducibility**: If you or others need to replicate your analysis in the future, having access to the exact original data is crucial. Changes made to the raw data could make it difficult or impossible to reproduce your results accurately.
-   **Error Prevention**: Working with copies of the raw data minimizes the risk of accidental changes or mistakes that could affect your analysis. If errors occur, you can always go back to the untouched raw data.
-   **Data Auditing**: In some cases, you might need to show the authenticity and accuracy of your data. Keeping the original data untouched allows you to demonstrate the reliability of your work.
-   **Multiple Analyses**: If you're working on different analyses, projects, or collaborations using the same data, maintaining the integrity of the raw data enables consistency across these efforts.

Best practices of maintaining the integrity of your raw data include

-   **Make copies**: Always work with copies of the original data files. This ensures that any changes you make are isolated from the raw data.
-   **Implement a structured workflow with detailed documentation**: Develop a repeatable workflow that includes data cleaning, transformation, and analysis steps. Maintain a clear and detailed documentation about the steps you've taken, the rationale behind each decisions, and any changes you've made to the data to ensure transparency.
-   **Create backups and use version control**: Regularly back up your data, including both the raw data and any processed versions, to prevent data loss. If you are taking data down on physical data sheets and then entering that into spreadsheets make sure you hang onto the physical data sheets. Use version control systems like Git to track changes to your data set. GoogleSheets also includes version control.

### Keep track of your formatting steps {#sec-keep-track-of-your-formatting-steps}

By working with copies and following a structured workflow, you can ensure the accuracy, reproducibility, and integrity of your data analysis work. While you should not modify the raw data directly, it's also important to apply necessary data preprocessing steps (like cleaning and transforming) as part of your analysis process. This means that you should do two things

1.  Any time you need to process or analyze your data make a copy instead of operating directly in your raw data[^a02_intro-spreadsheets-5] and then create a new file with your cleaned or analyzed data.
2.  Keep track of the exact steps you took to clean or analyze your data[^a02_intro-spreadsheets-6]; this is just as important as keeping a detailed record of the steps you took in an experiment; data analysis is an important part of your methods and should be documented as such. Good practice would be to keep a plain text file or similar in the same folder as your data set where you record any steps you take.

[^a02_intro-spreadsheets-5]: You will notice this semester that we will read in data from text files for analysis and while we can do a lot of manipulation and calculations this does not change our raw data file.

[^a02_intro-spreadsheets-6]: The second advantage of command-line programs like `R` is that because you are using a series of commands to wrangle and analyze your data your are automatically creating a very detailed, reproducible record of your your workflow

### Put variables in columns and observations in rows {#sec-put-variables-in-columns-and-observations-in-rows}

Observations and variables are two fundamental concepts that describe different aspects of data.

::: {.callout-tip title="Consider this"}
Compare and contrast what an observation is compared to a variable. Consider how the terms metric and units fit in.
:::

::: {.callout-note title="Answer" collapse="true"}
An **observation** is a single data point or unit within a data set, while a **variable** is a characteristic that is being measured or observed for each of those data points.

Together, observations and variables make up the structure of a data set, where each observation has values for the various variables being measured.

Variables are attributes being measured, metrics are calculated values that summarize those attributes. Units of measurements tells us *how* something is being measured depending on the scale we are using to quantify the attributes of a variable. For example, you could measure the length using kilometers or miles.
:::

Here is the key rule for structuring data in spreadsheet in a tidy way:

::: {.callout-warning title="Pay Attention"}
-   **Every variable gets its own column**
-   **Every observation gets its own rule**
-   **Do not combine multiple pieces of information in one cell.**

These principles follow Hadley Wickhams definition of "tidy data" which is an underlying principle for a lot the `R` code you will apply this semester.
:::

### Export data as text-based formats {#sec-export-data-as-text-based-formats}

While it is a lot easier to enter and look at data in a spreadsheet you should always export your raw and cleaned data set as a text-based format such as CSV, TSV, JSON, XML, etc.).

This offers several advantages compared to proprietary binary formats[^a02_intro-spreadsheets-7]:

[^a02_intro-spreadsheets-7]: An example would be Excel's `.xlsx`

-   **Simplicity and Data Integrity**: Text-based formats have a simple, human-readable structure. This makes it easier to understand the data's content, and it allows manual inspection and editing using basic text editors. Additionally, they are less prone to corruption and data loss. Proprietary binary formats can sometimes become corrupted, making data recovery difficult.
-   **Interoperability, Platform Independence, and Accessibility**: Text-based formats are widely supported by various software and programming languages which means they are platform-independent. This means that data can be easily shared and integrated into different applications and systems, regardless of the software being used. Especially if program have proprietary formats having a format that can be used on different operating systems without compatibility issues.
-   **Reduced File Size**: Text-based formats generally have smaller file sizes compared to their binary counterparts. This can be advantageous for sharing and storage, especially when dealing with large data sets.
-   **Analysis, Automation, and Scripting**: Text-based formats can be directly used in data analysis workflows and are well-suited for automation and scripting tasks. Many programming languages (Python, R, and SQL) have libraries and tools to read and write data from these formats easily.
-   **Version Control**: Text-based formats work well with version control systems (e.g., Git). Because changes can be easily tracked in plain text, it's easier to collaborate, review, and manage changes made to the data.

## Common spreadsheet formatting issues {#sec-common-spreadsheet-formatting-issues}

::: {.callout-tip title="Consider this"}
In the `scratch` folder of the `01_Stats` google folder there is a series of spreadsheets. Use them to create a list of **DON'TS**.

Describe common formatting mistakes formatting data in spreadsheets, explain why it might be tempting to format data in this way and why it might cause downstream issues for data analysis.
:::

::: {.callout-note title="Answer" collapse="true"}
Here are the key points you will want to discuss:

-   multiple tables in one tab
-   data spread across multiple tabs
-   not filling in zeros
-   using problematic null values for missing data
-   using formatting to convey information
-   using formatting to make the data sheet look pretty
-   placing comments or units in cells
-   entering more than one piece of information per cell
-   using problematic field/column names
-   using special characters in data
-   including metadata in the data table
-   date formatting - dates are the worst... but we are going to just avoid dates this semester.
:::

## Quality assurance and control {#sec-quality-assurance-and-control}

You will frequently hear people say something along the lines of "oh we still have to QC the data" or "we need to complete QA/QC before we can analyze the data. QA stands for **quality assurance** and QC stands for **quality control** and both processes are critical to ensure that data being used moving forward is accurate, reliable and valid.

Quality assurance focuses on preventing errors and ensuring that the processes used to generate and enter the data are effective and efficient and minimize error. It involves establishing guidelines, standards, and best practices to be followed during the processes. The goal is to identify and address potential issues before they can occur or at least as early as possible in the process.

By contrast, quality control focuses on identifying errors that may have occurred during the process of generating and entering data by performing checks and tests at various stages of the process to verify that the final output meets the predefined quality standards.

Ensuring that we have accurate and consistent data collection methods, checking for and removing or correcting data entry errors, and validating data against predefined criteria is a critical step in (data) science. It is important that you keep a good record of the steps you took, rules you apply to discern "good" vs "bad" errors, and which data was removed to ensure transparency and repeatability.

One important component of quality assurance is stopping from bad data being entered in the first place by creating a list of valid values which will then prohibit false values from being entered. For example, we might be working with different types of gear to catch sharks at different locations, longlines, gillnets, and hook-and-line. It would be easy to accidentally mistype one of these gear types or forget whether we are entering everything lowercase or using capitalization.

::: {.callout-tip title="Give it a try"}
Learn how to create a dropdown menu for acceptable values for a categorical variable:

-   Create a GoogleSheet.
-   In cell A1 type `gear`. Then place your cursor in cell A2 and navigate to `Data` \> `Data Validation` which will bring up a dialog on the right sight of your screen. Click on `Add rule`.
-   In the `Apply to range` box it will currently say `Sheet1!A2`, you can extend this to include all cells from `A2` to `A6` by modifying this to `Sheet1!A2:A6` (or by marking the area in the spreadsheet).
-   Click on the `Criteria` Dropdown menu to see how many different types of options you have to set rules in terms of what is allowed to be entered into the cells to which you are applying this rule.
-   We are going to use a `Dropdown`. By default you will have two fields. Fill those out as `longline` and `gillnet`. Then click on `add another item` and add `hook-and-line`.
-   Check out the advance options which allows you to change whether you just get a warning or the input is rejected if your entry is invalid, you can also play with the display style to see how that affects the formatting and ease of use. Then click `Done`.

The dropdown menu we have created comprises a short list of options so you you can easily see all three and select the correct one. For longer lists it is more helpful that you can start typing in you data and then select it.
:::

You can see how this option is helpful for categorical data where typos are an issue. But you can also restrict dates to certain time periods or numbers to certain values.

Using these types of rules help minimize errors, however it is almost inevitable that something will sneak in eventually which is where quality control comes in.

::: {.callout-warning title="Pay Attention"}
Remember, before you implement any quality control measure you want to make sure that you make a copy of your data and save the original data as your raw, unaltered data set. You will want to make sure that the file name reflects that it is your raw data.

Create a separate file that you will then clean, make sure that your filename includes some sort of versioning and/or a date so you have a good record of when you processed a data set. Then you want to make sure that your data are all values and not formulas which refer to specific cells. Once you start moving cells around this can screw with your data.

You will also want to create a text-file (a typical filename would be `README`) that keeps track of all your files and manipulations so that future you or a collaborator can easily understand and replicate any steps that you take.
:::

The goal of QC is to find erroneous data. This means that it is generally going to stick out from the rest of the values in a specific column[^a02_intro-spreadsheets-8].

[^a02_intro-spreadsheets-8]: Errors are not the same as outliers. Sometimes you know that e.g. certain values cannot be true, for example if all your sample locations where in the northern hemisphere then you cannot have latitudes from the southern hemisphere.

We can generally make the assumption that the vast majority of your data is correct. This means that if we sort the values in a column if there are a few errors they will stick out and in many cases they will sort at the very top or very bottom. For example, if your column is numeric anything that is a character will pop out or if you have null values or empty cells they will generally sort to the bottom of a column.

::: {.callout-warning title="Pay Attention"}
Any time you are going to sort data, make sure that you are sorting the **entire data set** not just a single column or you will corrupt your data set and everything will end up scrambled.

Generally, if you don't have any empty columns or too much missing data if you place the cursor in a cell with a value you can use the shortcut `Ctrl`/`Cmd` + `A` to select all.

**Always double check that you have expanded your sort to the entire data set**
:::

::: {.callout-tip title="Give it a try"}
Pick out some erroneous data:

-   In the `scratch` folder, open up the `catchdata_messy` Google Sheet. Then sort each column and see which errors you can spot. Make the entire data set is highlighted.
-   Go select `Data` \> `Sort range` \> `Advanced sorting options`. Make sure you check the `Data has header row` option.
-   Use the `sort by` drop down menu to select the column you want to sort. Once you are ready, click `sort`.
-   inspect your column to determine if there are unexpected values and describe your observations.

Discuss what you will do with you findings with your lab mates. Consider whether it is better (more ethical/responsible) to remove them or correct them.
:::

Similarly, we can use conditional formatting which allows you to apply specific rules for automatically color coding to a column based on specific rules. This makes it easier for unusual entries or entries outside the possible boundaries to stand out.

::: {.callout-tip title="Give it a try"}
Apply conditional formatting to identify unusual entries:

-   In the `catchdata_messy` Google Sheet, highlight the `STL` column. The select `Format` \> `Conditional formatting` from the toolbar. This will pull up a dialog on the right hand of your window.
-   Similar to the `Data Validation` dialog, you can select the range you want to apply this rule to either by typing it in or selecting it directly in the spreadsheet.
-   Click on the `Format rules` dropdown menus and look through the available options. Let's say that we know that the sharks cannot be smaller than 50cm or larger than 2m. Set up rules for conditional highlighting that will allow you to quickly pull out invalid entries.
-   Click `Done` once you are all set and evaluate your results.

Discuss with your lab mates whether this is a better option compared to sorting.
:::

## Exporting data {#sec-exporting-data}

Generally, want to make sure that we are storing our data in a universally accessible, open, and static format rather than e.g. the default Excel file format (`*.xls` or `*.xlsx`).

-   Excel files have a proprietary format and it is possible that in the future technology will change and you will no longer be able to access your files.
-   Other program may not be able to read Excel formatted files.
-   Different version of Excel may handle data differently which can lead to inconsistencies.
-   Frequently journals or grant agencies require you to deposit your data in a data repository that only accepts certain formats which may not include Excel.

::: {.callout-tip title="Consider this"}
Discuss whether you think Google Sheets has the same problems or if it is an acceptable format to avoid these issues.
:::

Text-based formats such as comma-delimited (`*.csv`) or tab-delimited (`*.txt` or `*.txv`) files overcome these issues. In CSV files, columns are separated by commas and in tab-delimited files by tabs[^a02_intro-spreadsheets-9]. The advantage of text files is that they can be opened in any plain text editors[^a02_intro-spreadsheets-10] but you can also import them into spreadsheet programs or command-line programs like `R`.

[^a02_intro-spreadsheets-9]: This will look like whitespace if you look at it in a text editor, but tabs, but using whitespace can cause issues when command-line files parse them, tabs are less ambiguous

[^a02_intro-spreadsheets-10]: Your operating system will have a built in plain text editors such as notepad. However, you are regularly operating with text files it can be helpful to have a more powerful program like Notepad++ or Atom.

::: {.callout-important title="Protip"}
You will be generating and entering data this semester that you will need to be able to download as a text file and move into your `data` folder for a project you are working on.

Google Sheets now make it a lot easier to export and download copies of spreadsheets in different formats including `*.csv` by selecting `File` \> `Download` from the main toolbar and the choosing `comma-delimited` or `tab-delimited` from the drop down menu.
:::
