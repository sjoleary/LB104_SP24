[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Labmanual General Biology II",
    "section": "",
    "text": "Course Logistics\nWelcome to our lab handbook for General Biology II (Spring 2024) - it will be continuously updated throughout the semester. For the most part an electronic copy of this lab manual will be sufficient1, if you need to print anything for a class you will be prompted to do so on Canvas as part of your pre-lab assignments.1 You can go “split screen” on your laptop which will allow you to cut and paste code if needed or if you have a tablet you can pull up the manual there which gives you a better simultaneous viewing experience.\nFor each lab unit you will download a research compendium that will hold your data, results, and editable quarto documents2 that you will complete in Rstudio as you work through each unit. We will start off with more scaffolded activities and assignments and as you become more confident in your skills you will be increasingly independent in designing your experiments.2 think of these digital worksheets based on our lab manual.\nThe lab portion of this course will consist of three research projects addressing questions at different levels of ecological organization from organisms, to populations, communities, and ecosystems. Our study organisms provide you with a deep dive into a selection of species that represent the wide range of species diversity you will learn about this semester.\n\nProject 1: Allelopathy\nProject 2: Ecological Succession\nProject 3: Lichens as Bioindicators\n\nIn addition, you will acquire fundamental skills in data entry and management, descriptive and inferential statistics, and data visualization. Over the course of the semester you will become increasingly independent in your ability engage in all aspects of the scientific process, starting with making observations on the foundation of which you will formulate a hypothesis, design an experiment to test this hypothesis and make predictions about the outcome, collect, visualize, and analyze data, and report your findings in a written format.\nAfter completing the lab component of this course, you will be able to\n\nmake observations and use these to formulate a question and generate a hypothesis.\ndesign an appropriate experiment to generate and collect data to answer your question.\ncollect, visualize, and analyze data using appropriate qualitative and quantitative methods.\ncommunicate your research findings following the convention of a written scientific report including (1) an introduction that describes the context for the general research question and specific hypothesis being tested, (2) a description of the methods used to collect and analyze data, (3) description of the results including summary figures and tables, and (4) a discussion which identifies and explains central results in the context of the specific hypothesis and general research question posed.\n\nDepending on the research project you will be working in groups of 2 - 4 to design and execute your research projects, including preparing for and setting up experiments, gathering data in the field or lab, and analyzing data. Additionally, you will learn and apply data analysis and visualization skills using R over the course of the semester. At the end of each research project you will submit a written report. Generally, you will be assigned some pre-lab work such as background reading before your lab period so we can maximize our time in lab. Each project is unique in the questions we are exploring and the methods we will use to answer them. You will at times be expected to complete tasks/assignments outside of our scheduled lab periods. Some projects will required more time outside of labs than others and you may be involved in more than one module at a time.\nFor the lab component of this course to run smoothly, you will need to take an active role in preparing for and participating in lab. It is your responsibility to be aware of communications made in the syllabus, through announcements on Canvas, or emails. While it is always our goal to make those in a timely fashion, some last minute calls may be unavoidable. Similarly, you are expected to arrive for your lab period on time and prepared for the planned activities, including completing all assigned tasks and/or reading assignments, bringing your laptop and charger, along with any lab-related handouts3. We will be working with a lot of digital documents this semester, avoid last minute computer-related disasters by saving your work frequently and regularly backing up your files on an external drive or the cloud that you can access from another computer. If you run into issues where you will not have access to a laptop, contact your instructor and Dr. O’Leary as quickly as possible so we can make sure you can participate e.g. by finding you an alternate laptop to complete assignment in/out of lab.3 For documents that are digital including write-ups of your study design, spreadsheets with data etc. make sure that you can easily access those during lab periods. For any hard copies make sure that you print those and bring them with you as instructed.\nAll members of a group are expected to contribute to the design of each project along with collecting and analyzing data. It’s important that you establish a good working relationship with your group and hold each other accountable for the contributions that you make to this project; this may involve scheduling meetings outside of lab time. Strive to be a good team player by making sure you are responsive to your group and have open lines of communication to arrange meetings and complete assignments. We will frequently combine data sets within groups and lab sections or across sections for analysis to get large sample sizes and/or replicates, therefore it is critical that you complete out-of-lab assignments by the posted deadlines in order for your group to be able to progress through the research projects. Procrastination is frequently at the root of assignments not being completed well. When you are collaborating with others be mindful that your time management will impact the time management of others. Account for this both when you plan as a group how to divide up work and in how you complete tasks assigned to you by making sure to get started early enough to not hold up your group. While it is primarily your responsibility to coordinate and work with your group, if you are having difficulties with group members not contributing as agreed upon or other problems working together, please reach out to Dr. O’Leary or your lab instructor and we will step in to help facilitate a solution.\nAttendance in lab is mandatory. You are allowed one absence per semester. You should always inform both your instructor and Dr. O’Leary in writing (an email) of your absence before lab. If you will be missing lab due to a direct conflict with a planned college-sponsored event you must contact your instructor and Dr. O’Leary at least one week in advance. Regardless of the reason you are missing lab, you are responsible for material covered in lab and completing any pre-lab and/or homework assignments associated with a missed lab period; we will communicate with you in which form you will need to make-up in-lab assignments and activities. Additionally, you should always contact your group members for whatever project(s) you are currently working on as soon as possible to learn what you will be responsible for on any group assignments.\nYour lab grade comprises the following components:\n\nPre-lab assignments (10%): Generally, you will complete reading/similar assignments before lab to familiarize yourself with key concepts before lab. Most weeks you will have quizzes, definitions, labeling/completion of key figures or similar assignments associated with your reading or other preparatory work for that week’s in-lab work.\nParticipation (5%): Your participation grade is based on engagement during discussions in lab, lab and field work and contribution to development and execution of your research projects.\nHomework assignments (15%): Out-of-lab assignments will include practice problem sets, write-ups of experimental designs, data collection/entry, or analysis or similar assignments. Frequently there will be time to start these during lab with the expectation of them to be completed and submitted before the the next lab period.\nLab reports (70%): Even though you will be working in groups throughout the semester and collaboration is encouraged, you will individually complete and submit 3 lab reports over the course of the semester.\n\nAll lab prep work and graded assignments should be completed by the assigned deadlines.\nThis semester will include some field-based activities. Your safety is always a priority and while it is always our goal to design activities that are accessible to everyone and minimize risk there are more factors that we cannot control outside of the lab. Help us out by using common sense, keeping an eye on your lab mates, and advocating for yourself. For field-based activities please make sure that you check the weather conditions ahead of time and then ensure you are appropriately dressed in terms of layers to stay warm or cool, with shoes that can get muddy or wet. If it is sunny/hot out make sure to wear sun screen and/or a head covering, bring water etc. if you have any allergies or other medical conditions that might require emergency treatment, please let your instructor know and carry any medication you might need on you (inhaler, EpiPen, etc). Occasionally there may be unavoidable weather-related last minute changes, please make sure to check your email/Canvas for last minute notifications. If you are doing any field work on your own outside of lab time, please be mindful of your safety, at the very least let somebody know where you are going and when you expect to be back, ideally bring a buddy with you.\nAcademic dishonesty, including but not limited to cheating, plagiarism, or turning in someone else’s work as your own will not be tolerated. Make sure you are properly citing the sources of ideas, figures, data, etc. in your lab reports. Plagiarism not only includes direct, word-for-word quotations without identifying it as a direct quote using quotation marks and citing the source but also paraphrasing other people’s ideas, arguments, or structure without properly acknowledging them. Learning how to properly summarize and paraphrase information from an original source is an important skill, as we generally do not use word-for-word citations in the scientific reports in the natural sciences. While it is acceptable to use large language models (LLMs) such as ChatGPT and other artificial intelligence applications to assist you in correcting grammar, syntax or spelling or to generate synonyms, you should not use LLMs generate larger sections of text for you4. If you are unsure if you are properly attributing components of your written assignments, please come talk to your lab instructor or Dr. O’Leary5, we are happy to help. Evidence of academic dishonesty will result in a zero on that assignment along with an automatic notification of the Dean.4 Given the definitions of plagiarism above consider how this can fall into both the narrow category of not attributing a direct citation but similarly you don’t know the extent to which the LLM is “borrowing” other peoples ideas, arguments, or structure of presenting them that you would not be properly attributing to those sources.5 As a rule of thumb: If you are using LLMs or AI in a way that you are not comfortable us as instructors knowing about, that is a strong indication that you are doing something you should not be."
  },
  {
    "objectID": "B_allelopathy.html#competition-is-a-key-factor-shaping-species-distribution-and-abundance",
    "href": "B_allelopathy.html#competition-is-a-key-factor-shaping-species-distribution-and-abundance",
    "title": "Allelopathy",
    "section": "Competition is a key factor shaping species distribution and abundance",
    "text": "Competition is a key factor shaping species distribution and abundance\nA key question within ecology is how the abiotic and biotic factors within an ecosystem affect the distribution and abundance of individual populations. Because most ecosystems are limited in terms of resources the competition for those resources with other species interspecific competition is frequently a key factor in shaping population dynamics of a population, as there will be less of one or more resources available in the environment to support the population of a given species.\nInterspecific competition can be classified as either exploitative competition4 or interference competition.4 This semester when we discuss species interactions during the lecture portion of this course, we will discuss exploitative competition and define it as an interaction which negatively impacts both populations involved.\nEspecially in the context of plants, you are probably more familiar with exploitative competition which occurs when there is no direct interaction between individuals of different species, rather be consuming (“exploiting”) a resource the individuals of one species lessen the amount that resource available to another species. For example, all the trees in a forests compete for a limited supply of space, water, and nutrients in the soil. Or if you have ever planted seeds in a garden, you would have been given instructions on how far to space them out to make sure that they have enough resources.\nBy contrast, with interference competition there is direct interaction between members of different species as the individuals of one species physically prevent the individuals of the second species from gaining access. A typical example here would be hyenas, jackals, and vultures actively fighting with each other to gain access to carcasses as their food source. Outside of fantasy novels, you probably would not associate plants with interference competition, slugging it out over access to sunlight and other resources. However, allelopathy5 is a form of interference competition where one plant species emits toxic substances that kill or inhibit the growth of other plant species.5 allelo means “others”, pathy means “cause harm”"
  },
  {
    "objectID": "B_allelopathy.html#allelopathy-is-a-form-of-interference-competition",
    "href": "B_allelopathy.html#allelopathy-is-a-form-of-interference-competition",
    "title": "Allelopathy",
    "section": "Allelopathy is a form of interference competition",
    "text": "Allelopathy is a form of interference competition\nOne of the best known examples of allelopathy is black walnut (Juglans nigra) which releases a substance called hydroxyjuglone from various tissues. Oxidation of this substance in the soil produces juglone which inhibits the growth of certain other species in that same soil while not affecting others. For example, tomato plants will not grow well under black walnut trees while Kentucky bluegrass common ground cover for suburban lawns are unaffected.\nWhile all allelopathic plants generate phytotoxic chemicals6, the way they enter the environment may differ. For example, some allelopathic plants excrete allelopathic compounds from their roots into the soil from where they are then absorbed by the roots of over plants. In other cases, toxic chemicals may be leached from dropped leaves and twigs when rainwater dissolves and carries soluble compounds into the soil. Finally, plants may release phytotoxic chemicals as volatile compounds into the air that are then deposited on the leaves and shoots of nearby plants.6 phyto means “plants”, toxic means “poisonous”.\nUnderstanding allelopathy can have important applications. For example, there is ongoing interest in using allelopathic plants instead of herbicides especially in the context of agriculture. Similarly, understanding these effects can be important for restoration ecology where foresters have discovered that the presence/absence of allelopathic plants influences the success of reseeding clear cuts.\nFor this research project you will design a bioassay to test for allelopathic effects of different plant species. Specifically, you will (1) identify an odoriferous plant to test for allelopathic effects and (2) design an experiment to test your hypothesis and (3) collect and analyze your data using some basic statistical techniques you will be introduced to this semester."
  },
  {
    "objectID": "B_allelopathy.html#acknowledgements",
    "href": "B_allelopathy.html#acknowledgements",
    "title": "Allelopathy",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis lab is adapted from the Johnson and Johnson (2002) Allelopathy Lab manual."
  },
  {
    "objectID": "A_Exp-Design-Analysis.html",
    "href": "A_Exp-Design-Analysis.html",
    "title": "Scientific Method & Experimental Design",
    "section": "",
    "text": "After completing this unit, you should be able to\n\nOutline the scientific method as an iterative process of observation, hypothesis formation, experimentation, data collection & analysis, and conclusion drawing.\nUse spreadsheets to enter and manage data.\nDescribe the key components of experimental design to ensure conclusions are reliable.\nGenerate descriptive statistics summarizing distribution, central tendency and variability of a data set using R.\nInterpret a t-test and \\(𝛘^2\\)-test to determine the probability that results are result of sampling error and do not represent reality.\n\nFor each of our units we will have a project folder1 with an Rproject, *.qmd-files to complete lab assignments and write your lab report along with sub-directories to hold the data and results that you will generate. At the beginning of the semester, you should have created a directory on your Desktop or Documents folder on your laptop (name it something like bi104) as a home directory for all of our project folders this semester.1 We will use “directory” and “folder” synonymously throughout this lab handbook\nDownload the 01_Stats project folder using the link provided to you. Once you have downloaded it, unzip the project directory into your bi104 directory2.2 On a MacOS, your system it will automatically unzip the folder for you and you should be able to directly move the entire unzipped directory to your bi104 folder. On a Windows OS you will need to right click and select extract all, then you should be able to specify which directory you want to unzip your file into.\nOnce you have done this, you can open the Rproj for this module either by double clicking on it which will launch Rstudio or by opening Rstudio and then using File &gt; Open Project or by clicking on the Rproject icon in the top right of your program window and selecting Open Project.\nOnce you have opened a project you should see the project name in the top right corner3.3 Pro tip: If you run into issues where a quarto document won’t render or file paths aren’t working (especially if things were working previously) one of your first steps should be to double check that the correct Rproj is loaded.\nThis unit which will introduce you to the scientific method, experimental, design, and statistics to provide you with an overview of the foundational aspects of scientific inquiry that you will apply to different research projects this semester. We will complete these activities in parallel to your first research project for which you will design a bioassay to test allelopathic effects of a plant on radish seeds."
  },
  {
    "objectID": "B_allelopathy.html#sec-competition-is-a-key-factor-shaping-species-distribution-and-abundance",
    "href": "B_allelopathy.html#sec-competition-is-a-key-factor-shaping-species-distribution-and-abundance",
    "title": "Allelopathy",
    "section": "Competition is a key factor shaping species distribution and abundance",
    "text": "Competition is a key factor shaping species distribution and abundance\nA key question within ecology is how the abiotic and biotic factors within an ecosystem affect the distribution and abundance of individual populations. Because most ecosystems are limited in terms of resources the competition for those resources with other species interspecific competition is frequently a key factor in shaping population dynamics of a population, as there will be less of one or more resources available in the environment to support the population of a given species.\nInterspecific competition can be classified as either exploitative competition4 or interference competition.4 This semester when we discuss species interactions during the lecture portion of this course, we will discuss exploitative competition and define it as an interaction which negatively impacts both populations involved.\nEspecially in the context of plants, you are probably more familiar with exploitative competition which occurs when there is no direct interaction between individuals of different species, rather be consuming (“exploiting”) a resource the individuals of one species lessen the amount that resource available to another species. For example, all the trees in a forests compete for a limited supply of space, water, and nutrients in the soil. Or if you have ever planted seeds in a garden, you would have been given instructions on how far to space them out to make sure that they have enough resources.\nBy contrast, with interference competition there is direct interaction between members of different species as the individuals of one species physically prevent the individuals of the second species from gaining access. A typical example here would be hyenas, jackals, and vultures actively fighting with each other to gain access to carcasses as their food source. Outside of fantasy novels, you probably would not associate plants with interference competition, slugging it out over access to sunlight and other resources. However, allelopathy5 is a form of interference competition where one plant species emits toxic substances that kill or inhibit the growth of other plant species.5 allelo means “others”, pathy means “cause harm”"
  },
  {
    "objectID": "B_allelopathy.html#sec-allelopathy-is-a-form-of-interference-competition",
    "href": "B_allelopathy.html#sec-allelopathy-is-a-form-of-interference-competition",
    "title": "Allelopathy",
    "section": "Allelopathy is a form of interference competition",
    "text": "Allelopathy is a form of interference competition\nOne of the best known examples of allelopathy is black walnut (Juglans nigra) which releases a substance called hydroxyjuglone from various tissues. Oxidation of this substance in the soil produces juglone which inhibits the growth of certain other species in that same soil while not affecting others. For example, tomato plants will not grow well under black walnut trees while Kentucky bluegrass common ground cover for suburban lawns are unaffected.\nWhile all allelopathic plants generate phytotoxic chemicals6, the way they enter the environment may differ. For example, some allelopathic plants excrete allelopathic compounds from their roots into the soil from where they are then absorbed by the roots of over plants. In other cases, toxic chemicals may be leached from dropped leaves and twigs when rainwater dissolves and carries soluble compounds into the soil. Finally, plants may release phytotoxic chemicals as volatile compounds into the air that are then deposited on the leaves and shoots of nearby plants.6 phyto means “plants”, toxic means “poisonous”.\nUnderstanding allelopathy can have important applications. For example, there is ongoing interest in using allelopathic plants instead of herbicides especially in the context of agriculture. Similarly, understanding these effects can be important for restoration ecology where foresters have discovered that the presence/absence of allelopathic plants influences the success of reseeding clear cuts.\nFor this research project you will design a bioassay to test for allelopathic effects of different plant species. Specifically, you will (1) identify an odoriferous plant to test for allelopathic effects and (2) design an experiment to test your hypothesis and (3) collect and analyze your data using some basic statistical techniques you will be introduced to this semester."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#spreadsheets-for-data-entry-and-management",
    "href": "A02_intro-spreadsheets.html#spreadsheets-for-data-entry-and-management",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.1 Spreadsheets for data entry and management",
    "text": "2.1 Spreadsheets for data entry and management\nThe foundation of any research project is good data organization. This not only includes your actual data points (observations) but also things like keeping track of specimen and other samples along with all the metadata1. Additionally, you should keep good records of how the data was produced (your methods). Thinking through ahead of time what measurements are important, i.e. what data you will record and how you will store your data is really important to make sure you are keeping track of the entire process. Good data management and clean data sets will make sharing and analyzing data a lot more straightforward.1 Metadata is data about your data. It helps describe, categorize, organize, and provide context to the main content or primary data it is associated with. It’s commonly used to provide additional information that helps users, systems, or processes understand and interpret the main data.\nWhile we are going to use R to wrangle, manipulate, analyze, and visualize data in R for a more efficient and reproducible approach compared to what can be using spreadsheets, spreadsheets are the better tool for data entry, data management/organization, and simple quality control (QC) and quality assurance (QA). Thinking ahead to how you want to organize and format your data in spreadsheets will prevent a lot of extra work and headaches down the line, especially when we plan ahead as to how we should organize it to make it more efficient to use with command-line computational tools such as R.\nSo, before we dive deep into manipulating data with R, we’ll take a small step back and think through a few fundamental rules for managing data in spreadsheets before learning how to do these and more advanced data wrangling and manipulation using R.\nWhile you can do some statistics and plotting using spreadsheet programs we will not be learning how to do this in this class2. Data analysis in spreadsheets requires a lot of manual work and generally any time you want to change one parameter or if you have to update your spreadsheet with new entries or you need to apply the same analysis to another data set you end up having to redo everything by hand. The more things you do by hand, the more likely you are to make a mistake. Even if you do apply some sophisticated coding in spreadsheets and/or use it for analysis or plotting3 it is very difficult to track the exact steps or document them in a way that makes it fully reproducible for another person.2 If you are interested, you can also take courses to learn how to leverage spreadsheets efficiently for a wide rang of tasks in the Computer Science department, depending on what quantitative reasoning course you take or if you get involved in a research lab, you will be exposed to various other programs for data analysis and a lot of the key principles you will learn this semester will still apply even if they are implemented a bit differently.3 It can be helpful to know the fundamentals for simple plots in spreadsheets for quick and dirty plotting to get a quick look at your data to get an idea of what it looks like and spot potential mistakes during data entry without having to export data and fire up R or a another command-line program."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#best-practices-for-formatting-data-in-spreadsheets",
    "href": "A02_intro-spreadsheets.html#best-practices-for-formatting-data-in-spreadsheets",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.2 Best practices for formatting data in spreadsheets",
    "text": "2.2 Best practices for formatting data in spreadsheets\n\n2.2.1 Format your data set for the tool you will use to analyze it with\nOur brains don’t work the same way as computers. Your spreadsheet is not a lab notebook and while a layout where there are notes in the margin, context of the experiment, or a specific layout of data might be something that you can interpret, it will more difficult for another person to follow your through process and understand your records/notes. Another person might have the opportunity to ask follow up questions and get the clarifications they need, but a computer cannot.\nOccasionally, you might use a spreadsheet instead of a lab notebook where it is a way of keeping track of an experiment and various people interacting with samples, completing different steps etc. However, if you are using a spreadsheet for data entry and management, then you want to ensure that you have set up your spreadsheet in a way where a computer is going to be able to correctly interpret it as intended. This means that we need to think through how we want to set up spreadsheets. There are generally a few different ways you can set things up and some of them will limit how easy it is for you and/or a future collaborator to work with it down the line4.4 The optimum software/interface for data input and layout/formatting may differ depending on your intended analysis, so keep in mind how you want to be able to analyze your data and whether that will require specific formats. In general, try to pick a format that will give you the advantage of being able to easily convert it between different formats - which is something we will learn to do with R specifically in the tidyverse which centers on a specific concept of what makes data tidy.\nThis semester you will be gathering data for three research projects, take the time to think through why we are organizing data in a certain way and how the principles we discuss today apply.\n\n\n2.2.2 Never touch your raw data\nRaw data is the original, unaltered data that is collected or generated before any manipulation, transformation, or analysis takes place. It’s the most fundamental form of data, directly obtained from observations, measurements, or data sources. Raw data is often in its most unstructured and basic state, and it serves as the foundation for subsequent data processing and analysis.\nIn the biological and environmental sciences typical sources include direct observations made in experiments, field studies, or natural phenomena or measurements from sensors or other instruments measuring physical and chemical parameters such as temperature, GPS coordinate, pH etc.\n\n\n\n\n\n\nPay Attention\n\n\n\nNever touch your raw data. Always keep a copy of your raw data that you never modify directly.\nFor any kind of data related work it is important that you preserve the original, unaltered version of your data when conducting data analysis. Avoid making changes directly to the original data files or data set. Instead, You should work with copies of the data or use a structured workflow that ensures the integrity and reproducibility of your analysis.\n\n\nKeeping your raw data as a separate file that is never altered is important for\n\nData Integrity: Modifying the raw data directly could lead to unintended changes or loss of information. By keeping the raw data untouched, you ensure that you have a reliable source of truth to refer back to if needed.\nReproducibility: If you or others need to replicate your analysis in the future, having access to the exact original data is crucial. Changes made to the raw data could make it difficult or impossible to reproduce your results accurately.\nError Prevention: Working with copies of the raw data minimizes the risk of accidental changes or mistakes that could affect your analysis. If errors occur, you can always go back to the untouched raw data.\nData Auditing: In some cases, you might need to show the authenticity and accuracy of your data. Keeping the original data untouched allows you to demonstrate the reliability of your work.\nMultiple Analyses: If you’re working on different analyses, projects, or collaborations using the same data, maintaining the integrity of the raw data enables consistency across these efforts.\n\nBest practices of maintaining the integrity of your raw data include\n\nMake copies: Always work with copies of the original data files. This ensures that any changes you make are isolated from the raw data.\nImplement a structured workflow with detailed documentation: Develop a repeatable workflow that includes data cleaning, transformation, and analysis steps. Maintain a clear and detailed documentation about the steps you’ve taken, the rationale behind each decisions, and any changes you’ve made to the data to ensure transparency.\nCreate backups and use version control: Regularly back up your data, including both the raw data and any processed versions, to prevent data loss. If you are taking data down on physical data sheets and then entering that into spreadsheets make sure you hang onto the physical data sheets. Use version control systems like Git to track changes to your data set. GoogleSheets also includes version control.\n\n\n\n2.2.3 Keep track of your formatting steps\nBy working with copies and following a structured workflow, you can ensure the accuracy, reproducibility, and integrity of your data analysis work. While you should not modify the raw data directly, it’s also important to apply necessary data preprocessing steps (like cleaning and transforming) as part of your analysis process. This means that you should do two things\n\nAny time you need to process or analyze your data make a copy instead of operating directly in your raw data5 and then create a new file with your cleaned or analyzed data.\nKeep track of the exact steps you took to clean or analyze your data6; this is just as important as keeping a detailed record of the steps you took in an experiment; data analysis is an important part of your methods and should be documented as such. Good practice would be to keep a plain text file or similar in the same folder as your data set where you record any steps you take.\n\n5 You will notice this semester that we will read in data from text files for analysis and while we can do a lot of manipulation and calculations this does not change our raw data file.6 The second advantage of command-line programs like R is that because you are using a series of commands to wrangle and analyze your data your are automatically creating a very detailed, reproducible record of your your workflow\n\n2.2.4 Put variables in columns and observations in rows\nObservations and variables are two fundamental concepts that describe different aspects of data.\n\n\n\n\n\n\nConsider this\n\n\n\nCompare and contrast what an observation is compared to a variable. Consider how the terms metric and units fit in.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAn observation is a single data point or unit within a data set, while a variable is a characteristic that is being measured or observed for each of those data points.\nTogether, observations and variables make up the structure of a data set, where each observation has values for the various variables being measured.\nVariables are attributes being measured, metrics are calculated values that summarize those attributes. Units of measurements tells us how something is being measured depending on the scale we are using to quantify the attributes of a variable. For example, you could measure the length using kilometers or miles.\n\n\n\nHere is the key rule for structuring data in spreadsheet in a tidy way:\n\n\n\n\n\n\nPay Attention\n\n\n\n\nEvery variable gets its own column\nEvery observation gets its own rule\nDo not combine multiple pieces of information in one cell.\n\nThese principles follow Hadley Wickhams definition of “tidy data” which is an underlying principle for a lot the R code you will apply this semester.\n\n\n\n\n2.2.5 Export data as text-based formats\nWhile it is a lot easier to enter and look at data in a spreadsheet you should always export your raw and cleaned data set as a text-based format such as CSV, TSV, JSON, XML, etc.).\nThis offers several advantages compared to proprietary binary formats7:7 An example would be Excel’s .xlsx\n\nSimplicity and Data Integrity: Text-based formats have a simple, human-readable structure. This makes it easier to understand the data’s content, and it allows manual inspection and editing using basic text editors. Additionally, they are less prone to corruption and data loss. Proprietary binary formats can sometimes become corrupted, making data recovery difficult.\nInteroperability, Platform Independence, and Accessibility: Text-based formats are widely supported by various software and programming languages which means they are platform-independent. This means that data can be easily shared and integrated into different applications and systems, regardless of the software being used. Especially if program have proprietary formats having a format that can be used on different operating systems without compatibility issues.\nReduced File Size: Text-based formats generally have smaller file sizes compared to their binary counterparts. This can be advantageous for sharing and storage, especially when dealing with large data sets.\nAnalysis, Automation, and Scripting: Text-based formats can be directly used in data analysis workflows and are well-suited for automation and scripting tasks. Many programming languages (Python, R, and SQL) have libraries and tools to read and write data from these formats easily.\nVersion Control: Text-based formats work well with version control systems (e.g., Git). Because changes can be easily tracked in plain text, it’s easier to collaborate, review, and manage changes made to the data."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#common-spreadsheet-formatting-issues",
    "href": "A02_intro-spreadsheets.html#common-spreadsheet-formatting-issues",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.3 Common spreadsheet formatting issues",
    "text": "2.3 Common spreadsheet formatting issues\n\n\n\n\n\n\nConsider this\n\n\n\nIn the scratch folder of the 01_Stats google folder there is a series of spreadsheets. Use them to create a list of DON’TS.\nDescribe common formatting mistakes formatting data in spreadsheets, explain why it might be tempting to format data in this way and why it might cause downstream issues for data analysis.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nHere are the key points you will want to discuss:\n\nmultiple tables in one tab\ndata spread across multiple tabs\nnot filling in zeros\nusing problematic null values for missing data\nusing formatting to convey information\nusing formatting to make the data sheet look pretty\nplacing comments or units in cells\nentering more than one piece of information per cell\nusing problematic field/column names\nusing special characters in data\nincluding metadata in the data table\ndate formatting - dates are the worst… but we are going to just avoid dates this semester."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#quality-assurance-and-control",
    "href": "A02_intro-spreadsheets.html#quality-assurance-and-control",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.4 Quality assurance and control",
    "text": "2.4 Quality assurance and control\nYou will frequently hear people say something along the lines of “oh we still have to QC the data” or “we need to complete QA/QC before we can analyze the data. QA stands for quality assurance and QC stands for quality control and both processes are critical to ensure that data being used moving forward is accurate, reliable and valid.\nQuality assurance focuses on preventing errors and ensuring that the processes used to generate and enter the data are effective and efficient and minimize error. It involves establishing guidelines, standards, and best practices to be followed during the processes. The goal is to identify and address potential issues before they can occur or at least as early as possible in the process.\nBy contrast, quality control focuses on identifying errors that may have occurred during the process of generating and entering data by performing checks and tests at various stages of the process to verify that the final output meets the predefined quality standards.\nEnsuring that we have accurate and consistent data collection methods, checking for and removing or correcting data entry errors, and validating data against predefined criteria is a critical step in (data) science. It is important that you keep a good record of the steps you took, rules you apply to discern “good” vs “bad” errors, and which data was removed to ensure transparency and repeatability.\nOne important component of quality assurance is stopping from bad data being entered in the first place by creating a list of valid values which will then prohibit false values from being entered. For example, we might be working with different types of gear to catch sharks at different locations, longlines, gillnets, and hook-and-line. It would be easy to accidentally mistype one of these gear types or forget whether we are entering everything lowercase or using capitalization.\n\n\n\n\n\n\nGive it a try\n\n\n\nLearn how to create a dropdown menu for acceptable values for a categorical variable:\n\nCreate a GoogleSheet.\nIn cell A1 type gear. Then place your cursor in cell A2 and navigate to Data &gt; Data Validation which will bring up a dialog on the right sight of your screen. Click on Add rule.\nIn the Apply to range box it will currently say Sheet1!A2, you can extend this to include all cells from A2 to A6 by modifying this to Sheet1!A2:A6 (or by marking the area in the spreadsheet).\nClick on the Criteria Dropdown menu to see how many different types of options you have to set rules in terms of what is allowed to be entered into the cells to which you are applying this rule.\nWe are going to use a Dropdown. By default you will have two fields. Fill those out as longline and gillnet. Then click on add another item and add hook-and-line.\nCheck out the advance options which allows you to change whether you just get a warning or the input is rejected if your entry is invalid, you can also play with the display style to see how that affects the formatting and ease of use. Then click Done.\n\nThe dropdown menu we have created comprises a short list of options so you you can easily see all three and select the correct one. For longer lists it is more helpful that you can start typing in you data and then select it.\n\n\nYou can see how this option is helpful for categorical data where typos are an issue. But you can also restrict dates to certain time periods or numbers to certain values.\nUsing these types of rules help minimize errors, however it is almost inevitable that something will sneak in eventually which is where quality control comes in.\n\n\n\n\n\n\nPay Attention\n\n\n\nRemember, before you implement any quality control measure you want to make sure that you make a copy of your data and save the original data as your raw, unaltered data set. You will want to make sure that the file name reflects that it is your raw data.\nCreate a separate file that you will then clean, make sure that your filename includes some sort of versioning and/or a date so you have a good record of when you processed a data set. Then you want to make sure that your data are all values and not formulas which refer to specific cells. Once you start moving cells around this can screw with your data.\nYou will also want to create a text-file (a typical filename would be README) that keeps track of all your files and manipulations so that future you or a collaborator can easily understand and replicate any steps that you take.\n\n\nThe goal of QC is to find erroneous data. This means that it is generally going to stick out from the rest of the values in a specific column8.8 Errors are not the same as outliers. Sometimes you know that e.g. certain values cannot be true, for example if all your sample locations where in the northern hemisphere then you cannot have latitudes from the southern hemisphere.\nWe can generally make the assumption that the vast majority of your data is correct. This means that if we sort the values in a column if there are a few errors they will stick out and in many cases they will sort at the very top or very bottom. For example, if your column is numeric anything that is a character will pop out or if you have null values or empty cells they will generally sort to the bottom of a column.\n\n\n\n\n\n\nPay Attention\n\n\n\nAny time you are going to sort data, make sure that you are sorting the entire data set not just a single column or you will corrupt your data set and everything will end up scrambled.\nGenerally, if you don’t have any empty columns or too much missing data if you place the cursor in a cell with a value you can use the shortcut Ctrl/Cmd + A to select all.\nAlways double check that you have expanded your sort to the entire data set\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nPick out some erroneous data:\n\nIn the scratch folder, open up the catchdata_messy Google Sheet. Then sort each column and see which errors you can spot. Make the entire data set is highlighted.\nGo select Data &gt; Sort range &gt; Advanced sorting options. Make sure you check the Data has header row option.\nUse the sort by drop down menu to select the column you want to sort. Once you are ready, click sort.\ninspect your column to determine if there are unexpected values and describe your observations.\n\nDiscuss what you will do with you findings with your lab mates. Consider whether it is better (more ethical/responsible) to remove them or correct them.\n\n\nSimilarly, we can use conditional formatting which allows you to apply specific rules for automatically color coding to a column based on specific rules. This makes it easier for unusual entries or entries outside the possible boundaries to stand out.\n\n\n\n\n\n\nGive it a try\n\n\n\nApply conditional formatting to identify unusual entries:\n\nIn the catchdata_messy Google Sheet, highlight the STL column. The select Format &gt; Conditional formatting from the toolbar. This will pull up a dialog on the right hand of your window.\nSimilar to the Data Validation dialog, you can select the range you want to apply this rule to either by typing it in or selecting it directly in the spreadsheet.\nClick on the Format rules dropdown menus and look through the available options. Let’s say that we know that the sharks cannot be smaller than 50cm or larger than 2m. Set up rules for conditional highlighting that will allow you to quickly pull out invalid entries.\nClick Done once you are all set and evaluate your results.\n\nDiscuss with your lab mates whether this is a better option compared to sorting."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#exporting-data",
    "href": "A02_intro-spreadsheets.html#exporting-data",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.5 Exporting data",
    "text": "2.5 Exporting data\nGenerally, want to make sure that we are storing our data in a universally accessible, open, and static format rather than e.g. the default Excel file format (*.xls or *.xlsx).\n\nExcel files have a proprietary format and it is possible that in the future technology will change and you will no longer be able to access your files.\nOther program may not be able to read Excel formatted files.\nDifferent version of Excel may handle data differently which can lead to inconsistencies.\nFrequently journals or grant agencies require you to deposit your data in a data repository that only accepts certain formats which may not include Excel.\n\n\n\n\n\n\n\nConsider this\n\n\n\nDiscuss whether you think Google Sheets has the same problems or if it is an acceptable format to avoid these issues.\n\n\nText-based formats such as comma-delimited (*.csv) or tab-delimited (*.txt or *.txv) files overcome these issues. In CSV files, columns are separated by commas and in tab-delimited files by tabs9. The advantage of text files is that they can be opened in any plain text editors10 but you can also import them into spreadsheet programs or command-line programs like R.9 This will look like whitespace if you look at it in a text editor, but tabs, but using whitespace can cause issues when command-line files parse them, tabs are less ambiguous10 Your operating system will have a built in plain text editors such as notepad. However, you are regularly operating with text files it can be helpful to have a more powerful program like Notepad++ or Atom.\n\n\n\n\n\n\nProtip\n\n\n\nYou will be generating and entering data this semester that you will need to be able to download as a text file and move into your data folder for a project you are working on.\nGoogle Sheets now make it a lot easier to export and download copies of spreadsheets in different formats including *.csv by selecting File &gt; Download from the main toolbar and the choosing comma-delimited or tab-delimited from the drop down menu."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#sec-best-practices-for-formatting-data-in-spreadsheets",
    "href": "A02_intro-spreadsheets.html#sec-best-practices-for-formatting-data-in-spreadsheets",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.2 Best practices for formatting data in spreadsheets",
    "text": "2.2 Best practices for formatting data in spreadsheets\n\n2.2.1 Format your data set for the tool you will use to analyze it with\nOur brains don’t work the same way as computers. Your spreadsheet is not a lab notebook and while a layout where there are notes in the margin, context of the experiment, or a specific layout of data might be something that you can interpret, it will more difficult for another person to follow your through process and understand your records/notes. Another person might have the opportunity to ask follow up questions and get the clarifications they need, but a computer cannot.\nOccasionally, you might use a spreadsheet instead of a lab notebook where it is a way of keeping track of an experiment and various people interacting with samples, completing different steps etc. However, if you are using a spreadsheet for data entry and management, then you want to ensure that you have set up your spreadsheet in a way where a computer is going to be able to correctly interpret it as intended. This means that we need to think through how we want to set up spreadsheets. There are generally a few different ways you can set things up and some of them will limit how easy it is for you and/or a future collaborator to work with it down the line4.4 The optimum software/interface for data input and layout/formatting may differ depending on your intended analysis, so keep in mind how you want to be able to analyze your data and whether that will require specific formats. In general, try to pick a format that will give you the advantage of being able to easily convert it between different formats - which is something we will learn to do with R specifically in the tidyverse which centers on a specific concept of what makes data tidy.\nThis semester you will be gathering data for three research projects, take the time to think through why we are organizing data in a certain way and how the principles we discuss today apply.\n\n\n2.2.2 Never touch your raw data\nRaw data is the original, unaltered data that is collected or generated before any manipulation, transformation, or analysis takes place. It’s the most fundamental form of data, directly obtained from observations, measurements, or data sources. Raw data is often in its most unstructured and basic state, and it serves as the foundation for subsequent data processing and analysis.\nIn the biological and environmental sciences typical sources include direct observations made in experiments, field studies, or natural phenomena or measurements from sensors or other instruments measuring physical and chemical parameters such as temperature, GPS coordinate, pH etc.\n\n\n\n\n\n\nPay Attention\n\n\n\nNever touch your raw data. Always keep a copy of your raw data that you never modify directly.\nFor any kind of data related work it is important that you preserve the original, unaltered version of your data when conducting data analysis. Avoid making changes directly to the original data files or data set. Instead, You should work with copies of the data or use a structured workflow that ensures the integrity and reproducibility of your analysis.\n\n\nKeeping your raw data as a separate file that is never altered is important for\n\nData Integrity: Modifying the raw data directly could lead to unintended changes or loss of information. By keeping the raw data untouched, you ensure that you have a reliable source of truth to refer back to if needed.\nReproducibility: If you or others need to replicate your analysis in the future, having access to the exact original data is crucial. Changes made to the raw data could make it difficult or impossible to reproduce your results accurately.\nError Prevention: Working with copies of the raw data minimizes the risk of accidental changes or mistakes that could affect your analysis. If errors occur, you can always go back to the untouched raw data.\nData Auditing: In some cases, you might need to show the authenticity and accuracy of your data. Keeping the original data untouched allows you to demonstrate the reliability of your work.\nMultiple Analyses: If you’re working on different analyses, projects, or collaborations using the same data, maintaining the integrity of the raw data enables consistency across these efforts.\n\nBest practices of maintaining the integrity of your raw data include\n\nMake copies: Always work with copies of the original data files. This ensures that any changes you make are isolated from the raw data.\nImplement a structured workflow with detailed documentation: Develop a repeatable workflow that includes data cleaning, transformation, and analysis steps. Maintain a clear and detailed documentation about the steps you’ve taken, the rationale behind each decisions, and any changes you’ve made to the data to ensure transparency.\nCreate backups and use version control: Regularly back up your data, including both the raw data and any processed versions, to prevent data loss. If you are taking data down on physical data sheets and then entering that into spreadsheets make sure you hang onto the physical data sheets. Use version control systems like Git to track changes to your data set. GoogleSheets also includes version control.\n\n\n\n2.2.3 Keep track of your formatting steps\nBy working with copies and following a structured workflow, you can ensure the accuracy, reproducibility, and integrity of your data analysis work. While you should not modify the raw data directly, it’s also important to apply necessary data preprocessing steps (like cleaning and transforming) as part of your analysis process. This means that you should do two things\n\nAny time you need to process or analyze your data make a copy instead of operating directly in your raw data5 and then create a new file with your cleaned or analyzed data.\nKeep track of the exact steps you took to clean or analyze your data6; this is just as important as keeping a detailed record of the steps you took in an experiment; data analysis is an important part of your methods and should be documented as such. Good practice would be to keep a plain text file or similar in the same folder as your data set where you record any steps you take.\n\n5 You will notice this semester that we will read in data from text files for analysis and while we can do a lot of manipulation and calculations this does not change our raw data file.6 The second advantage of command-line programs like R is that because you are using a series of commands to wrangle and analyze your data your are automatically creating a very detailed, reproducible record of your your workflow\n\n2.2.4 Put variables in columns and observations in rows\nObservations and variables are two fundamental concepts that describe different aspects of data.\n\n\n\n\n\n\nConsider this\n\n\n\nCompare and contrast what an observation is compared to a variable. Consider how the terms metric and units fit in.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAn observation is a single data point or unit within a data set, while a variable is a characteristic that is being measured or observed for each of those data points.\nTogether, observations and variables make up the structure of a data set, where each observation has values for the various variables being measured.\nVariables are attributes being measured, metrics are calculated values that summarize those attributes. Units of measurements tells us how something is being measured depending on the scale we are using to quantify the attributes of a variable. For example, you could measure the length using kilometers or miles.\n\n\n\nHere is the key rule for structuring data in spreadsheet in a tidy way:\n\n\n\n\n\n\nPay Attention\n\n\n\n\nEvery variable gets its own column\nEvery observation gets its own rule\nDo not combine multiple pieces of information in one cell.\n\nThese principles follow Hadley Wickhams definition of “tidy data” which is an underlying principle for a lot the R code you will apply this semester.\n\n\n\n\n2.2.5 Export data as text-based formats\nWhile it is a lot easier to enter and look at data in a spreadsheet you should always export your raw and cleaned data set as a text-based format such as CSV, TSV, JSON, XML, etc.).\nThis offers several advantages compared to proprietary binary formats7:7 An example would be Excel’s .xlsx\n\nSimplicity and Data Integrity: Text-based formats have a simple, human-readable structure. This makes it easier to understand the data’s content, and it allows manual inspection and editing using basic text editors. Additionally, they are less prone to corruption and data loss. Proprietary binary formats can sometimes become corrupted, making data recovery difficult.\nInteroperability, Platform Independence, and Accessibility: Text-based formats are widely supported by various software and programming languages which means they are platform-independent. This means that data can be easily shared and integrated into different applications and systems, regardless of the software being used. Especially if program have proprietary formats having a format that can be used on different operating systems without compatibility issues.\nReduced File Size: Text-based formats generally have smaller file sizes compared to their binary counterparts. This can be advantageous for sharing and storage, especially when dealing with large data sets.\nAnalysis, Automation, and Scripting: Text-based formats can be directly used in data analysis workflows and are well-suited for automation and scripting tasks. Many programming languages (Python, R, and SQL) have libraries and tools to read and write data from these formats easily.\nVersion Control: Text-based formats work well with version control systems (e.g., Git). Because changes can be easily tracked in plain text, it’s easier to collaborate, review, and manage changes made to the data."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#sec-common-spreadsheet-formatting-issues",
    "href": "A02_intro-spreadsheets.html#sec-common-spreadsheet-formatting-issues",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.3 Common spreadsheet formatting issues",
    "text": "2.3 Common spreadsheet formatting issues\n\n\n\n\n\n\nConsider this\n\n\n\nIn the scratch folder of the 01_Stats google folder there is a series of spreadsheets. Use them to create a list of DON’TS.\nDescribe common formatting mistakes formatting data in spreadsheets, explain why it might be tempting to format data in this way and why it might cause downstream issues for data analysis.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nHere are the key points you will want to discuss:\n\nmultiple tables in one tab\ndata spread across multiple tabs\nnot filling in zeros\nusing problematic null values for missing data\nusing formatting to convey information\nusing formatting to make the data sheet look pretty\nplacing comments or units in cells\nentering more than one piece of information per cell\nusing problematic field/column names\nusing special characters in data\nincluding metadata in the data table\ndate formatting - dates are the worst… but we are going to just avoid dates this semester."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#sec-quality-assurance-and-control",
    "href": "A02_intro-spreadsheets.html#sec-quality-assurance-and-control",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.4 Quality assurance and control",
    "text": "2.4 Quality assurance and control\nYou will frequently hear people say something along the lines of “oh we still have to QC the data” or “we need to complete QA/QC before we can analyze the data. QA stands for quality assurance and QC stands for quality control and both processes are critical to ensure that data being used moving forward is accurate, reliable and valid.\nQuality assurance focuses on preventing errors and ensuring that the processes used to generate and enter the data are effective and efficient and minimize error. It involves establishing guidelines, standards, and best practices to be followed during the processes. The goal is to identify and address potential issues before they can occur or at least as early as possible in the process.\nBy contrast, quality control focuses on identifying errors that may have occurred during the process of generating and entering data by performing checks and tests at various stages of the process to verify that the final output meets the predefined quality standards.\nEnsuring that we have accurate and consistent data collection methods, checking for and removing or correcting data entry errors, and validating data against predefined criteria is a critical step in (data) science. It is important that you keep a good record of the steps you took, rules you apply to discern “good” vs “bad” errors, and which data was removed to ensure transparency and repeatability.\nOne important component of quality assurance is stopping from bad data being entered in the first place by creating a list of valid values which will then prohibit false values from being entered. For example, we might be working with different types of gear to catch sharks at different locations, longlines, gillnets, and hook-and-line. It would be easy to accidentally mistype one of these gear types or forget whether we are entering everything lowercase or using capitalization.\n\n\n\n\n\n\nGive it a try\n\n\n\nLearn how to create a dropdown menu for acceptable values for a categorical variable:\n\nCreate a GoogleSheet.\nIn cell A1 type gear. Then place your cursor in cell A2 and navigate to Data &gt; Data Validation which will bring up a dialog on the right sight of your screen. Click on Add rule.\nIn the Apply to range box it will currently say Sheet1!A2, you can extend this to include all cells from A2 to A6 by modifying this to Sheet1!A2:A6 (or by marking the area in the spreadsheet).\nClick on the Criteria Dropdown menu to see how many different types of options you have to set rules in terms of what is allowed to be entered into the cells to which you are applying this rule.\nWe are going to use a Dropdown. By default you will have two fields. Fill those out as longline and gillnet. Then click on add another item and add hook-and-line.\nCheck out the advance options which allows you to change whether you just get a warning or the input is rejected if your entry is invalid, you can also play with the display style to see how that affects the formatting and ease of use. Then click Done.\n\nThe dropdown menu we have created comprises a short list of options so you you can easily see all three and select the correct one. For longer lists it is more helpful that you can start typing in you data and then select it.\n\n\nYou can see how this option is helpful for categorical data where typos are an issue. But you can also restrict dates to certain time periods or numbers to certain values.\nUsing these types of rules help minimize errors, however it is almost inevitable that something will sneak in eventually which is where quality control comes in.\n\n\n\n\n\n\nPay Attention\n\n\n\nRemember, before you implement any quality control measure you want to make sure that you make a copy of your data and save the original data as your raw, unaltered data set. You will want to make sure that the file name reflects that it is your raw data.\nCreate a separate file that you will then clean, make sure that your filename includes some sort of versioning and/or a date so you have a good record of when you processed a data set. Then you want to make sure that your data are all values and not formulas which refer to specific cells. Once you start moving cells around this can screw with your data.\nYou will also want to create a text-file (a typical filename would be README) that keeps track of all your files and manipulations so that future you or a collaborator can easily understand and replicate any steps that you take.\n\n\nThe goal of QC is to find erroneous data. This means that it is generally going to stick out from the rest of the values in a specific column8.8 Errors are not the same as outliers. Sometimes you know that e.g. certain values cannot be true, for example if all your sample locations where in the northern hemisphere then you cannot have latitudes from the southern hemisphere.\nWe can generally make the assumption that the vast majority of your data is correct. This means that if we sort the values in a column if there are a few errors they will stick out and in many cases they will sort at the very top or very bottom. For example, if your column is numeric anything that is a character will pop out or if you have null values or empty cells they will generally sort to the bottom of a column.\n\n\n\n\n\n\nPay Attention\n\n\n\nAny time you are going to sort data, make sure that you are sorting the entire data set not just a single column or you will corrupt your data set and everything will end up scrambled.\nGenerally, if you don’t have any empty columns or too much missing data if you place the cursor in a cell with a value you can use the shortcut Ctrl/Cmd + A to select all.\nAlways double check that you have expanded your sort to the entire data set\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nPick out some erroneous data:\n\nIn the scratch folder, open up the catchdata_messy Google Sheet. Then sort each column and see which errors you can spot. Make the entire data set is highlighted.\nGo select Data &gt; Sort range &gt; Advanced sorting options. Make sure you check the Data has header row option.\nUse the sort by drop down menu to select the column you want to sort. Once you are ready, click sort.\ninspect your column to determine if there are unexpected values and describe your observations.\n\nDiscuss what you will do with you findings with your lab mates. Consider whether it is better (more ethical/responsible) to remove them or correct them.\n\n\nSimilarly, we can use conditional formatting which allows you to apply specific rules for automatically color coding to a column based on specific rules. This makes it easier for unusual entries or entries outside the possible boundaries to stand out.\n\n\n\n\n\n\nGive it a try\n\n\n\nApply conditional formatting to identify unusual entries:\n\nIn the catchdata_messy Google Sheet, highlight the STL column. The select Format &gt; Conditional formatting from the toolbar. This will pull up a dialog on the right hand of your window.\nSimilar to the Data Validation dialog, you can select the range you want to apply this rule to either by typing it in or selecting it directly in the spreadsheet.\nClick on the Format rules dropdown menus and look through the available options. Let’s say that we know that the sharks cannot be smaller than 50cm or larger than 2m. Set up rules for conditional highlighting that will allow you to quickly pull out invalid entries.\nClick Done once you are all set and evaluate your results.\n\nDiscuss with your lab mates whether this is a better option compared to sorting."
  },
  {
    "objectID": "A02_intro-spreadsheets.html#sec-exporting-data",
    "href": "A02_intro-spreadsheets.html#sec-exporting-data",
    "title": "2  Introduction to Data Management using spreadsheets",
    "section": "2.5 Exporting data",
    "text": "2.5 Exporting data\nGenerally, want to make sure that we are storing our data in a universally accessible, open, and static format rather than e.g. the default Excel file format (*.xls or *.xlsx).\n\nExcel files have a proprietary format and it is possible that in the future technology will change and you will no longer be able to access your files.\nOther program may not be able to read Excel formatted files.\nDifferent version of Excel may handle data differently which can lead to inconsistencies.\nFrequently journals or grant agencies require you to deposit your data in a data repository that only accepts certain formats which may not include Excel.\n\n\n\n\n\n\n\nConsider this\n\n\n\nDiscuss whether you think Google Sheets has the same problems or if it is an acceptable format to avoid these issues.\n\n\nText-based formats such as comma-delimited (*.csv) or tab-delimited (*.txt or *.txv) files overcome these issues. In CSV files, columns are separated by commas and in tab-delimited files by tabs9. The advantage of text files is that they can be opened in any plain text editors10 but you can also import them into spreadsheet programs or command-line programs like R.9 This will look like whitespace if you look at it in a text editor, but tabs, but using whitespace can cause issues when command-line files parse them, tabs are less ambiguous10 Your operating system will have a built in plain text editors such as notepad. However, you are regularly operating with text files it can be helpful to have a more powerful program like Notepad++ or Atom.\n\n\n\n\n\n\nProtip\n\n\n\nYou will be generating and entering data this semester that you will need to be able to download as a text file and move into your data folder for a project you are working on.\nGoogle Sheets now make it a lot easier to export and download copies of spreadsheets in different formats including *.csv by selecting File &gt; Download from the main toolbar and the choosing comma-delimited or tab-delimited from the drop down menu."
  },
  {
    "objectID": "A03_intro-R.html#sec-install--set-up-r-and-rstudio-on-your-computer",
    "href": "A03_intro-R.html#sec-install--set-up-r-and-rstudio-on-your-computer",
    "title": "3  Introduction to R and Rstudio",
    "section": "3.1 Install & Set up R and Rstudio on your computer",
    "text": "3.1 Install & Set up R and Rstudio on your computer\nIf you have already installed R and Rstudio on your laptop, make sure your R version is up to date. Whenever you open Rstudio the version will be printed in the console. In addition, you can always check what version is installed by typing sessionInfo() into your console. You should be using version 4.0.0 or later. You do not need to uninstall old version of R. If you do have to update, you will need to re-install packages (see below) for R 4.0.0\n\n3.1.1 Windows\nInstall R\n\nDownload most recent version of R for Windows here.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\nInstall Rstudio\n\nGo to Rstudio download page.\nScroll down to select the Rstudio current version for Windows XP/Vista/7/8/10.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard.\n\n\n\n3.1.2 Mac OS X\nDownload & install R\n\nGo to (CRAN)[http://cran.r-project.org/], select Download R for (Mac) OS X.\nDownload the .pkg file for your OS X version.\nRun the downloaded file to install R.\n\nDownload & install Rstudio\n\nGo to Rstudio download page.\nScroll down to select the Rstudio current version for Mac OS X.\nRun the .exe file that was downloaded and follow instructions in the set-up wizard."
  },
  {
    "objectID": "A03_intro-R.html#sec-get-to-know-rstudio",
    "href": "A03_intro-R.html#sec-get-to-know-rstudio",
    "title": "3  Introduction to R and Rstudio",
    "section": "3.2 Get to know Rstudio",
    "text": "3.2 Get to know Rstudio\nRstudio is an Integrated Development Environment (IDE) that you can use to write code, navigate files, inspect objects, etc. The advantage of using an IDE is that you have access to shortcuts, visual cues, troubleshooting, navigation, and auto complete help.\n\n3.2.1 GUI Layout\nGUI stands for graphic user interface and refers to a type of user interface that allows users to interact with software applications and electronic devices through visual elements such as icons, buttons, windows, and menus, rather than using text-based command-line interfaces.\nYou have probably mostly interacted with computer programs through a GUI, where you manipulate graphical elements using a pointing device like a mouse, touch screen, or stylus. GUIs provide a more intuitive and user-friendly way for individuals to interact with computers and software because you can “see” what the effect of what you are doing is having. GUIs are a major departure from earlier text-based interfaces like command-line interfaces. They have contributed significantly to the widespread adoption of computers and software by making them more accessible to a broader range of users. GUIs are used in various types of software, from operating systems to applications like web browsers, image editors, word processors, and more.\nNot too long ago, if you had wanted to learn R or another programming language you would have been working directly on a console instead of an IDE like Rstudio which has made coding a lot more accessible to beginners because you can more easily use scripts, interactively run code and visualize data.\n\n\n\n\n\n\nProtip\n\n\n\nUse this link to access an Rstudio IDE Cheatsheet pointing out the key features using annotated impages of the different panes. You can also download a pdf version and keep a printout handy as you get used to the GUI.\n\n\nOpen Rstudio and identify the four panes in the interface (default layout).\n\nEditor (top left): Edit scripts/other documents, code can be sent directly to the console.\nR console (bottom left): Run code either by directly typing the code or sending it from the editor pane.\nEnvironment/History (top right): Contains variables/objects as you create them & full history of functions/commands that have been run.\nFiles/Plots/Packages/Help/Viewer (bottom right): Different tabs in this pane will let you explore files on your computer, view plots, loaded packages, and read manual pages for various functions.\n\nThe panes can be customized (View -&gt; Panes -&gt; Pane Layout) and you can move/re-size them using your mouse.\n\n\n\n\n\n\nPay Attention\n\n\n\nWe are going to switch to have the Console in our top right and the Environment in the bottom left which makes it easier to see your code output and your script/quarto document at the same time.\nThe easiest way to do this is to go to View -&gt; Panes -&gt; Console on Right.\n\n\nBefore we move on, let’s look at two easy ways to navigate longer documents and also communicate with others where we are in the document if we need help trouble shooting.\nFirst, take a look at the top of the Viewer Pane. There should be a button labeled Outline. You can toggle that on and off to show and outline of the headers used in a document. Using headers helps you structure your document into logical parts - it also means that you can jump to different sections.\nSecond, look at the bottom left of your Viewer pane. You should see a little orange square with a # in it, if you are reading along in your quarto document it will currently say GUI Layout next to the orange #. If you click on it, it will give you a menu where you can select not only sections based on the headers, you will also see all the code chunks numbered. If they have been given a name using the label code chunk option you will be able to see that as well.\n\n\n3.2.2 Interacting with R in Rstudio\nThink of R as a language that allows you to give your computer precise instructions (code) to follow.\n\nCommands are the instructions we are giving the computer, usually as a series of functions.\nExecuting code or a program means you are telling the computer to run it.\n\nThere are three main ways to interact with R - directly using console, script files (*.R), or code chunks embedded in R markdown (*.Rmd) or quarto files (*.qmd). We will generally be working with quarto documents this semester.\nThe console is where you execute code and see the results of those commands1. You can type your code directly into the console and hit Enter to execute it. You can review those commands in the history pane (or by saving the history) but if you close the session and don’t save the history to file those commands will be forgotten.1 You can think of the console as a super-powered calculator\nBy contrast, writing your code in the script editor either as a standard script or as a code chunk in an quarto document allows you to have a reproducible workflow (future you and other collaborators will thank you).\nExecuting an entire script, a code chunk, or individual functions from a script will run them in the console.\n\nCtrl + Enter will execute commands directly from the script editor or a code chunk. You can use this to run the line of code your cursor is currently in in the script editor or you can highlight a series of lines to execute.\nIf you are using a quarto file you can execute an entire code chunk by pressing the green arrow in the top right corner.\n\nWe will run through these options, but you can always check back here while you are getting used to R.\n\n\n\n\n\n\nProtip\n\n\n\nIf the console is ready for you to execute commands you should see a &gt; prompt. If you e.g. forget a ) you will see a + prompt - R is telling you that it is expecting further code. When this happens and you don’t know what you are missing (usually it is an unmatched quotation or parenthesis), make sure your cursor is in the console and hit the Esc key.\n\n\nFor each of our units we will have a project folder2 with an Rproject, *.qmd-files to complete lab assignments and write your lab report, along with sub-directories to hold the data and results that you will generate.2 We will use “directory” and “folder” synonymously throughout this lab handbook\nUsing Rprojects allows us to set the working directory to the folder you are currently working out of which means that for everyone the file paths will be the same. You can open an Rproj file by double clicking it in a file manager which will then open an instance of Rstudio. Alternatively, you can use the Project Icon in the top right corner of the Rstudio IDE to open an existing Rproject. If you look in the bottom left hand pane in the Files tab, the bread crumbs should lead to your project folder which has now become your working directory, i.e. all paths are relative to this location. If you navigate away from your working directory (project directory) you can quickly get back to your project directory by clicking on the project icon in the Files pane or by clicking the cog icon (More) and selecting Go to Working Directory.\n\n\n\n\n\n\nPay Attention\n\n\n\nAlways make sure that you are in the correct Rproject.\nWe solve about 50% of problems with error messages about “files not being found” and things not running as they should by making sure that you are working out of the correct project folder and loaded Rproject. You can always check by looking if the name of your Rproject is next to the Rproject icon in the top right corner of Rstudio.\n\n\nWe are going to be using quarto documents such as the one you are currently working in throughout this semester.\nAn qmd-file consists of three components:\n\nHeader: written in YAML format the header contains all the information on how to render the .qmd file.\nMarkdown Sections: written in Rmarkdown syntax.\nCode chunks: Chunks of R code (or other code such as bash, python, …). These can be run interactively while generating your document and will be rendered when knitting the document.\n\nRstudio now also has a WYSIWYG3 visual editor that will allow you to interact with quarto documents similar to the way you use a word processor to get bold, italics and similar formatting, so you do not need to learn how to write in Rmarkdown.3 What you see is what you get\nYou can use the render button in the editing pane to convert your quarto document to a wide range of formats including Word, PDF, and html.\n\n\n3.2.3 Customize Rstudio\nThere are several options to customize Rstudio including setting a theme, and other formatting preferences. You can access this using Tools &gt; Global Options. I recommend using a dark theme (it’s a lot easier on the eyes) and keeping the panes in the same positions outlined above because it will make troubleshooting a lot easier4.4 “You should see xx in the top left” is a lot more helpful if your top left looks like my top left!"
  },
  {
    "objectID": "A03_intro-R.html#sec-installing-and-using-packages-in-r",
    "href": "A03_intro-R.html#sec-installing-and-using-packages-in-r",
    "title": "3  Introduction to R and Rstudio",
    "section": "3.3 Installing and using packages in R",
    "text": "3.3 Installing and using packages in R\n\n3.3.1 Install a package\nThink of R packages or libraries as tool kit comprising a set of functions (tools) to perform specific tasks. R comes with a set of packages already installed that gives you base R functions; you can view these and determine which have been loaded in the Packages tab in the bottom right pane. For other tasks we will need additional packages. 55 Most R packages are found in the CRAN repository and on Bioconducter, developmental packages are available on github.\nA central group of packages for data wrangling and processing form the tidyverse, described as “… an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.” - We are going to heavily rely on core functions from the tidyverse to wrangle, summarize, visualize and analyze data.\nWhen you install packages they will be downloaded and installed onto your computer. Determine what your default path is using .libPaths() and change if necessary.\nThe easiest way to install packages directly in the console is to use the install.packages() function.\nExecute the code chunk below to install some libraries to get us started6 by placing your cursor somewhere in the code and hitting Ctrl + Enter or by clicking on the green arrow in the top right corner of the code chunk.6 We will install other libraries as needed down the line.\n\n# install central packages in the tidyverse\ninstall.packages(\"tidyverse\", \n                 \"janitor\", \n                 \"glue\",\n                 \"here\",\n                 \"tibble\",\n                 \"ggthemes\", \n                 \"knitr\",\n                 \"tidymodels\")\n\n\n\n\n\n\n\nProtip\n\n\n\nYou can also install packages using the Packages tab in the bottom right pane. Select the Packages tab and then click on the Install button to pull up a dialogue box. Type the packages you want to install into the Packages box and confirm using Install.\n\n\nLet’s check if you were able to successfully install those packages by ensuring you can load them. Any time you start a new R session (e.g. by closing Rstudio and restarting it), you will need to load your libraries beyond the base libraries that are automatically loaded using the library() function in order to be able to use the functions specific to that package7.7 Troubleshooting tip: if you get an error along the lines of function() cannot be found the first thing you will want to do is check if your libraries are loaded!\n\n# load library\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.1\n\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIf you don’t see any error messages in the console along the lines of there is no package called ... you are all set. If you look in the packages tab in the lower right panel you should also see that packages such as dplyr and tidyr (two of the central tidyverse packages) now have a little check box next to them.\n\n\n3.3.2 Updating R packages\nYou should generally make sure to keep your R packages up to date as new versions include important bugfixes and additional improvements. The easiest way to update packages is to use the Update button in the Packages tab in the bottom right panel. Over the course of the semester you should not have to do this, but when you install new packages you might get message that some of your packages need to be updated which you can then either choose to do at that point or ignore.\n\n\n\n\n\n\nPay Attention\n\n\n\nBe aware that updating packages might break some code you have previously written. For most of what we will be doing this should not be the case. If you used R for a previous course, make sure to update you packages at the beginning of this course and we should be set for the semester."
  },
  {
    "objectID": "A03_intro-R.html#sec-r-is-all-about-objects",
    "href": "A03_intro-R.html#sec-r-is-all-about-objects",
    "title": "3  Introduction to R and Rstudio",
    "section": "3.4 R is all about Objects",
    "text": "3.4 R is all about Objects\nYou can think of the R console as a super-powerful calculator.\nYou can get output from R by simply typing math directly into the console.\n\n13 + 29\n\n[1] 42\n\n\nor\n\n546 / 13\n\n[1] 42\n\n\nWell that’s fun - but not super helpful in our context.\nIn the R programming language, an object is a fundamental concept used to store, manipulate, and represent data. Everything in R is treated as an object, whether it’s a number (numeric), a text string (character), a data set (data.frame), or even more complex data structures.\nObjects in R can be created, modified, and used to perform various operations. Objects are assigned names that you can then use to reference them in your code. When you create an object, you’re essentially creating a container that holds a value or data.\nCreating an object is straightforward. First, we give it a name, then we use the assignment operator to assign it a value. The assignment operator (&lt;-) assigns the value on the right of the &lt;- to the object on the left8.8 Start building good habits starting now in terms of your coding style. For example, your code is a lot more readable if you use white space to your advantage. For example, make sure you have a space before and after your &lt;-\nExecute the code in the code chunk below by placing your cursor somewhere in the line of code and hitting Ctrl + Enter or by clicking on the little green arrow on the right hand side of the code chunk.\n\n# create object\nlength_mm &lt;- 344\n\nIf you look at your Global Environment (bottom left panel) you should now see length and the value you assigned it.\nNotice, how when you assigned a value to your new object nothing was printed in the console compared to when you were typing in math.\nTo print the value of an object you can type the name of the object into the console.\n\n# print value in the console\nlength_mm\n\n[1] 344\n\n\nNow that length is in your environment we can use it to compute instead of the value itself.\nFor example, we might need to convert our length from millimeters (mm) to centimeters (cm).\n\n# divide value of object by 10\nlength_mm / 10 \n\n[1] 34.4\n\n\nWe can change the value of an object any time by assigning it a new one. Changing the value of one object does not change the values of other objects.\n\n# assign new value\nlength_mm &lt;- 567\n\n\n\n\n\n\n\nGive it a try\n\n\n\nCreate a new object called length_cm with the length in centimeters. Then change the value of our object with the length in millimeters to 50. What do you think the value of length_mm will be now?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n# create object with length in cm\nlength_cm &lt;- length_mm / 10\n\n# change value\nlength_mm &lt;- 50\n\nYou should see that only the length_mm variable has changed but not the length_cm object. Those are completely independent from each other even though you initially used the length_mm object to create the other one.\n\n\n\n\n\n\n\n\n\nProtip\n\n\n\nTheoretically, we can name objects anything we want - but before that gets out of hand let’s think about some guidelines for naming objects.\n\nMake them simple, specific, and not too long (otherwise you will end up with a lot of typing to do and difficulties remembering which object is which).\nObject names cannot start with a number.\nR is case sensitive, length_mm is not the same as Length_mm.\nAvoid using dots (.) in names. Typically dots are used in function names and also have special meaning (methods) in R.\nSome names are already taken by fundamental functions (e.g. if, else, for) and cannot be used as names for objects; in general avoid using names that have already been used by other function names.\nRule of thumb: nouns for object names, verbs for function names.\n\nThis semester you will mostly execute code already written for you or make minimal modifications. However, be observant of the coding style and mimic it to develop good practices. Using a consistent style for naming your objects is part of adopting a consistent styling of your code; this includes things like spacing, how you name objects, and upper/lower case. Clean, consistent code will make following your code a lot easier for yourself and others."
  },
  {
    "objectID": "A03_intro-R.html#sec-using-comments",
    "href": "A03_intro-R.html#sec-using-comments",
    "title": "3  Introduction to R and Rstudio",
    "section": "3.5 Using comments",
    "text": "3.5 Using comments\nYou can add comments to your R scripts using #. Essentially, once you type an # in a line anything to the right of it will be ignored.\nThis is really helpful as it will allow you to comment your script, i.e. you can leave notes and explanations as to what your code is doing for future you and for other collaborators. This is especially helpful if you come back to some of your code after a period of time, if you are sharing your code with others, and when you are debugging code. You will find that as you become more experienced your comments will become shorter and more concise and you might even be tempted to leave them out completely - don’t9!9 To help you build a habit of good commenting practice, commenting your code is a requirement for your homework assignment and skills tests.\nYou can add comments above or next to a line of code. For detailed comments you may want to include multiple lines of comments but you will need to add a # for every line of comment.\n\n\n\n\n\n\nGive it a try\n\n\n\nExecute this code line by line by placing your cursor above the first comment and hitting Ctrl + Enter and compare differences in behavior.\n\n# total length fish \nlength_mm &lt;- 436  \n\nlength_mm &lt;- 436  # total length fish  \n\n# total length of fish \n# this measurement is in millimeters \nlength_mm &lt;- 436\n\n\n\n\n\n\n\n\n\nProtip\n\n\n\nYou can comment/uncomment multiple lines at once by highlighting the lines you want to comment (or uncomment) and hitting Ctrl + Shift + C. This can be useful if you are playing around with code and don’t want to delete something but don’t want it to be run either."
  },
  {
    "objectID": "A03_intro-R.html#sec-functions",
    "href": "A03_intro-R.html#sec-functions",
    "title": "3  Introduction to R and Rstudio",
    "section": "3.6 Functions",
    "text": "3.6 Functions\nWhen we installed R packages earlier we mentioned that they comprise sets of predefined functions. These are essentially mini-scripts that automate using specific sets of commands. So instead of having to run multiple lines of code (this can be 10s - 100s of lines code) you call the function instead.\nEach function usually requires multiple inputs (arguments) and once executed will generally return a value .\nFor example the function round() can be used to round a number10.10 This is an excellent example of naming things well!\n\n# create object with rounded number\nlength_cm &lt;- round(34.8821)\n\nIf we print the value of our object to the console, we see the following value is returned.\n\n# call vector\nlength_cm\n\n[1] 35\n\n\nFor this function the input (argument) is a number and the returned value is also a number. This is not always the case, arguments can be numbers, objects, file paths …\nMany functions have set of arguments that alter the way a function operates - these are referred to as options. Generally, they have a default value which are used unless specified otherwise by the user.\nYou can determine the arguments as function by calling the function args().\n\n# query arguments\nargs(round)\n\nfunction (x, digits = 0) \nNULL\n\n\nOr you can call up the help page using ?round or by typing it into the search box in the Help tab in the lower right panel.\nFor example, our round() function has an argument called digits, we can use this to specify the number of significant digits we want our rounded value to have.\n\n# round value to two digits\nround(34.8821, digits = 2)\n\n[1] 34.88\n\n\n\n\n\n\n\n\nProtip\n\n\n\nGood code style is to put the non-optional arguments (frequently the object, file path or value you are using) first and then specify the names of all the optional arguments you are specifying. This provides clarity and makes it easier for yourself and others to follow your code.\n\n\nOccasionally you might even want to use comments to further specify what each argument is doing or why you are choosing a specific option.\n\nround(34.8821,     # number to round\n      digits = 2)  # specify number of significant digits\n\n[1] 34.88"
  },
  {
    "objectID": "A03_intro-R.html#sec-data-types-i-vectors",
    "href": "A03_intro-R.html#sec-data-types-i-vectors",
    "title": "3  Introduction to R and Rstudio",
    "section": "3.7 Data Types I: Vectors",
    "text": "3.7 Data Types I: Vectors\nNow that we’ve figured out what objects and functions are let’s get to know the two data types we will be spending the most time with this semester - vectors and data frames (data.frame)11.11 Other data types include lists (list), factors (factor) matrices (matrix), and arrays (array).\nThe most simple data type in R is the (atomic) vector which is a linear vector of a single type. There are six main types -\n\ncharacter: strings or words.\nnumeric or double: numbers.\ninteger: integer numbers (usually indicated as 2L to distinguish from numeric).\nlogical: TRUE or FALSE (i.e. boolean data type).\ncomplex: complex numbers with real and imaginary parts (we’ll leave it at that).\nraw: bitstreams (we won’t use those either).\n\nYou can check the data type of any object using class().\n\n# query class of object\nclass(length_mm)\n\n[1] \"numeric\"\n\n\nCurrently, our length_mm object consists of a single value. The function c() (concatenate) will allow us to assign a series of values to an object.\n\n# create numerical vector\nlength_mm &lt;- c(454, 234, 948, 201)\n\n# print to console\nlength_mm\n\n[1] 454 234 948 201\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nPredict what data type you expect this vector to be.\n\n\nWe call the same function to create a character vector.\n\n# create character vector\nspecies &lt;- c(\"Adelie\", \"Gentoo\", \"Chinstrap\")\n\n# query class\nclass(species)\n\n[1] \"character\"\n\n\nThe quotes around \"Adelie\" etc. are essential because they indicate that this is a character.\n\n\n\n\n\n\nProtip\n\n\n\nIf we do not use quotes, R will assume that we are trying to call an object and you will get an error code along the lines of “! object 'Adelie' not found”.\n\n\nYou can use c() to combine an existing object with additional elements (assuming they are the same data type).\n\n# add element to vector\nspecies &lt;- c(species, \"Emperor\")\n\n# call vector\nspecies\n\n[1] \"Adelie\"    \"Gentoo\"    \"Chinstrap\" \"Emperor\""
  },
  {
    "objectID": "A03_intro-R.html#sec-data-types-ii-data-frames",
    "href": "A03_intro-R.html#sec-data-types-ii-data-frames",
    "title": "3  Introduction to R and Rstudio",
    "section": "3.8 Data Types II: Data frames",
    "text": "3.8 Data Types II: Data frames\nRecall that atomic vectors are linear vectors of a simple type, essentially they are one dimensional. Frequently we will be using data frames (data.frame) which you can think of as consisting of several vectors of the same length where each vector becomes a column and the elements are the rows.\nLet’s create a new object that is a data.frame with three columns containing information on species and length in millimeters.\n\n# combine vectors into data frame\ndf &lt;- data.frame(species, length_mm)\n\nYou should now see a new object in your Global Environment and you will now also see that there are two categories of objects Data and Values. You will see that the data.frame is described as having 3 obs (observations, those are your rows) of 2 variables (those are your columns). If you click on the little blue arrow it will give you additional information on each column - note that because each column is essentially a vector, each one must consist of a single data type which is also indicated.\nYou can further inspect the data.frame by clicking on the little white box on the right which will open a tab in the top left panel next to your R script. You can also always view a data.frame by calling the View() function.\n\n# view data frame in View Panes\nView(df)\n\nThis can be a helpful way to explore your data.frame, for example, clicking on the headers will sort the data frame by that column. Usually we won’t build or data.frames by hand, rather we will read them in from e.g. a tab-delimited text file - but more on that later."
  },
  {
    "objectID": "A01_intro-exp-design.html#the-scientific-method",
    "href": "A01_intro-exp-design.html#the-scientific-method",
    "title": "1  Scientific Method & Experimental Design",
    "section": "1.1 The scientific method",
    "text": "1.1 The scientific method"
  },
  {
    "objectID": "A01_intro-exp-design.html#controlled-experiments",
    "href": "A01_intro-exp-design.html#controlled-experiments",
    "title": "1  Scientific Method & Experimental Design",
    "section": "1.2 Controlled Experiments",
    "text": "1.2 Controlled Experiments"
  },
  {
    "objectID": "A01_intro-exp-design.html#observational-experiments",
    "href": "A01_intro-exp-design.html#observational-experiments",
    "title": "1  Scientific Method & Experimental Design",
    "section": "1.3 Observational Experiments",
    "text": "1.3 Observational Experiments"
  },
  {
    "objectID": "A04_descript-stats.html#learning-objectives",
    "href": "A04_descript-stats.html#learning-objectives",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.1 Learning Objectives",
    "text": "4.1 Learning Objectives\nAfter completing this activity, you should be able to\n\nDefine descriptive statistics and describe its role in summarizing and describing data sets.\nVisualize counts of categorical data using bar plots and explain how to interpret them.\nVisualize distributions of continuous data using histograms and explain how to interpret them.\nRecognize and explain how to calculate measures of central tendency, including mean, median, and mode, and understand when each is appropriate to use.\nRecognize and explain how to calculate measures of variability, including range, variance, and explain how they describe the spread of data.\nUse R to calculate standard descriptive statistics.\n\nQuantitative research is a core component of the natural sciences, including biology. Here, we collect and analyze numerical data1 to find patterns and averages, test causal relationships, generalize results based on a specific sample to the whole population and be able to make predictions beyond that specific case. Over the last few decades the availability of computational power and the ability to generate increasingly large data sets2 means that arguably all biology has become computational biology and your ability to apply statistics is increasingly essential to pursue a career in most biological fields including genetics, ecology, animal behavior, environmental science etc. This semester as you design different research projects you will learn how analyze data using R.1 data is plural, a single numerical observation would be a datum2 This is why we frequently also call this the “Era of Big Data”.\nAfter you have performed an observational or controlled experiment and compiled your individual observations into a data set, your next step will be to analyze that data and draw conclusions from the data using statistics. The goal of descriptive statistics is to summarize and describe the characteristics of a data set. By contrast, inferential statistics are applied to allow you to make generalizations about the population based on your sample or to test a specific hypothesis."
  },
  {
    "objectID": "A04_descript-stats.html#organizing-and-summarizing-raw-data-using-descriptive-statistics",
    "href": "A04_descript-stats.html#organizing-and-summarizing-raw-data-using-descriptive-statistics",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.2 Organizing and summarizing raw data using descriptive statistics",
    "text": "4.2 Organizing and summarizing raw data using descriptive statistics\nTo learn how to use R to generate descriptive statistics we are going to explore a data set collected from 2007 to 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program which is part of the US Long Term Ecological Research Network (Gorman, Williams, and Fraser 2014). We are going to use a curated version3 of the data set.3 This means that the data wrangling has already been done for us and we can directly access a tidy data set.\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3). https://doi.org/10.1371/journal.pone.0090081.\nExecuting the following code chunk4 to install an R package holding the Palmer Penguins data set.4 You will only have to do this once. Note that the eval option of your code chunk is set to false so that when you render your document it will not execute this code chunk\n\ninstall.packages(\"palmerpenguins\")\n\nNow we can get started by loading several R libraries, including the palmerpenguins library, that contain the functions we will need to explore this data set\n\n\n\nLet’s take a quick look at what variables the data set contains using the function glimpse().\n\n# overview of rows and columns\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nYou can also use the function View() to open the data.frame in the Viewer Pane.\n\n# open dataframe in Viewer\nView(penguins)\n\nWe generally differentiate between two different types of variables:\n\nContinuous variables are numeric variables that can take any value within a given range; they are typically measured on interval or ratio scales5, allowing for meaningful mathematical operations like addition and subtraction.\nCategorical variables represent categories or groups and can take on a limited, distinct set of values; they are either measured on a nominal scale6 or an ordinal scale7.\n\n5 These are both types quantitative scales: An interval scale is a measurement scale where the intervals between consecutive values are equal, but the scale lacks a true zero point. A ratio scale has the same properties but also a true zero point indicating the absence of the measured quantity making the ratios of measurements meaningful.6 Categories are assigned labels with no inherent order or ranking7 There is a meaningful order to the categories.\n\n\n\n\n\nConsider this\n\n\n\nTake a look at the data set in the Viewer Pane and get an overview of what information it contains.\n\nHow many different species were included?\nOn how many islands where observations made?\nWhen were the observations made?\nWhat biological measurements were taken?\nHow many individual observations are there?\n\nIndicate which of these variables are continuous and which are categorical variables.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can see that we observations of three different species of penguins (Adelie, Chinstrap, Gentoo) on three different islands in the Antarctica (Togersen, Dream, Biscoe). These are all categorical variables.\nThe biological measurements include sex (categorical variable) as well as several continuous measurements:\n\nbody mass\nflipper length\nbeak measurements (depth, length)\n\n\n\n\nAs you probably noticed having to scroll through raw data8 on 344 individuals that were observed, it is difficult to get much information on general patterns. You were probably able to scroll through and figure out how many different islands were observed and what species were counted and that both males and females were observed - but would you be able to make a statement about whether some species are more common on some islands compared to others? Whether there are more males or females in the data set? If there are differences in size among species? Among sexes within species? What a typical beak size looks like?8 Unprocessed data comprising all the individual observations.\nGenerally our first step in analyzing a data set would be to apply descriptive statistics to summarize the data and identify the following three characteristics\n\nFrequency Distribution\nMeasures of central tendency\n\nmean\nmedian\nmode\n\nMeasures of variability\n\nrange\nstandard deviation\nvariance\ninterquartile range\n\n\nYou can apply these statistics to assess only one variable at a time (univariate analysis) or to compare two (bivariate analysis) or more (multivariate analysis) variables.\nAfter we have calculated our descriptive statistics we will generally visualize them as either a table or graph. A table allows us to organize values into columns and rows. While we frequently gather and organize our raw data into tables, any tables you are presenting in a report should be summarized data. Generally, we reserve tables for complex data that is not easily visualized or include them in an appendix for anyone who needs very specific details. Instead whenever possible we translate tables into a graph or plot which makes it easier to show the relationship between variables and identify trends and patterns, the most commonly used graphs are scatter plots, line plots, box plots, bar plots, and histograms9.9 Today you will use bar plots and histograms to visualize summary statistics."
  },
  {
    "objectID": "A04_descript-stats.html#frequency-distribution",
    "href": "A04_descript-stats.html#frequency-distribution",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.3 Frequency Distribution",
    "text": "4.3 Frequency Distribution\n\n4.3.1 Simple frequency distribution\nEvery data set is made up of a distribution of values. We can summarize the frequency of every possible value of a variable using either counts or by indicating the proportion of all observations that take on a specific value.\nFor example we might want to know how many individuals of each species where observed in the data set. To do this we would create a frequency distribution table that lists every possible value or measurement (in this case the species) and the how many times each value was observed.\nWe can have R do the work for us using the function count()and specifying which column the number of observations per value should be counted for10.10 You will frequently see that we link lines of codes using %&gt;% which is called a “pipe”. You can interpret this as you telling R “and now do this”. The reason we call this piping is tha the first line of code creates and output and then we are saying “take this output and now make that the input and do this” as if we were creating a giant marble run.\n\n# count number of individual per species\npenguins %&gt;%\n  count(species) %&gt;%\n  kable()\n\n\n\n\nspecies\nn\n\n\n\n\nAdelie\n152\n\n\nChinstrap\n68\n\n\nGentoo\n124\n\n\n\n\n\nFrequently, it is more helpful to compare relative proportions. We can do this by creating a new column using the function mutate(). Then, we give it the column name for the column we want to create and define how the content for each row should be calculated based on existing values. In this case, we divide the content of column n (which contains the counts) by the number of individuals observed by the total number of all penguins in the data set which we can calculate as sum(n).\n\n# create frequency table\npenguins %&gt;%\n  count(species) %&gt;%                 # count number of observations\n  mutate(percent = n/sum(n)*100) %&gt;% # calculate percent\n  kable(digits = 1)                  # print table with 1 digit\n\n\n\n\nspecies\nn\npercent\n\n\n\n\nAdelie\n152\n44.2\n\n\nChinstrap\n68\n19.8\n\n\nGentoo\n124\n36.0\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nDescribe your results and consider what this tells you about penguins in the Antarctica.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis gives us some insight into fundamental patterns of species richness and diversity. We know there are three species observed (richness) and the relative abundance (Adelie penguins are the most commonly at 44% while Chinstrap penguins are the most rare at 19%).\n\n\n\nDetermining the number of observations and relative proportions is an important descriptive statistic for any categorical variable.\n\n\n\n\n\n\nGive it a try\n\n\n\nModify/complete the code below to create a frequency table that shows the counts and proportion of males and females in the data set. Describe your results and consider what this tells you about penguins in the Antarctica.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\npenguins %&gt;%\n  count(sex) %&gt;%\n  mutate(percent = n/sum(n)*100) %&gt;%\n  kable(digits = 1)\n\n\n\n\nsex\nn\npercent\n\n\n\n\nfemale\n165\n48.0\n\n\nmale\n168\n48.8\n\n\nNA\n11\n3.2\n\n\n\n\n\nWe see that we have a small number of individuals in the data set for which sex was not determined (NA). More importantly, we see that overall, the penguin populations comprise approximately the same amount of males and females.\nAnother option would be to count the number of individuals per male and female within each species.\n\npenguins %&gt;%\n  count(species, sex) %&gt;%\n  mutate(percent = n/sum(n)*100) %&gt;%\n  kable(digits = 1)\n\n\n\n\nspecies\nsex\nn\npercent\n\n\n\n\nAdelie\nfemale\n73\n21.2\n\n\nAdelie\nmale\n73\n21.2\n\n\nAdelie\nNA\n6\n1.7\n\n\nChinstrap\nfemale\n34\n9.9\n\n\nChinstrap\nmale\n34\n9.9\n\n\nGentoo\nfemale\n58\n16.9\n\n\nGentoo\nmale\n61\n17.7\n\n\nGentoo\nNA\n5\n1.5\n\n\n\n\n\nThis gives us the additional insight that this pattern of equal number of male and females is consistent across all three species.\nCaveat: We don’t know if the the sampling scheme was designed to be random in respect to sex and thus represent the proportions in the species, or if it was more important to include similar numbers of males/females to be able to compare differences in the other metrics in the data set among sexes.\n\n\n\nSo far, we have been looking at frequencies of occurrences across the entire data set, however, in our case we have observations from three different islands. One question we probably have is whether the distribution of different penguin species is consistent across the different locations.\nOne of the advantages of using R is that we can add just one additional line of code to group our data by island and then count the number of observations per island.\n\npenguins %&gt;%\n  group_by(island) %&gt;%               # group data by island\n  count(species) %&gt;%                 # count number of observations in each subset\n  pivot_wider(names_from = species,  # create one column per species\n              values_from = n) %&gt;%   # add counts per island per island/species\n  kable()\n\n\n\n\nisland\nAdelie\nGentoo\nChinstrap\n\n\n\n\nBiscoe\n44\n124\nNA\n\n\nDream\n56\nNA\n68\n\n\nTorgersen\n52\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nDescribe your results and consider what this tells you about penguins in the Antarctica.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAgain, just by looking at simple descriptive stats comparing distribution of observations across these categories we have gained a lot of insight and we now see that the islands are quite different in terms of the which species are present. Adelie penguins are observed on every island, while Gentoo and Chinstrap penguins never co-occur, they are each observed on one island each, in which case they are the more common species compared to Adelie penguins.\n\n\n\nBoth species and sex are categorical variables, so if we wanted to visualize the distribution we would typically use a bar chart where we plot the different categories of a value on the x axis and then scale the height of each bar to correspond to the number of individuals observed.\nWe can do this using the function ggplot()11.11 We aren’t going to go into detail how to customize figures using ggplot, but you can use the comments to see what modifications you would be able to make to e.g. change color coding etc.\n\n\n\n\n\n\nGive it a try\n\n\n\nAdd a descriptive figure caption and x/y-axis titles12.\n\nggplot(penguins, aes(x = species)) +  # specify data frame and column name to plot on x-axis   \n  geom_bar(stat = \"count\",            # define chart type as bar plot            \n           color = \"black\",           # define color of bar lines            \n           fill = \"darkorange\") +     # define color of bars   \n  labs(x = \"species\",                 # x axis label        \n       y = \"number of individuals\") + # y axis label   \n  theme_classic() \n\n\n\n\nFigure 4.1: Number of individuals observed per species.\n\n\n\n\n\n\n12 By giving this code chunk a label that starts with fig it will automatically number your figures.We already know from looking at the frequency table that the patterns are quite different across the islands. We can similarly add a line of code to plot the counts for each island in a separate bar plot.\n\n\n\n\n\n\nGive it a try\n\n\n\nAdd a descriptive figure caption and x/y-axis titles.\n\n# define colors to use\ncol &lt;- c(\"darkorange\", \"purple\", \"cyan4\")\n\n\nggplot(penguins,                             # define data set to plot\n       aes(x = species,                      # define variables to plot on x axis\n           fill = species)) +                # color code bars by species\n  geom_bar(stat = \"count\",                   # define chart type as bar plot\n           color = \"black\") +         \n  facet_grid(. ~ island) +                   # create separate plots per island\n  scale_fill_manual(values = col) +          # define colors for fill\n  labs(x = \"species\",                        # x axis label\n       y = \"number of individuals\") +        # y axis label\n  theme_classic() +\n  theme(legend.position = \"bottom\")          # place legend underneath figure\n\n\n\n\nFigure 4.2: Number of individuals observed per species on each island.\n\n\n\n\n\n\n\n\n4.3.2 Grouped (binned) frequency distributions\nWhat about the distribution of some of the other values in our data set? For example, we have measured the body mass of each individual13. Therefore, we probably want to know what the distribution of body size is for each of the penguin species.13 For second visualize a scientist with a scale and a bunch of penguins standing in line waiting to be weighed.\nHowever, this data set is different from counting the number of individuals in a specific category like we did above, body weight is a continuous variable. In this case, we would use a histogram to visualize the distribution. We do this by grouping our observations into bins that represent equally sized groups of possible values.\n\n\n\n\n\n\nGive it a try\n\n\n\nFor example, we could create bins of 100g and then count how many individuals weigh between 0-50g, 50-100g etc. Then we can plot those bins on the x-xis and the scale each bar to correspond to the number of individuals whose weight falls into that bin.\nPlot a histogram for each penguin species using the table below, and consider how this helps summarize your data.\n\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 100,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4.3: Distribution of body weight for each species.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nHistograms are really helpful for exploring general patterns in the distribution of your data because you easily determine\n\nAll the values occurring in your data set\nWhich values occur more/less frequently (the “shape”)\nThe center of your values\nThe amount of variability in your data\n\nIn short, assessing the distribution with a histogram gives you an idea of central tendency and variability of the data set as well - descriptive statistics that we’ll dig into in just a bit.\n\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nWe should think a little bit about how important choosing the “right” bin size is - or if there even is a “right” bin size. Manipulate the code below to try 3 different bin sizes. Compare the figures you produce with your lab mates sitting near you and briefly describe what you learn about picking the right bin size.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n# try a small bin size\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 10,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n# try a large bin size\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 100,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n# try a REALLY LARGE bin size\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 1000,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nLet’s spend a little bit of time thinking about how to correctly interpret a histogram.\nImagine you and two of your lab mates are riding your snowmobiles on one of the Antarctic islands in the study. You come across a Chinstrap penguin colony. They are very cute, so as you (carefully) drive through the penguins you randomly grab five of them (let’s assume you have a little snowmobile side car). One of your lab mate comes across an Adelie penguin colony and also grabs 5 random penguins, while your last partner in crime nags 5 Gentoo penguins.\nYou meet back up at your research station - discuss with your lab mates how heavy you think each of your sets of penguins are. Be as specific as possible.\nBonus question - given your data set who do you think found their colony first?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nGiven that your histogram shows values from roughly 2,500g to 5,000g for Adelie penguins, you can probably assumed that your penguins are somewhere in there. But you can be even more exact, if you carefully look at the height of the bars - they higher bar the more common than weight has been observed and this means that there is a higher probability that your penguins are in that weight bin.\nEven though the absolute height of the bars is lower for Chinstrap penguins is smaller (because there are fewer in the data set), the distribution is actually quite similar and those two sets of penguins are probably quite similar in weight.\nThe smallest observed Gentoo penguins are about as large as the smallest Adelie and Chinstrap penguins. They most commonly are observed to be between 4,500 and 5,500g - your penguins are likely around that size.\n\n\n\n\n\n\n\n\n4.3.3 Normal distribution\nThe data set can have a wide range of shapes and the shape itself will give you some insight into the statistical properties which can be useful down the line, as you make decisions about for example what statistical test to apply14.14 Statistical tests make specific assumptions about the data, if those assumptions are violated your conclusions are not well supported even if your test results say they should be\nOne of the most well-understood distribution is called a normal distribution. Because it is bell-shaped it is also frequently referred to as the bell curve.\n\n\n\n\n\n\nGive it a try\n\n\n\nExecute the code chunk below to generate a normally distributed data set and plot it.\nThen take a look at the figure you just plotted and identify these key characteristics of a normal distribution:\n\nThere is a singe peak right around the center line.\nMost of the observations are at the center of the distribution.\nThe distribution is symmetric around the center line.\n\n\n# randomly draw values from a specified normal distribution\ndf &lt;- rnorm(n = 1000,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.5,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n\nFigure 4.4: Distribution of 1,000 observations randomly drawn from a normal distribution with mean = 0 and standard deviation = 1.\n\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nTake another look at your penguin body mass histograms and argue whether or not you think we can assume that body mass is normally distributed.\nA good way to argue this is to take the list of key characteristics from the list above and explain whether or not you see this in your data set.\n\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 100,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4.5: Distribution of body weight for Adelie, Chinstrap, and Gentoo penguins. Weights are binned in 100g increments.\n\n\n\n\n\n\nOne thing you will notice is that your choice of bin width will play a bit of a role. For example, let’s re-plot the same figure with a slightly larger binwidth and see if you change your mind.\n\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 250,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4.6: Distribution of body weight for Adelie, Chinstrap, and Gentoo penguins. Weights are binned in 250g increments.\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nAnother factor that we might need to consider is that our data set contains both males and females. Sexual dimorphism is common in animal species which can include that one sex is larger compared to the other.\nLet’s create separate plots of each species and sex. Then, re-consider whether or not you think the weight data is normally distributed.\n\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 150,             # define bin width\n                 color = \"black\") +\n  facet_grid(sex ~ species) +                  # create separate plot per species and sex\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4.7: Distribution of body weight for male and female Adelie, Chinstrap, and Gentoo penguins. Weights are binned in 150g increments.\n\n\n\n\n\n\n\n\n4.3.4 Sample vs Population\nBefore we move on to the next statistic, let’s take a moment to think about the difference between a sample and a population. Our data set is a sample of 344 penguins. That is the number of individuals that were observed. However, unless this was a comprehensive census in which the researchers tracked down every single penguin on those three islands this is only a subset of the actual population, which would be all available observations (in this case pengiuns) at a specific point in time for a specific define geographic region (in this case the three islands).\nThe goal is to take a sample that is representative of the population. The larger our sample, the larger the proportion of the population is included which means that our sample will more closely resemble the statistical attributes including the distribution of the population.\n\n\n\n\n\n\nGive it a try\n\n\n\nManipulate the code below to simulate 4 different normal distributions but for three different sample sizes and compare how this affects the distributions. Take a peak at your lab mate’s plots as well, and see if your plots look the same for the same sample size. Assume that body mass of penguins is normally distributed, how large do you think sample sizes should be to make sure your sample is representative.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n# use a small sample size\ndf &lt;- rnorm(n = 10,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.25,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n# use a medium size sample size\ndf &lt;- rnorm(n = 100,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.25,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n# use a large size sample size\ndf &lt;- rnorm(n = 1000,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.25,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n# use a REALLY large size sample size\ndf &lt;- rnorm(n = 10000,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.25,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n\nYou should see that the smaller the sample size the more oddly shaped the distributions. If you regenerate a sample of a normal distribution for the same sample size you will also notice that for small sample sizes those plots jump around quite a bit."
  },
  {
    "objectID": "A04_descript-stats.html#measures-of-central-tendency",
    "href": "A04_descript-stats.html#measures-of-central-tendency",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.4 Measures of central tendency",
    "text": "4.4 Measures of central tendency\nMeasures of central tendency give us an estimate of the center (average) of a data set. This is a complicated (but very specific way) of way it tells us what the “average” is. Typically, when you hear the term average you probably are thinking of the mean, but other important metrics are the median and the mode which give us additional important information about a data set.\n\nThe mean is often used synonymous to average and the metric you are likely most familiar with. You calculate it by adding up all the values and divide by the total number of observations. This is metric is most informative when the data has a normal distribution.\nThe median is the value in the middle of the data set. This means that 50% of values fall below and 50% fall above. For a normal distribution the mean and the mode should be very similar. For skewed data sets this is a more approporiate metric to determine centrality.\nThe mode is most commonly observed value, it’s not typically used in statistics, so we are going to skip it here.\n\nLet’s look at each one in a bit more detail.\n\n4.4.1 The mean\nThe mean is the sum of all the values divided by the sample size (number of observations or values). We can write this as a mathematical formula as follows\n\\[\n\\bar{x} = \\sum x\\_{n} = \\frac{x_{1} + x_{2} + x_{3} + ... x_{n}}{n}\n\\tag{4.1}\\]\nwith \\(\\bar{x}\\) as the sample mean15 , \\(n\\) is the sample size16, and \\(x_{n}\\) being the individual observations.15 It’s important to note that we are almost always determining the sample mean {x} and then use it as an estimate of the true population mean \\(\\mu\\).16 If you are referring to the true size of the population we would generally denote that as \\(N\\).\nThe Equation 4.1 is implemented in the base R function mean() which we can easily group our data by species using group_by() and then calculate the mean body mass using the function summarize(). We need to give it a column name for a summary statistic and then tell it how to calculate the contents for that column. In this case we will specify that we want to apply the function mean() to the column body_mass_g. We will use the argument na.rm to tell R to ignore any missing values.\n\npenguins %&gt;%                                                       # define data frame\n  group_by(species) %&gt;%                                            # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE)) %&gt;%  # calculate mean\n  kable(digits = 1)                                                # print table\n\n\n\n\nspecies\nmean_body_mass\n\n\n\n\nAdelie\n3700.7\n\n\nChinstrap\n3733.1\n\n\nGentoo\n5076.0\n\n\n\n\n\n\n\n4.4.2 The median\nTo determine the median by hand, you would first determine if there is an odd or an even number of observations. Then you need to order all your observations from smallest to largest. If there is an odd number of observations, the median is the middle value17. If you have an even number of observations you would determine the median by finding the values at the positions \\(\\frac{n}{2}\\) and \\(\\frac{n+1}{2}\\) and then the median is the mean of those two values.17 You can determine that position as \\(\\frac{n+1}{2}\\) with \\(n\\) being the sample size.\nFortunately, we again have a base R function (median()) that will allow us to quickly calculate the median for a set of values contained in a single column of a data frame. We can do this using the same functions as above, however, we will add an additional argument to the summarize() function that we would like to add a column called median_body_mass that should contain the values calculated by apply the function median() to the column body_mass_g, again we will specify to ignore any missing values.\n\npenguins %&gt;%                                                           # define data frame\n  group_by(species) %&gt;%                                                # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE),          # calculate mean\n            median_body_mass = median(body_mass_g, na.rm = TRUE)) %&gt;%  # calculate median\n  kable(digits = 1)                                                    # print table\n\n\n\n\nspecies\nmean_body_mass\nmedian_body_mass\n\n\n\n\nAdelie\n3700.7\n3700\n\n\nChinstrap\n3733.1\n3700\n\n\nGentoo\n5076.0\n5000\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nCompare and contrast the mean and median body weight for our three penguin data sets in the table you have just produced.\n\nDiscuss whether these values are surprising after having spent (maybe too much?) time looking at the histograms.\nConsider what this means in the context of what you have learned about the distribution of data sets.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nEven though our Adelie and Chinstrap penguins differ slightly (33g) in their mean body mass, they have the same median weight. Gentoo penguins are on average 2,000g heaver.\nIn all three cases the mean and mode are almost identical, this is indicative of a normal distribution, if a distribution is skewed left (a long tail to the right) the median will be smaller than the mean and if it is skewed right (a long tail to the left) the median will be larger than the mean."
  },
  {
    "objectID": "A04_descript-stats.html#measures-of-variability",
    "href": "A04_descript-stats.html#measures-of-variability",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.5 Measures of variability",
    "text": "4.5 Measures of variability\nThe last category we have to think about is what metrics we can use to describe the variance of the a data set. We use measures of centrality to describe the most typical values we should expect, however most things in nature are not the same. Therefore, we need metrics that tell us how different we should expect values to be from the mean or median to have a sense of how spread out the values are going to be.18.18 Think of this as you now knowing how heave a penguin is on average but you want to know how large you should expect the smallest and the largest penguins to be, how different on average the weight of a penguin is going to be from the mean.\nTypical metrics that are used to describe variability are\n\nthe range tells us how far apart the observed values are; described either as the minimum and maximum observed value or as the difference between the two.\nthe variance is the average of the squared deviations of each value from the mean.\nthe standard deviation is the average amount of variability in the data set and describe how far away and observed value is from the mean on average.\n\n\n4.5.1 The range\nOne of the most straight forward metrics we can determine to describe the variability of the data set is the range. To do this, we simply determine the highest and lowest value for a variable. We can easily add these values to the table we have been expanding by adding additional lines to apply the function min() and max() to the body_weight_g column.\n\npenguins %&gt;%                                                           # define data frame\n  group_by(species) %&gt;%                                                # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE),          # calculate mean\n            median_body_mass = median(body_mass_g, na.rm = TRUE),      # calculate median\n            min_body_mass = min(body_mass_g, na.rm = TRUE),            # determine largest value\n            max_body_mass = max(body_mass_g, na.rm = TRUE)) %&gt;%        # determine largest value\n  kable(digits = 1) \n\n\n\n\n\n\n\n\n\n\n\nspecies\nmean_body_mass\nmedian_body_mass\nmin_body_mass\nmax_body_mass\n\n\n\n\nAdelie\n3700.7\n3700\n2850\n4775\n\n\nChinstrap\n3733.1\n3700\n2700\n4800\n\n\nGentoo\n5076.0\n5000\n3950\n6300\n\n\n\n\n\nEven though the range is easy to determine and can be informative in some cases, it is actually rarely used as a measure of variability because the sample range does not always reflect the range of the actual population. There generally is a pattern where up to a certain point your range will increase with sample size.\n\n\n\n\n\n\nConsider this\n\n\n\nWe are going to plot all of our observations for body mass on the x-axis grouped by species. Use this figure to argue why using the range as a measure of variability can be misleading in some cases because it ignores important contextual information.\n\nggplot(penguins, aes(x = body_mass_g, y = species, fill = species)) +\n  geom_jitter(size = 3, shape = 21, alpha = .4, height = .1) +\n  scale_fill_manual(values = col) +\n  labs(x = \"body mass [g]\", y = \"species\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 4.8: Observed body weight for Adelie, Chinstrap, and Gentoo pengiuns.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can see that Chinstrap penguins have a wider range compared to the Adelie penguins but most of their points actually cluster more tightly compared to the other species. We don’t know if this is just because their sample size is smaller and so we are more likely not to observe the full range of body weight exhibited by Gentoo populations or if these are true outliers.\n\n\n\n\n\n4.5.2 Variance\nOne way we can avoid the issues that can be introduced by using the range to describe variability is to determine how far away each point is from the mean value (deviation from the mean).\nLet’s visualize this by plotting all the weights for the Chinstrap penguins on the x-axis and adding a line between each point and the mean.\n\n# create data frame with only chinstrap penguins\nchinstrap &lt;- penguins %&gt;%\n  filter(species == \"Chinstrap\") %&gt;%\n  rownames_to_column(\"indv\")\n\n# plot all observed weights\nggplot(chinstrap, aes(x = body_mass_g, y = indv)) +\n  geom_vline(aes(xintercept = mean(body_mass_g, na.rm = TRUE)), \n             color = \"darkred\", size = 1) +\n  geom_segment(aes(x = body_mass_g, xend = mean(body_mass_g),\n                   yend = indv)) +\n  geom_point(size = 3, shape = 21, fill = \"cyan4\") +\n  labs(x = \"body mass [g]\") +\n  theme_classic() +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\nFigure 4.9: Body weights for Chinstrap penguins. Red line indicates mean body mass. Black lines denote the deviation from the mean for each observation.\n\n\n\n\nAt first glance, just summing up all the values seems like a simple solution to get the “total amount of deviation” in the data set. However, some of our values are positive while others are negative so that would be problematic because they will cancel each other out.\nInstead, we need to square each deviation and then we can sum them up19. The greater the variability in our data, the greater the sum of the squares. We generally refer to the sum of the squared deviations as the variance \\(s^{2}\\). To calculate the sample variance, you need to subtract each value from the sample mean, square each value, add up all the values, and then divide this sum by the number of samples minus 120.19 Squaring the mean means that our values are always positive. It also means that values that are at extreme distances from the mean will have a greated effect on the variability.20 We do this to correct for the fact that we are calculting the sample variance and not the variance of the true population.\n\\[\ns^{2}=\\frac{\\sum (x_{i}-\\bar{x})^2}{n-1}\n\\tag{4.2}\\]\nWith \\(x_{i}\\) being in individual observation, \\(\\bar{x}\\) the sample mean, and \\(n\\) the sample size.\nAgain, it is important that we differ between the sample variance that you can calculate using your data set and provides an estimate of the population variance.\nYou may have assumed that we can calculate this in R using a function called variance() - close, it is actually called var(), but you are correct that we can add the variance in body mass measurements for each penguin species to our summary stats table using the same syntax we have been applying.\n\npenguins %&gt;%                                                           # define data frame\n  group_by(species) %&gt;%                                                # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE),          # calculate mean\n            median_body_mass = median(body_mass_g, na.rm = TRUE),      # calculate median\n            min_body_mass = min(body_mass_g, na.rm = TRUE),            # determine largest value\n            max_body_mass = max(body_mass_g, na.rm = TRUE),            # determine largest value\n            var_body_mass = var(body_mass_g, na.rm = TRUE)) %&gt;%         # calculate variance\n  kable(digits = 1) \n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nmean_body_mass\nmedian_body_mass\nmin_body_mass\nmax_body_mass\nvar_body_mass\n\n\n\n\nAdelie\n3700.7\n3700\n2850\n4775\n210282.9\n\n\nChinstrap\n3733.1\n3700\n2700\n4800\n147713.5\n\n\nGentoo\n5076.0\n5000\n3950\n6300\n254133.2\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nCompare and contrast the range and the variance for body mass in across the different species and argue which metric you think is more informative.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAs indicate before, Adelie have a smaller range of body mass compared to Chinstrap penguins, however, we can see that the variance of Chinstrap is quite a bit smaller. Recall the visualization that showed that Chinstrap body weights points cluster closer together compared to Adelie pengiuns despite those extreme values that are creating the misleading greater range.\n\n\n\n\n\n4.5.3 Standard deviation\nOne of the drawbacks of using the variance to describe variability is that because we have squared the deviations our unites are no longer meaningful21, we can compare variance by magnitude but it is not helpful to say that the variance in Adelie penguin body weight is about 62,000 squared grams larger compared to Chinstrap penguins.21 The we of course remain open to good ideas on how to interpret “square grams”.\nTo get around this issue, we generally calculate the standard deviation \\(s\\) by drawing the square root of the variance.\n\\[\ns = \\sqrt{s^{2}}\n\\tag{4.3}\\]\nWe can add this to our summary stats table using the function sd().\n\npenguins %&gt;%                                                           # define data frame\n  group_by(species) %&gt;%                                                # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE),          # calculate mean\n            median_body_mass = median(body_mass_g, na.rm = TRUE),      # calculate median\n            min_body_mass = min(body_mass_g, na.rm = TRUE),            # determine largest value\n            max_body_mass = max(body_mass_g, na.rm = TRUE),            # determine largest value\n            var_body_mass = var(body_mass_g, na.rm = TRUE),            # calculate variance\n            std_body_mass = sd(body_mass_g, na.rm = TRUE)) %&gt;%         # calculate standard deviation\n  kable(digits = 1) \n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nmean_body_mass\nmedian_body_mass\nmin_body_mass\nmax_body_mass\nvar_body_mass\nstd_body_mass\n\n\n\n\nAdelie\n3700.7\n3700\n2850\n4775\n210282.9\n458.6\n\n\nChinstrap\n3733.1\n3700\n2700\n4800\n147713.5\n384.3\n\n\nGentoo\n5076.0\n5000\n3950\n6300\n254133.2\n504.1\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nWe frequently report means as mean +/- the standard deviation. Describe your results comparing the body mass of these three species using this format.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nGentoo penguins were observed to be the largest species with a mean body mass of 5,076g +/- 504g. Adelie and Chinstrap penguins are smaller with observed means of 3,700 +/- 458g and 3,733g +/- 384g, respectively, indicating that though on average they are similar in size, Adelie penguins exhibit a larger variability."
  },
  {
    "objectID": "A04_descript-stats.html#homework",
    "href": "A04_descript-stats.html#homework",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.6 Homework",
    "text": "4.6 Homework\nComplete the questions below to review some of what you have learned about descriptive statistics. Render your document as an html-file and submit that document through Canvas. Double check that you are submitting the html file, not the quarto document.\nOur data set contains information on the flipper length for each individual in the data set.\n\n\n\n\n\n\nConsider this\n\n\n\nDescribe the individual steps you would complete to create a figure summarizing the distribution of the data.\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\n[Outline your steps here.]\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nExecute the code chunk below to plot a histogram of flipper length by for each species.\n\nAdd a descriptive figure caption consisting of a concise title and a description that explains what is being displayed\nadd x and y-axis titles.\nAdjust the binwidth (currently at 25) to a width you find appropriate and explain why you chose that number\nArgue whether or not you can assume a normal distribution for flipper length.\n\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\nModify the figure by defining which data to plot on the x-axis and finding an appropriate binwidth:\n\nggplot(penguins, aes(x = columname, # define data set & variable on x-axis\n                     fill = species)) +     # color code by species\n  geom_histogram(binwidth = 25,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                # create separate plot per species\n  scale_fill_manual(values = col) +        # define color palette\n  labs(x = \"title\",          # x-axis title\n       y = \"title\") +      # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n[Argue whether normal distribution].\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nExecute the code chunk below to calculate a series of descriptive statistics summarizing the flipper length measurements.\n\nExplain the difference between measures of central tendency and variability and for metric in the table determine which category it falls under (Protip: A good way to set up an answer for a question like this is to first define a category, then pick a variable and state “this variable tells me …, which falls into this category because …”.\nConsider which summary statistics you think are most informative and argue which two you would report if you were limited to reporting only two. (Protip: Since we have two categories you probably want one in each category. Then a good way to argue is to pick a metric, explain the information it conveys is important and then also explain why it does this better than other variables in that category).\nDescribe your results based on the metrics you chose.\n\n\npenguins %&gt;%                                                       # define data frame\n  group_by(species) %&gt;%                                            # create subsets by species\n  summarize(mean = mean(flipper_length_mm, na.rm = TRUE),          # calculate mean\n            median = median(flipper_length_mm, na.rm = TRUE),      # calculate median\n            min = min(flipper_length_mm, na.rm = TRUE),            # determine largest value\n            max = max(flipper_length_mm, na.rm = TRUE),            # determine largest value\n            var = var(flipper_length_mm, na.rm = TRUE),            # calculate variance\n            sd = sd(flipper_length_mm, na.rm = TRUE)) %&gt;%          # calculate sd\n  kable(digits = 1) \n\n\n?(caption)\n\n\n\n\n\n\nspecies\nmean\nmedian\nmin\nmax\nvar\nsd\n\n\n\n\nAdelie\n190.0\n190\n172\n210\n42.8\n6.5\n\n\nChinstrap\n195.8\n196\n178\n212\n50.9\n7.1\n\n\nGentoo\n217.2\n216\n203\n231\n42.1\n6.5"
  },
  {
    "objectID": "A05_inferent-stats.html#learning-objectives",
    "href": "A05_inferent-stats.html#learning-objectives",
    "title": "5  Introduction to inferential statistics",
    "section": "5.1 Learning Objectives",
    "text": "5.1 Learning Objectives\nAfter completing this activity, you should be able to\n\nDescribe how to quantify the uncertainty of an estimate.\nDescribe the concept of statistical inference.\nExplain how sample size influences sample distributions.\nDefine what sampling error is and explain the main causes.\nDefine and calculate the standard error.\nOutline the key steps for statistical hypothesis testing.\nPerform a t-test using R to determine if two means are significantly different.\nPerform a \\(\\chi^{2}\\)-test using R to determine if two frequencies are significantly different.\n\nRecall from our foray into describing penguin distributions and body mass on three Antarctic Islands that we discovered that Adelie, Chinstrap, and Gentoo Penguins are found on the islands in different frequencies and that they have different distributions of body weight.\nEspecially, when we were exploring patterns in body mass, we emphasized the fact that we were calculating the sample mean \\(\\bar{x}\\) which is an estimate of the true population mean \\(\\mu\\). Even though we really want to know \\(\\mu\\), it is unlikely that we will ever know the true value because that would entail catching and measuring every. single. penguin. on each each of those islands. So generally, we are going to have to be satisfied with the estimate provided by calculating \\(\\bar{x}\\). As a result, it is important that we can assume that we have a reliable estimate based on a sufficiently large, randomly drawn sample. It is important that our sample estimate is accurate because we want to be able to draw conclusions about the population as a whole by making inferences based on our sample which includes making generalizations about the population and testing hypotheses. This is generally referred to as inferential statistics.\nBefore we get started, we will need to load several R libraries."
  },
  {
    "objectID": "A05_inferent-stats.html#sample-size-and-sampling-distributions",
    "href": "A05_inferent-stats.html#sample-size-and-sampling-distributions",
    "title": "5  Introduction to inferential statistics",
    "section": "5.2 Sample size and sampling distributions",
    "text": "5.2 Sample size and sampling distributions\nExecute the code below to create a table with the sample size and mean body mass for each species in the data set.\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(sample_size = n(),\n            mean_mass = mean(body_mass_g, na.rm = TRUE)) %&gt;%\n  kable(digits = 1)\n\n\n\n\n\n\nspecies\nsample_size\nmean_mass\n\n\n\n\nAdelie\n152\n3700.7\n\n\nChinstrap\n68\n3733.1\n\n\nGentoo\n124\n5076.0\n\n\n\nTable 5.1: Mean body weight for Adelie, Chinstrap, and Gentoo penguins in Antarctica.\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nNotice that the sample sizes are quite different across the species. We have observed more than twice the amount of Adelie and Gentoo penguins compared to Chinstrap penguins.\nArgue whether you think this means that our estimate of the mean Chinstrap penguin body mass is less reliable compared to the value we have calculated for the other two species.\n\n\nTo figure out whether or not you where correct, let’s hop back on our snowmobiles, collect 30 Adelie penguins, and then calculate the sample mean.\n\n\n\n\n\n\nGive it a try\n\n\n\nLet’s assume that our data set contains ALL the Adelie penguins on the three islands and therefore our mean represents the true population mean \\(\\mu\\).\nExecute the code chunk below to randomly sample 30 Penguins from our data set and calculate the mean.\n\npenguins %&gt;%\n  filter(species == \"Adelie\") %&gt;%      # retain only Adelie penguins\n  slice_sample(n = 30) %&gt;%             # specify number to randomly draw\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE)) %&gt;%\n  kable(digits = 1)\n\n\n\n\nmean_body_mass\n\n\n\n\n3731.9\n\n\n\n\n\nCompare the sample mean you just calculated to the sample mean in the table above and to the values your classmates around you have calculated.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou should have noticed that your mean based on 30 individuals is different from the value you obtained based on all 152 observations. Because are randomly pulling 30 individuals, you would have pulled 30 different individuals compared to your classmates around you, so you should all end up with different values.\n\n\n\nWe could compile all the different means calculated for different samples of 30 individuals into a histogram to look at the distribution of values. This is what is called a sampling distribution.\n\n\n\n\n\n\nGive it a try\n\n\n\nExecute the code below to create 1000 samples, calculate the mean for each, and then plot it as a histogram.\nDescribe your results and use this information to make a statement about how representative the sample mean \\(\\bar{x}\\) based on 30 individuals is compared to the true mean \\(\\mu\\) for all 152 individuals.\n\n# create an empty data frame for results\nsample_means &lt;- data.frame(sample = numeric(),\n                           mean = numeric())\n\n# create loop to draw 500 samples\nfor(i in 1:1000){\n  \n  sample_means &lt;- penguins %&gt;%\n    filter(species == \"Adelie\") %&gt;%                        # retain only Adelie penguins\n    slice_sample(n = 30) %&gt;%                               # specify number to randomly draw\n    summarize(mean = mean(body_mass_g, na.rm = TRUE)) %&gt;%  # calculate mean\n    mutate(sample = 1) %&gt;%                                 # column with replicate number\n    bind_rows(sample_means)                                # add to results data frame\n  \n  \n}\n\n# plot distribution\nggplot(sample_means, aes(x = mean)) +\n  geom_histogram(binwidth = 25,\n                 color = \"black\", fill = \"darkorange\") +\n  labs(x = \"sample means\",\n       y = \"Number of samples\") +\n  theme_classic()\n\n\n\n\nFigure 5.1: Sampling distribution of mean body mass for 500 samples of 30 individuals.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe histogram gives you an idea of the range of possible estimates based on 30 individuals. Your data should appear to be normally distributed, with the most commonly occurring values being around 3,700g which is the true population mean. Your estimates should ranges from approximately 3,500g to about 4,000g which means that your estimates are within 250-300g of your true mean body weight.\n\n\n\nNow that we know how to create a sampling distribution and we can interpret it to give us an idea of how well our sample mean \\(\\bar{x}\\) estimates the true mean \\(\\mu\\), we can explore how our sample size impacts how “good” our estimate is.\n\n\n\n\n\n\nGive it a try\n\n\n\nManipulate the code below to create sampling distributions for a very small, medium, and large sample size. Compare your three histograms to each other as well as to those of your lab mates around you, including if they used the same numbers as you to determine the role of sample size on the values you obtain for the sample mean \\(\\bar{x}\\).\n\n# PICK A SMALL VALUE ----\n\nsample_size &lt;- 5 # THIS IS THE NUMBER YOU SHOULD CHANGE\n\n# create an empty data frame for results\nsample_means &lt;- data.frame(sample = numeric(),\n                           mean = numeric())\n\n# run loop to draw a SMALL sample\nfor(i in 1:1000){\n  \n  sample_means &lt;- penguins %&gt;%\n    filter(species == \"Adelie\") %&gt;%                        \n    slice_sample(n = sample_size) %&gt;%  \n    summarize(mean = mean(body_mass_g, na.rm = TRUE)) %&gt;%  \n    mutate(sample = 1) %&gt;%                                \n    bind_rows(sample_means)                               \n  \n}\n\n# plot distribution\nggplot(sample_means, aes(x = mean)) +\n  geom_histogram(binwidth = 25,\n                 color = \"black\", fill = \"darkorange\") +\n  scale_x_continuous(limits = c(3000, 4500)) +\n  labs(x = \"sample means for N = 5\",\n       y = \"Number of samples\") + # add your sample size here\n  theme_classic()\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n# PICK A MEDIUM VALUE ----\n\nsample_size &lt;- 50 # THIS IS THE NUMBER YOU SHOULD CHANGE\n\n# create an empty data frame for results\nsample_means &lt;- data.frame(sample = numeric(),\n                           mean = numeric())\n\n# run loop to draw a MEDIUM sample\nfor(i in 1:1000){\n  \n  sample_means &lt;- penguins %&gt;%\n    filter(species == \"Adelie\") %&gt;%                        \n    slice_sample(n = sample_size) %&gt;%\n    summarize(mean = mean(body_mass_g, na.rm = TRUE)) %&gt;%  \n    mutate(sample = 1) %&gt;%                                \n    bind_rows(sample_means)                               \n  \n}\n\n# plot distribution\nggplot(sample_means, aes(x = mean)) +\n  geom_histogram(binwidth = 25,\n                 color = \"black\", fill = \"darkorange\") +\n  scale_x_continuous(limits = c(3000, 4500)) +\n  labs(x = \"sample means for N = 50\",\n       y = \"Number of samples\") + # add your sample size here\n  theme_classic()\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n# PICK A LARGE  VALUE ----\n\nsample_size &lt;- 100 # THIS IS THE NUMBER YOU SHOULD CHANGE (remember you cannot go over 152)\n\n# create an empty data frame for results\nsample_means &lt;- data.frame(sample = numeric(),\n                           mean = numeric())\n\n\n# run loop to draw a LARGE sample \nfor(i in 1:1000){\n  \n  sample_means &lt;- penguins %&gt;%\n    filter(species == \"Adelie\") %&gt;%                        \n    slice_sample(n = 125) %&gt;%  # THIS IS THE NUMBER YOU SHOULD CHANGE to change the sample size\n    summarize(mean = mean(body_mass_g, na.rm = TRUE)) %&gt;%  \n    mutate(sample = 1) %&gt;%                                \n    bind_rows(sample_means)                               \n  \n}\n\n# plot distribution\nggplot(sample_means, aes(x = mean)) +\n  geom_histogram(binwidth = 25,\n                 color = \"black\", fill = \"darkorange\") +\n  scale_x_continuous(limits = c(3000, 4500)) +\n  labs(x = \"sample means for N = 100\",\n       y = \"Number of samples\") + # add your sample size here\n  theme_classic()\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn each case you should end up with a normal distribution and the means should be similar across all histograms and fall right around the mean we calculated for all 152 individuals (3,700g). The key distinction is that the smaller your sample size the wider the distribution, i.e. for larger samples the values that you obtain are increasingly less dispersed."
  },
  {
    "objectID": "A05_inferent-stats.html#standard-error",
    "href": "A05_inferent-stats.html#standard-error",
    "title": "5  Introduction to inferential statistics",
    "section": "5.3 Standard error",
    "text": "5.3 Standard error\nGetting back to our initial question of whether we can quantify how well our sample mean represents the population mean. From our exploration of sampling distributions we now know that the spread of the sampling distribution gives us an indication of how “good” our estimate is, the tighter that sampling distribution, the better our estimate. If we were to calculate the standard deviation of each of the sampling distributions we would expect that the sampling distributions based on larger sample sizes to have smaller standard deviations and those based on smaller sample sizes to have larger standard deviations.\nThe standard deviation of a sampling distribution of \\(\\bar{x}\\) values is called the standard error of the mean or simply standard error1. The standard error provides us a metric to assess the level of precision of our sample estimate, the smaller the standard error the more likely that estimate is an accurate representation of the true population.1 You can of course, calculate the standard error for any sample statistic but we frequently just call it the standard error and assume that from the context it is clear which statistic we are referring to\nTypically, we would calculate the standard error by dividing the standard deviation by the square root of the number of sample data2.2 Deriving this equation which allows you to determine the standard error without resampling is beyond the scope of our class so we’ll just go straight to the solution.\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\tag{5.1}\\]\nfor \\(s\\) as the standard deviation and a sample size of \\(n\\).\nWe do not have a function already implemented in R to calculate this for us, but we can use the function mutate() to create a new column called se and then give R the equation to calculate it based on sample size and standard deviation.\n\n\n\n\n\n\nGive it a try\n\n\n\nExecute the code below to calculate the sample size, mean, standard deviation, and standard error. Compare your results to confirm whether you were correct in your earlier assessment of whether the estimate of the mean Chinstrap penguin body mass is less reliable compared to the value we have calculated for the other two species.\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(sample_size = n(),\n            mean_mass = mean(body_mass_g, na.rm = TRUE),\n            sd = sd(body_mass_g, na.rm = TRUE)) %&gt;%\n  mutate(se = sd/sqrt(sample_size)) %&gt;%\n  kable(digits = 1)\n\n\n\n\nspecies\nsample_size\nmean_mass\nsd\nse\n\n\n\n\nAdelie\n152\n3700.7\n458.6\n37.2\n\n\nChinstrap\n68\n3733.1\n384.3\n46.6\n\n\nGentoo\n124\n5076.0\n504.1\n45.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe standard error is lowest for Adelie penguins, which also have the largest sample size. However, we do see that the standard error for Chinstrap and Gentoo is very similar, despite the large difference in sample size."
  },
  {
    "objectID": "A05_inferent-stats.html#sampling-error",
    "href": "A05_inferent-stats.html#sampling-error",
    "title": "5  Introduction to inferential statistics",
    "section": "5.4 Sampling Error",
    "text": "5.4 Sampling Error\nOur experiment of re-sampling the Adelie penguins showed us that the sample size and whether a sample is randomly taken is really important for us to be able to make inferences of the population based on the sample because we are are making the assumption that our sample is representative of the population as a whole. We use to term sampling error to describe obtaining a sample that does not truly represent the population as a whole.\n\n\n\n\n\n\nConsider this\n\n\n\nHop back on your snowmobile with intent of gathering a sample of penguins. Before you get going, consider what you would need to due to avoid bias in your sample due to sampling error.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe main causes are:\n\nsmall sample size: You would want to make sure that you are snagging enough individuals from each species to ensure that you are capturing the full range of body sizes for each species. The larger you sample size, the more likely you not only have individuals representing that full range, you should also end up with proportionally the same number of individuals in each weight category in your sample as there are in the entire population.\nnon-random sampling: You need to make sure that you are sampling in a non-random manner. If you think the tiny ones are cuter and start picking up those more frequently then you would end up with a biased sample. If you are lazy, and just check the areas near your station, you might end up biasing towards larger ones e.g. because that is a spot where more food is available. If you are trying to spot them from far off and then driving towards them instead of having e.g. a random sampling grid you might only be spotting the larger ones etc.\noutliers can cause sampling error especially if the sample size is small. This is generally not something you can avoid by your sampling strategy, generally you want to have a sample size that is large enough that you can identify outliers.\n\n\n\n\nWhen we are making inferences based on our data set either by generalizing from the sample to the population as a whole or when testing a specific hypothesis based on the data we’ve collected, we need to make sure that our conclusion are erroneous due to sampling error. Therefore, once we’ve calculated our descriptive statistics we will generally ask the question “What is the probability that our results are the results of sampling error and do not reflect reality?” and then use statistical tests to quantify that probability. The probability of results being due to sampling error increase the smaller our sample size is, the more variability that is observed, and the smaller the difference between metrics3 being compared.3 We will look at the specific examples of testing whether means and frequencies are statistically different."
  },
  {
    "objectID": "A05_inferent-stats.html#hypothesis-testing",
    "href": "A05_inferent-stats.html#hypothesis-testing",
    "title": "5  Introduction to inferential statistics",
    "section": "5.5 Hypothesis testing",
    "text": "5.5 Hypothesis testing\nHypothesis testing is a formal process to make inferences about a population based on sample data. It involves evaluating two competing hypotheses to determine if the results are due to sampling error alone or are representative of the population. The key steps are the same, regardless of the the specific test being applied.\n\nDefine your research question or goal.\nSpecify what measurements you plan to compare and collect the data.\nState your your null and alternate hypothesis and predict the outcome assuming the null hypothesis is true.\nSpecify a significance threshold.\nGenerate sampling distributions and calculate your test statistic.\nGet the p-value.\nInterpret the results and draw conclusions.\n\nThe R package infer contains a series of functions to that implements a workflow to perform statistical inference using this framework. Each function is a “verb” to execute a specific action/step of the process.\n\nspecify(): specify the variable or relationship of variables you are interested in.\nhypothesize(): formulate your null hypothesis.\ngenerate(): generate a null distribution4.\ncalculate(): calculate the distribution of test statistics based on the generated data to form the null hypothesis\nvisualize(): see where your observed values falls relative to the null distribution.\nget_p_value(): calculate p-values.\n\n4 data reflecting the null hypothesisWhich statistical test you apply will depend on the type of data you are looking at5 and whether they fulfill specific assumptions6. We are going to learn how to perform a t-test to evaluate the difference between two means and a \\(\\chi^{2}\\) test of independence to evaluate the difference between frequencies.5 For example, are we looking at continuous or numerical data.6 For example, we might assume that the data has a normal distribution.\n\n5.5.1 T-test to evaluate the difference between two means\n\n5.5.1.1 Step 1: Define your research question or goal.\nFigure out what the larger question is you want to address based on observations you have made.\nFor example, sexual dimorphism is common in birds. This means that we frequently observe differences in the morphology of males compared to females, this can include coloration7 but also body size. As you have been riding around the Antarctica on your snowmobile you have not noticed any differences in terms of coloration but you have seen that there is a broad range of sizes exhibited by adult penguins.7 Look up a picture of a male compared to a female peacock.\n\n\n\n\n\n\nGive it a try\n\n\n\nFormulate a research question or goal based on this observation about penguin body size in the Antarctica.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nEvaluate these possible answers:\n\nWhat factors contribute to the observed variation in body sizes among adult penguins in Antarctica?\nCan observed variation in body sizes among adult penguins be attributed to sexual dimorphism?\nAre male Chinstrap penguins larger than female penguins?\n\nNotice that they vary in how specific they are, e.g. “what factors” is more general than placing your question in the context of “sexual dimorphism”, i.e. you are picking a specific factor you think could be the cause. Similarly, you might want to start with a specific species, before looking at all the species.\nLet’s pick this question:\nIs the observed variation in body size among adult Chinstrap penguins due to sexual dimorphism?\nYou can also state it as a hypothesis:\nChinstrap penguins exhibit sexual dimorphism in terms of body size..\n\n\n\n\n\n5.5.1.2 Step 2: Specify what measurements you plan to compare and collect the data.\nExplicitly state which groups of individuals or objects you plan to sample and describe what you will measure to generate a data set that can answer your question. You will need to ensure that you are generating a sufficiently large, random sample. Then go out and gather the data.\n\n\n\n\n\n\nGive it a try\n\n\n\nDescribe how you will generate the data set you need to answer your question. Be as specific as possible.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFirst, we need to decide how we are going to measure “size” or “largeness” to quantify differences in body size, in this case you could include for example height or body mass. For our example, we will use body mass.\nThen you need to determine how you will obtain your sample in a way that ensure that you are creating a sample that is representative of the population as a whole. This includes making sure that your sample size is large enough. In this case, you might have multiple colonies on the island you are sampling so you would want to make sure to include individuals from every colony. Another consideration is that you might have outlier years with e.g. a low amount of food sources and so if you had the option to sample across multiple years that could improve the quality of your sample.\nIn this case you are comparing two groups within a population (males and females) so you need to make sure that you have an approximately equal representation of males and females in your data set.\n\n\n\nYou’ve returned from your field work and entered all your data. Let’s take a quick look at the descriptive statistics of your results.\nWe’ll start by calculate in the mean and standard deviation of body mass for males and females.\n\n# dataframe with gentoo penguin data\nchinstrap &lt;- penguins %&gt;%\n  filter(!is.na(sex) & species == \"Chinstrap\")\n  \nchinstrap %&gt;%\n  group_by(sex) %&gt;%\n  summarize(mean_mass = mean(body_mass_g),\n            sd_mass = sd(body_mass_g)) %&gt;%\n  kable(digits = 1)\n\n\n\n\n\n\nsex\nmean_mass\nsd_mass\n\n\n\n\nfemale\n3527.2\n285.3\n\n\nmale\n3939.0\n362.1\n\n\n\nTable 5.2: Mean and standard deviation body mass for male and female Chinstrap penguins.\n\n\nThe males have a higher mean body mass, however, we do see that the standard deviation indicates that the spread of the data is wide enough that you could have males and females with the same weight. Let’s plot a histogram to see if we are correct and there is an overlap in the weight distributions.\n\nggplot(chinstrap, aes(x = body_mass_g, fill = sex)) +\n  geom_histogram(binwidth = 100,\n                 color = \"black\") +\n  scale_fill_manual(values = c(\"cyan4\", \"darkorange\")) +\n  facet_grid(sex ~ species) +\n  labs(x = \"body mass [g]\",\n       y = \"number of individuals\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 5.2: Distribution of body mass for male and female Chinstrap penguins.\n\n\n\n\nWe do see that our distributions overlap, our sample sizes are on the smaller end - so the question we now have is whether or not the difference in means we observe is due to sampling error. Our next steps will let us quantify that.\n\n\n5.5.1.3 Step 3: State your your null and alternate hypothesis and predict the outcome assuming the null hypothesis is true.\nWe have already developed our initial research hypothesis as Chinstrap penguins exhibit sexual dimorphism in terms of body size.. Now we need to restate this as a null (\\(H_{O}\\)) and alternate (\\(H_{a}\\) or \\(H_{1}\\)) so we can mathematically test it by quantifying the probability that our results are due to chance alone.\nThe null hypothesis \\(H_{O}\\) states what our measurements will look like when there is no change or no effect in the population, or no relationship or no difference among groups being compared.\nAt first, stating our hypothesis as “there is no difference” seems a bit odd because that’s not normally what we are interested in. However, it is actually quite difficult to prove an idea but much easier to disprove and idea. Therefore, we want to formulate a hypothesis that we can falsify. If the sample provides enough evidence against the null hypothesis then we can reject the hypothesis, otherwise we fail to reject the null hypothesis8.8 Be careful not to use phrases like “we proved” or we “accept” the null hypothesis.\n\n\n\n\n\n\nGive it a try\n\n\n\nYou can use a standard way of formulating your null hypothesis by identifying your dependent and independent variable of your experiment9. For the null hypothesis you will always state that the Independent variable does not affect your dependent variable.\nDefine the null hypothesis for your research question and predict what your results should look like if the null hypothesis is correct.\n\n\n9 These are also referred to as the response and explanatory variables.\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn this case our independent variable is body mass and the dependent variable is sex.\n\n\\(H_{O}\\) Sex does not affect body mass.\nPrediction of the results: There is no difference in the mean body mass for males and females.\n\n\n\n\nThe alternate hypothesis \\(H_{a}\\) is the complement to the null hypothesis; they should be mutually exclusive in that only one can be true at a time. The alternative hypothesis is a claim that there is an effect in the population. When you formulate your alternative hypothesis you are usually going to use phrases like “there is an effect”, “there is a difference”, “there is a relationship”.\n\n\n\n\n\n\nGive it a try\n\n\n\nSimilarly, you can use a standard way of formulating your alternate hypothesis by identifying your dependent and independent variable of your experiment. In this case, you will always state that the Independent variable does affect your dependent variable.\nDefine the alternate hypothesis for your research question and predict what your results should look like if the null hypothesis is correct.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\\(H_{a}\\) Sex affects body mass.\nPrediction of the results: There is a difference in the mean body mass for males and females.\n\n\n\n\n\n\n5.5.1.4 Step 4: Specify a significance threshold.\nNow that we have stated our null hypothesis we can assess the probability of obtaining our observed results if our null hypothesis is true. However, before we do that we need to decide how far off from the null hypothesis we will let our observations be before we reject the null hypothesis.\nWe do this by specifying the significance level \\(\\alpha\\). This is our predetermined threshold for statistical significance.\n\nif our p-value is higher than \\(\\alpha\\) we fail to reject the null hypothesis and our results are not statistically significant.\nif our p-value is smaller than \\(\\alpha\\) we reject the null hypothesis and report our results as statistically significant.\n\nTypically, \\(\\alpha\\) is set to 0.05 (5%) but for a more conservative test it can be set to a lower value such as 0.01 (1%).\n\n\n5.5.1.5 Step 5: Calculate your test statistic and generate sampling distribution.\nNow we are ready to calculate our test statistic.\nA test statistic measures how far the empirical observations are from the expectations based on the null hypothesis. Even though the exact test statistic you apply will depend on the data and type of variable you are assessing, they are all based on comparing spread of the data within a category (within-group variance) to how different categories are from each other (between-group variance). For large between-group variance (little/no overlap between groups) the more unlikely it is that the differences between these groups is due to chance alone (small p-value). By contrast, for a high within-group and low between-group variance the more likely the differences you are observing are due to chance alone (high p-value).\nGiven the assumption that our data has a normal distribution the correct test to apply is a t-test, this test will account for the sample size, the variability in the data, and the size of the difference of the means.\n\\[\nt = \\frac{|\\bar{x}_{1}-\\bar{x}_{2}|}{\\sqrt{\\frac{s_{2}^{p}}{n_{1}}+\\frac{s_{2}^{p}}{n_{2}}}}\n\\]\nwhere \\(\\bar{x}_{1}\\) and \\(\\bar{x}_{2}\\) are the means of the samples, \\(n_{1}\\) and \\(n_{2}\\) are the sample means and \\(s_{2}^{p}\\) is the pooled estimate of variance for the two samples.\nLet’s calculate \\(t\\) for our data.\n\n# calculate the observed statistic\nobserved_statistic &lt;- chinstrap %&gt;%\n  specify(response = body_mass_g,\n          explanatory = sex) %&gt;%               # specify values we are interested in\n  calculate(stat = \"diff in means\",            # test statistic\n            order = c(\"male\", \"female\"))       # order to subtract the mean values\n\n# print observed value\nobserved_statistic\n\nResponse: body_mass_g (numeric)\nExplanatory: sex (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  412.\n\n\nThat gives us our observed test-statistic. However, we want to know how this value compares to the sampling distribution of the test statistic based on data that is generated according to the null hypothesis10.10 We would refer to this as the null distribution.\nRecall, that our null hypothesis is that sex does not affect body mass. So we would generate a null distribution by drawing a random subsample from our data and then randomly redistributing our the values for sex and then calculating the test statistic for that sample. We keep repeating that process to create a distribution of values that we should observe for our test statistic assuming our null hypothesis is correct.\n\nnull_dist &lt;- chinstrap %&gt;%\n  specify(response = body_mass_g,\n          explanatory = sex) %&gt;%               # specify values we are interested in\n  hypothesize(null = \"independence\") %&gt;%       # null hypothesis: sex and weight are independent\n  generate(reps = 1000,                        # generate 1000 samples for null distribution\n           type = \"permute\") %&gt;%               # randomly assign sex to weight to break association\n  calculate(stat = \"diff in means\",            # statistic is t\n            order = c(\"male\", \"female\"))       # order to subtract the mean values\n\n# print first few rows of null distribution\nhead(null_dist)\n\nResponse: body_mass_g (numeric)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 6 × 2\n  replicate  stat\n      &lt;int&gt; &lt;dbl&gt;\n1         1  85.3\n2         2  47.1\n3         3  25  \n4         4  29.4\n5         5  10.3\n6         6  30.9\n\n\nLet’s visualize our null distribution and see where our observed test statistic falls relative to that distribution.\n\n# visualize the randomization-based null distribution and test statistic\nnull_dist %&gt;%\n  visualize() + \n  shade_p_value(observed_statistic,\n                direction = \"two-sided\")\n\n\n\n\nFigure 5.3: Null distribution for test statistic t. Observed test statistic is visualized by red line.\n\n\n\n\nThis visualization shows us that our observed test statistic is unlikely if there truly is no relationship between sex and body size, i.e. there is no difference in mean body size between male and female Adelie penguins.\n\n\n5.5.1.6 Step 6: Get the p-value.\nOnce we have calculated a test-statistic and compare it to the null distribution, we can estimate the p-value as the probability that our observations or something more extreme than our observations will occur if the null hypothesis is true.\n\n# calculate p value from null distribution and observed statistic\nnull_dist %&gt;%\n  get_p_value(obs_stat = observed_statistic,\n              direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nIf there is no relationship between sex and body weight (null hypothesis) it is extremely unlikely that we would observe a test statistic as or more more extreme than the one calculated based on our data.\n\n\n\n\n\n\nProtip\n\n\n\nWe used the infer package because it makes all the steps explicit and also helps us visualize what it means to compare the observed test statistic to be “likely” or “unlikely” given a sampling distribution. We calculated our p-value based on a randomization-based null distribution. Randomization-based p-values are straightforward to interpret at “under the null distribution we would expect to find a test-statistic as extreme as the one calculated x% of the time”. In our case, our test statistic is so far outside of the distribution that we never observed it, despite 1,000 random samples.\nAlternatively, you can calculate t and compare it to a table of critical values based on degrees of freedom, implemented in R using the function t.test().\n\nt.test(body_mass_g ~ sex, data = chinstrap)\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by sex\nt = -5.2077, df = 62.575, p-value = 2.264e-06\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -569.7903 -253.7391\nsample estimates:\nmean in group female   mean in group male \n            3527.206             3938.971 \n\n\nNotice how our p-value is extremely small but not 0 as reported by the randomization-based p-value. Both are valid approaches.\n\n\n\n\n5.5.1.7 Step 7: Interpret the results and draw conclusions.\nAt this point, we have not actually proven anything, rather we have gathered data that may or may not be likely given the particular null hypothesis. Our next step is to compare the p-value to the significance threshold that we defined earlier to determine if we will reject or fail to reject our null hypothesis.\n\nif our p-value is higher than \\(\\alpha\\) we fail to reject the null hypothesis and our results are not statistically significant.\nif our p-value is smaller than \\(\\alpha\\) we reject the null hypothesis and report our results as statistically significant.\n\n\n\n\n\n\n\nGive it a try\n\n\n\nWhen you are writing a report, you would give a brief summary of your data and results of any statistical tests your performed in the results section. If our results are statistically significant, we interpret this as supporting the alternate hypothesis in the discussion.\nWrite a brief summary (2-3 sentences) of how you would state your findings in the results section and then write 2-3 sentences of how you might interpret this results in your discussion.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe could state our results along the lines of\n\nMean body mass of female and male Chinstrap penguins is 3,527 +/- 285g and 3,939 +/- 363g, respectively. The difference in mean body mass is significant (p &lt; 0.05).\n\nYou might report the exact p-value or as here that it is smaller than the significance threshold.\nIn the discussion we would interpret these results in light of our hypothesis.\n\nThe observed significant difference of body mass for male and female penguins is consistent with our hypothesis that Chinstrap penguins exhibit sexual dimorphism in terms of body weight.\n\nIn a stats class you are likely going to be asked to explicitly state “reject” or “fail to reject” to demonstrate your understanding of how hypothesis testing works, however, in a report you would use phrases like “is consistent with our hypothesis” or “supports our hypothesis” if results are significant. If results are not statistically significant, we would use phrases like “is inconsistent with our hypothesis” or “does not provide evidence for our hypothesis”.\n\n\n\n\n\n\n5.5.2 \\(\\chi^{2}\\) test to evaluate the difference between frequencies\n\n5.5.2.1 Step 1: Define your research question or goal.\nLet’s look at another question we might ask.\nYou observe that Biscoe and Dream island are quite different from each other in terms of geographic features and, critically, available areas that can be used as habitat differ. In each case, they co-occur with other species. You hypothesize that the proportion of penguins that are Adelie penguins are going to differ by island.\n\n\n5.5.2.2 Step 2: Specify what measurements you plan to compare and collect the data.\nWe use a stratified sampling design that encompasses all the habitats on each island so that we obtain a sample that isn’t biased toward one species or the other. Then we count the number of individuals that we observe for either Adelie or “other” penguins.\nLet’s take a look at our data.\n\n# count number of adelie vs other species\nislands &lt;- penguins %&gt;%\n  filter(!island == \"Torgersen\") %&gt;%\n  mutate(species = ifelse(species == \"Adelie\", \"Adelie\", \"other\"))\n\n# view counts per island\nislands %&gt;%\n  group_by(island) %&gt;%\n  count(species) %&gt;%\n  pivot_wider(names_from = species, values_from = n) %&gt;%\n  kable()\n\n\n\n\n\n\nisland\nAdelie\nother\n\n\n\n\nBiscoe\n44\n124\n\n\nDream\n56\n68\n\n\n\nTable 5.3: Comparison of number of observed Adelie and other penguins.\n\n\nWe probably want to convert the absolute counts into percent for easier comparison among the islands.\n\n# view counts per island\nislands %&gt;%\n  group_by(island) %&gt;%\n  count(species) %&gt;%\n  mutate(percent = n/sum(n)*100) %&gt;%\n  select(-n) %&gt;%\n  pivot_wider(names_from = species, values_from = percent) %&gt;%\n  kable(digits = 2)\n\n\n\n\n\n\nisland\nAdelie\nother\n\n\n\n\nBiscoe\n26.19\n73.81\n\n\nDream\n45.16\n54.84\n\n\n\nTable 5.4: Comparison of proportion of observed Adelie and other penguins on Biscoe and Dream island.\n\n\n\n\n5.5.2.3 Step 3: State your your null and alternate hypothesis and predict the outcome assuming the null hypothesis is true.\n\n\n\n\n\n\nGive it a try\n\n\n\nUse the standard way of formulating your null and alternate hypothesis that we established earlier to formulate your hypothesis and predict what your results should look like for each case.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe null hypothesis is a claim that there is no effect.\n\n\\(H_{O}\\) The proportion of penguins that are Adelie penguins does not depend on the sampled island.\nPrediction of the results: The proportion of penguins that are Adelie penguins is the same on each island.\n\nThe alternative hypothesis is a claim that there is an observed effect.\n\n\\(H_{a}\\) There is a relationship between the observed island and the proportion of penguins that are Adelie penguins.\nPrediction of the results: The proportion of penguins that are Adelie penguins will differ depending on which island is observed.\n\n\n\n\n\n\n5.5.2.4 Step 4: Specify a significance threshold.\nWe will specify our threshold of significance \\(\\alpha\\) as 0.05. This means that if our observed results have a less than 5% probability occurring assuming our null distribution we will reject our null hypothesis.\n\n\n5.5.2.5 Step 5: Calculate your test statistic and generate sampling distribution.\nIn this case, we are comparing frequencies. The appropriate test to be using is a \\(\\chi^{2}\\) test of independence to test the association between two categorical variables (island and species).\nThe \\(\\chi^{2}\\) test statistic will account for sample size and the size of difference in sample frequencies. The smaller the sample size and the difference in frequencies, the more likely our sample frequencies are different due to sampling error and not because the true population frequencies are different.\nOur data is in the format of what we call a 2x2 table.\n\n\n\n\nColumn 1\nColumn 2\nTotal\n\n\n\n\nRow 1\nf11\nf12\nR1\n\n\nRow 2\nf21\nf22\nR2\n\n\nTotal\nC1\nC2\nn\n\n\n\nfor \\(f_{ij}\\) being the observation in the \\(i\\)th row and the \\(j\\)th column, \\(C_{1}\\), \\(C_{2}\\), \\(R_{1}\\), and \\(R_{2}\\) are the column and row totals and \\(n\\) is the total number of observations.\nThen we can calculate \\(\\chi^{2}\\) as\n\\[\n\\chi^{2} = \\frac{n(f_{11}f_{22}-f_{12}f_{21})^{2}}{C_{1}C_{2}R_{1}R_{2}}\n\\] Let’s go ahead and calculate the observed test statistic.\n\n# calculate observed statistic\nobserved_statistic &lt;- islands %&gt;%\n  specify(response = species,\n          explanatory = island) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nDropping unused factor levels Torgersen from the supplied explanatory variable\n'island'.\n\n\nWarning: The statistic is based on a difference or ratio; by default, for\ndifference-based statistics, the explanatory variable is subtracted in the\norder \"Biscoe\" - \"Dream\", or divided in the order \"Biscoe\" / \"Dream\" for\nratio-based statistics. To specify this order yourself, supply `order =\nc(\"Biscoe\", \"Dream\")` to the calculate() function.\n\n# print test statistic\nobserved_statistic\n\nResponse: species (factor)\nExplanatory: island (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  10.6\n\n\nOur next step is to simulate a null distribution. We can do this like we did earlier using a permutation where we shuffle the response and explanatory variables so that each species value in a sample is matched with a random island. Then we calculate the test statistic and continue that process to build a distribution.\n\n# create null distribution\nnull_dist &lt;- islands %&gt;%\n  specify(response = species,\n          explanatory = island) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, \n           type = \"permute\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nDropping unused factor levels Torgersen from the supplied explanatory variable\n'island'.\n\n\nWarning: The statistic is based on a difference or ratio; by default, for\ndifference-based statistics, the explanatory variable is subtracted in the\norder \"Biscoe\" - \"Dream\", or divided in the order \"Biscoe\" / \"Dream\" for\nratio-based statistics. To specify this order yourself, supply `order =\nc(\"Biscoe\", \"Dream\")` to the calculate() function.\n\n# print first few lines\nhead(null_dist)\n\nResponse: species (factor)\nExplanatory: island (factor)\nNull Hypothesis: independence\n# A tibble: 6 × 2\n  replicate   stat\n      &lt;int&gt;  &lt;dbl&gt;\n1         1 0.0666\n2         2 0     \n3         3 0.241 \n4         4 0     \n5         5 1.01  \n6         6 0     \n\n\nLet’s visualize our null distribution along with our observed test statistic. The distribution that we generated based on our data with be a histogram. Additionally, we can calculate a \\(\\chi^{2}\\)-distribution based on theory. We will print that on top for comparison.\n\n# visualize the null distribution and test statistic!\nnull_dist %&gt;%\n  visualize(method = \"both\") + \n  shade_p_value(observed_statistic,\n                direction = \"greater\")\n\nWarning: Check to make sure the conditions have been met for the theoretical\nmethod. {infer} currently does not check these for you.\n\n\n\n\n\nAgain, our observed statistic is improbable to observed if there was not relationship between island and proportion of Adelie penguins observed as stated by our null hypothesis.\n\n\n5.5.2.6 Step 6: Get the p-value.\nOur next step is to calculate the p-value.\n\nnull_dist %&gt;%\n  get_p_value(obs_stat = observed_statistic,\n              direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.002\n\n\n\n\n\n\n\n\nProtip\n\n\n\nAgain, we used the implementation in the infer package using a randomized approach to generate a null distribution and determine a p-value.\nAlternatively, we can use the pchisq() function to have R look up the p-value in a critical value table. We need to specify the degrees of freedom as number of categories - 1. In this case, we are using a null distribution that is based on theory as opposed to randomizing our observed distribution.\n\npchisq(observed_statistic$stat, 1, lower.tail = FALSE)\n\n  X-squared \n0.001146154 \n\n\nAdditionally, we could create a 2x2 table to use as an input for the function chisq.test().\n\n# create 2x2 table\ndf &lt;- islands %&gt;%\n  group_by(island) %&gt;%\n  count(species) %&gt;%\n  pivot_wider(names_from = species, values_from = n) %&gt;%\n  column_to_rownames(\"island\")\n\n# apply chi sqared test\nchisq.test(df)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  df\nX-squared = 10.575, df = 1, p-value = 0.001146\n\n\n\n\n\n\n5.5.2.7 Step 7: Interpret the results and draw conclusions.\nOur last step is to compare our p-value to our \\(\\alpha\\) value. In this case, our p-value is smaller than our specified threshold of significance so we would reject our null hypothesis.\n\n\n\n\n\n\nGive it a try\n\n\n\nWrite a brief summary (2-3 sentences) of how you would state your findings in the results section and then write 2-3 sentences of how you might interpret this results in your discussion.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe could state our results along the lines of\n\nAdelie penguins comprise 26.2% and 45.2% of penguins observed on Bisco and Dream island. This difference is statistically significant (p &lt; 0.05).\n\nYou might report the exact p-value or as here that it is smaller than the significance threshold.\nIn the discussion we would interpret these results in light of our hypothesis.\n\nWe observe Adelie penguins to be significantly more common on Dream Island (45%) compared to Bisco Island (26%). This is consistent with our hypothesis that differences in geographic parameters including the availability of breeding habitat is and important factor determining which penguin species are more commonly encountered.\n\nNote, that in your discussion you would like refer to other examples from the literature showing that availability of habitat and co-occurring species are important for these and/or other species. Then you would discuss that you did not specifically test for habitat conditions. In this or a later study you would likely follow up by quantifying the contributing factors in terms of geography, breeding and other habitat availability, which species the specifically co-occur with etc. and performing a more sophisticated test than what we have done here which really is just demonstrating the the demographics differ among islands, we cannot actually contribute it to specific factors."
  },
  {
    "objectID": "A06_writing-report.html#learning-objectives",
    "href": "A06_writing-report.html#learning-objectives",
    "title": "6  Writing a scientific report",
    "section": "6.1 Learning Objectives",
    "text": "6.1 Learning Objectives\nAfter completing this activity, you should be able to\n\nList the four (five) sections of a research paper and what their central purpose is.\nOutline the structure of a lab report.\nDescribe key components for each section of a lab report.\n\nA scientific research paper is structured into four sections:\n\nAn introduction of relevant background information providing context for your central question/hypothesis question and the objectives of your study.\nA clear description and justification of your experimental design, including the methods you used both to collect and analyze your data.\nA summary of your results.\nA clear interpretation and discussion of your results leading to an explicit (set of) conclusions.\n\nIn addition, you will have a “fifth” component, the Abstract which summarizes the Introduction, Methods, Results, and Discussion/Conclusion in a single paragraph. Generally, scientific research papers have an hourglass shape, where they start off very broad, become increasingly narrow as you describe your very specific study and then become more broad as you interpret and discuss the results in the broad context of the original research question you posed.\nYou will structure your lab reports in this format - don’t worry, we will give you guidance, though as you progress through the semester and (hopefully) feel more confident you will increasingly become more independent and require less scaffolding."
  },
  {
    "objectID": "A06_writing-report.html#lab-report-components",
    "href": "A06_writing-report.html#lab-report-components",
    "title": "6  Writing a scientific report",
    "section": "6.2 Lab report components",
    "text": "6.2 Lab report components\n\n6.2.1 Title\nYour title should be informative and clearly state the main focus or purpose of your study. It does not need to be especially clever or thought-provoking, you focus should be on clear communication. There are typically two ways to do this, either by a stating what was done (“Effect of Plant X on Radish Seed Growth.”) or what was found (“Plant X inhibits radish seed growth.”). You can consider how broad to make your title (“Allelopathic effects of Plant X”).\n\n\n6.2.2 Introduction\nYour introduction should establish the context the reader needs to understand the topic.\nThink of your introduction as the funnel or top piece of an hourglass that starts off broad and become increasingly focused on the specific focus of your study.\n\nIntroduce the broad topic/general research question you are asking, including why this is an important question to be asking. Give a brief overview of the current state of understanding of the research question. Make sure you introduce any key terms/concepts to set up the context of a large question/scientific field or overarching theme.\nIncreasingly narrow your scope down to the specific focus of your study. Describe why your question is relevant and how it furthers the understanding of this question: Even though you cannot fully answer the big question you laid out, you can contribute to part of the answer by filling a knowledge gap or clarifying existing conflicting results.\nClearly state the purpose of your research, you should state what your hypothesis is, what you expect to find and why, and how you are testing this1.\n\n1 No details about your methods/results yet at this point, just brief “how”.You should always have at least two paragraphs. The first, sets up the broad context2.The last paragraph summarizes what the specific study at hand is investigating and how that will be done. It forms a “bridge” between the introduction that sets up relevant background information (why is my question important?) and the methods section which is a very detailed account of how data was acquired (experimental design), processed, and analyzed.2 For a lab report you can likely do this in a single paragraph, in a longer research paper you will likely then take several following paragraphs to flesh out relevant background information\nYou can use a “formula” to make sure that your introduction ends with a clear statement of the purpose of your study that summarizes your introduction.\n\nIn this study, we investigated [CENTRAL QUESTION OR HYPOTHESIS]. To do this we used [DESCRIPTION OF HOW DATA WAS GENERATED] to [METRICS THAT WERE CALCULATED].\n\nYou should always be able to make a 2-3 sentence statement summarizing what you are investigating and how you did it, it’s a good self-check that you know what you’re trying to accomplish.\nChecklist for a good introduction\n\nAm I giving just enough context for my reader to understand my research question and rationale for my hypothesis or is there a lot of unrelated information?\nDo I explain how my study fits into the current understanding of the field and how it contributes to the overarching research question?\nAm I clearly defining all key terms?\nDid I include components that should actually be in my methods, results, or discussion?\n\nYour introduction should comprise about 15-20% of your lab report.\n\n\n6.2.3 Methods\nYour method section should describe your study design, how you collected, processed, and analyzed the data.\nYour “Introduction Funnel” has now landed you in the most specific part of your report, where you need to describe how you conducted your study, specifically how you gathered and analyzed your data. It should be sufficiently detailed so your study could be repeated by somebody else and reproduce the same results. Make sure you strike the balance of providing necessary details and why you made specific decisions without getting lost in the weeds3. It can be helpful to structure your method section with subheadings to include important components of your study, e.g.3 For example, we need to know that seedling length was measured in millimeters but we don’t need to know that each seedling was removed from the petri dish, laid out on a paper towel, measured by lab partner A etc\n\nStudy area: Describe geographic location, habitat of field experiment, you may want to include a map as a figure.\nExperimental design: Concisely describe how you set up your experiment in terms of sample size, replicates, what measurements where taken to produce the data set that was then analyzed.\nData Analysis: include how you manipulated data, specific calculations, descriptive statistics to summarize data (what metrics you produced), statistical tests.\n\nRemember, that your methods section should be written in paragraph form, not as a list of individual steps. Justify specific decisions you made about design where needed. You should not include a list of materials, rather you would include that information in the paragraph at the appropriate time, including specialized equipment, concentrations of chemical solutions etc.\nChecklist for a good methods section\n\nDid I provide enough details to allow study to be reproduced?\nDid I structure my methods in easy-to-follow, logical paragraphs as opposed to a bullet list or recipe-style set of instructions?\n\nYour methods should comprise about 10-15% of your report.\n\n\n6.2.4 Results\nThis section should present your results based the descriptive and inferential statistics you applied to analyze your data set.\nYou results section should include a written description along with data visualizations in the form of tables and figures that present your results in a way that makes it easy for the reader to identify key findings4. Focus on describing trends and patterns and report which findings are significant. Highlight notable results, when you mention anomalous and unexpected results make sure you pick up on those in the discussion. Remember that you should not be interpreting your data by comparing them to results from other studies, or interpreting them in the context of your research hypothesis.4 You should not include any raw data, it should always be summarized/analyzed.\nGenerally, you should only present your results in only one format (text, table, figure). So for example, if you have a table with the mean +/- standard deviation of your seedling lengths you would describe your results as “Seedlings exposed to plant xx where xx mm shorter (see Table 1)”. In you text you are describing the pattern, you are being specific and quantitative but you are not repeating the information in the Table which has the exact mean values. Using the written description allows you to give context to your results (“increased”, “wider”, “smaller”) and guide the reader through your results that you have spent a lot of time thinking about but are new to them.\nWhile your written description alone should provide all the needed information without the figures and tables, your figures and tables should complement the text of your report. You do want to make sure that your figures and tables stand alone5 by giving them a descriptive figure caption comprising of a descriptive title along with any other information that is needed to understand the information being presented. Figures and Tables should be numbered, by convention captions are placed below figures and above tables. Any figure or table that you include should be referenced in your text.5 This means they should be understood without reading the entire paper\nChecklist for a good results section\n\nHave I described all my summarized data and statistical results?\nDid I point out all the trends and patterns?\nDid I highlight notable/anomalous results?\nDid I report which results are significant?\nDid I make sure not to include an interpretation or explanation of my results?\n\nYour results should comprise about 10-15% of your lab report, figures/tables not included.\n\n\n6.2.5 Discussion\nA good discussion evaluates context, acknowledges constraints, and justifies conclusions.\nNow it is time to take your specific results and broaden the scope of your report as you interpret and discuss your results. Your discussion should mirror your introduction in structure and scope, as now you start with your specific study and build your discussion back out to the broad context you set up initially.\nTransition from your results section by making an explicit statement whether your results support your research question or not6. A good way to do this is by reiterating the original (broad) question you asked with short description of how you specifically investigated it, your key results7, and then interpreting/discussing those. Similar to the last paragraph of your introduction section, you can essentially follow a fill-in-the-blank-formula, which as you become more comfortable communicating your research will sound a little less formulaic and a little bit more you8:6 Remember that we generally use the terms “support” and “reject” when referring to a hypothesis not “prove” and “disprove”7 Make sure that you are making a very concise summary, not restating all of your results in detail.8 Notice how this statement is quite similar to how you ended your introduction, except now you have the results/an answer.\n\nIn this study, I investigated [broad question asked/hypothesis tested]. To achieve this, I [specific data set + analysis used]. I found that [key results] which [statement about how this does or does not support your hypothesis].\n\nBuild on this statement to write an effective discussion that reiterates the key results and interprets them, acknowledges constraints of the study design and other possible caveats limiting clear interpretation, states clear conclusions/take-home messages, and points to results (figures or tables) as evidence thereof. In addition to ending with a clear set of conclusions you should include these four components:\n\nInterpret your results and compare them to your expectations and other studies9:\n\nDid your results match your predictions?\nDo your results support your hypothesis? How do you interpret them in that context?\nWhy do you think you ended up with these results?\nAre your results consistent with similar studies/your classmates results? Different? Why?\n\nDiscuss constraints/limitations, inconsistencies, other possible explanations10\n\nAcknowledge and discuss constraints, point out shortcomings and any caveats for how your data should be interpreted.\nExplain unexpected results.\nIf results are inconsistent/ambiguous why are you confident you are interpreting them way you are?\nAre the other possible explanations? Other reasons of why you may have ended up with the results you did?\nAre there any errors, assumptions, ambiguities etc that could have impacted your results? Why you are still confident interpreting you data as you are?\nHow might you redesign the experiment for more clear results?\n\nDiscuss implications of your results11.\n\nHow do your results contribute to answering your overarching research question?\nWhy is your study important? What do we know now that we didn’t before?\n\nFuture steps\n\nWhat questions where generated?\nWhat is the next step to even better understand your overarching research question?\n\n\n9 In this semester we will generally keep the “compare to other studies very small, don’t worry, you won’t have to go on an extended literature search.10 This is still really specific to your study.11 Now you are broadening your scope to the broader context in which you asked your questionChecklist for a good discussion\n\nDoes your discussion state whether it supports your hypothesis?\nDid you discuss any issues with your study?\nDid you explain how your results fit into and contribute to the broader research question?\n\nYour discussion should comprise about 40-45% of your Lab report12.12 The more practice you get, the more you will find that this is the case."
  },
  {
    "objectID": "A06_writing-report.html#general-notes-on-scientific-reports",
    "href": "A06_writing-report.html#general-notes-on-scientific-reports",
    "title": "6  Writing a scientific report",
    "section": "6.3 General Notes on scientific reports",
    "text": "6.3 General Notes on scientific reports\nThe way you structure and format your report plays a critical role in how the content is delivered. Your goal should be to produce a report that is well-organized, coherent, and the presentation of the information is clear, engaging, and enhances the understanding. This means, that you want to lower the cognitive burden of your reader, i.e. help them think by guiding them through your thought process - you want them to come to the same conclusion as you. Structure your information to help them do this and make that structuring explicit by using headers and sub-headers to make the hierarchy of information and connections obvious to them.\nHere are a few things you can incorporate in terms of structure to help keep your reader engaged and following the story you are telling.\n\n6.3.1 Think of every paragraph as a container of a single thought\nParagraphs are your fundamental building block. A paragraphs says one thing: Your first sentence13 summarizes the overall message or main points of a paragraph and functions as the logical transition from the previous paragraph. The rest of the paragraph supports, elaborates, and defends it. Effective paragraphs comprise about 100 - 200 words, much longer and you usually have more ideas creeping in, much shorter and you’re likely missing a key point. A good way to proof read your paragraphs is to read it out loud to help you spot sentence fragments, run-on sentences, places where the subject and verb are mismatched.13 This is generally referred to as a topic sentence\n\n\n6.3.2 Structure your thinking and then your writing\nRemember to embrace the hour glass structure: Your introduction should start broad, then become more specific as you narrow in on your specific question. After describing your study design and completing your analysis and describing your results (this is the most specific piece of content) you start narrow with your results and then become increasingly broad in your discussion points as you put your results back into the original context of the question you asked and end with your conclusions.\nBefore you start writing paragraphs, create an outline to clarify your thinking and then your writing. Start with the key components, then for each section add your main points and the supporting details14. At this bare bones stage you are just looking at the skeleton and so it is easier to spot anything in that paragraph that does not belong in there or needs to be in another place. Flesh out each “main point” into a topic sentence and gradually build out your supporting points into full sentences.14 You may find that some of your supporting details are actually main points and vice versa it is a lot easier to move those around at this point.\n\n\n6.3.3 Don’t write your report linearly:\nStart with the last statement in your introduction15:15 Think of this as a trailer previewing the remainder of a season “This season on XXX”\n\nIn this study, we investigated [CENTRAL QUESTION OR HYPOTHESIS]. To do this we used [DESCRIPTION OF HOW DATA WAS GENERATED] to [METRICS THAT WERE CALCULATED].\n\nNext, move onto your methods. Then, take the subheading of your methods and fill in the results for each section. Now that you have your results you can write the first section of your discussion16:16 Think of this as the quick summary before a tv episode “Previously, on XXX\n\nIn this study, I investigated [broad question asked/hypothesis tested]. To achieve this, I [specific data set + analysis used]. I found that [key results] which [statement about how this does or does not support your hypothesis].\n\nBuild out your discussion, by starting with the interpretation of your results and then your discussion. Finally, widen the scope to discuss how your study fits into the broad research context. Think about your introduction and discussion as bookends framing your specific study: There is an element of them being mirror images where the key points are quite similar with the difference being that in the discussion you are revisiting key statements and questions you raised in the introduction with the specific information you have just gleaned.\nNow, that you know what that broad research context is you know what themes you need to set up in your introduction. And then as a final touch, come up with a descriptive title for your lab report.\n\n\n6.3.4 Use parallel structures where possible:\nThe hour glass structure, Introduction/Discussion as bookend with matching last intro/first discussion paragraphs are parallels that help your reader because it is easy to follow your line of thought. Additionally you can add explicit parallels to lower the cognitive burden by intentionally structuring steps/points consistently in the same way. For example, if you list three major parts in your last introduction paragraph list them in the same logical sequence you will complete them in your data analysis section, in your data analysis section use sub-headers to indicate which component you are currently completing and then summarize your results after each section (this becomes even more important when you separate methods and results into separate sections). In your first paragraph of the discussion list those key take-aways in the same sequence and then in your discussion, have the paragraphs discussing each key point in the same sequence as well.\n\n\n6.3.5 Don’t just give the what, make sure you include the why:\nDon’t just list what you are doing, always explicitly state why you are taking this step, e.g. don’t say “I made a scatterplot” say “I am exploring the relationship of xx and xx using a scatterplot”. An easy formulaic way to write this out is “I am going to do xxx to determine xxx”. Similarly, where this is appropriate justify why this is an appropriate choice.\n\n\n6.3.6 Be precise and accurate\nYou need to make sure that your reader is understanding the exact meaning that you want to convey.\n\nUse formal, written English\nMake sure you are using correct terminology. For example, only use “significant” when you are talking about statistical results.\nAvoid being jargony17.\nBe specific and concrete. For example, use “for 48 hours”, not “a period of time”.\n\n17 Make sure people know the words you are using, for example, by defining them the first time they are used.\n\n6.3.7 Use active voice\nFor this course, use the active voice when possible as this will allow you to write in a more direct and clear fashion.\nWhile passive voice is not grammatically incorrect, it can lead to awkward syntax and be less clear. There is some debate whether scientific reports should be written in passive voice18 or in passive voice (“People do science”). Passive voice has a sentence construction of Object-Verb-Subject which makes the object the focus of the sentence and can obscure who is acting. By contrast, active voice places an emphasis on who is acting by making the subject the focus of the sentence (Subject-Verb-Object).18 “Science happens”; this emphasizes the objectivity\n\n\n6.3.8 Use the correct Verb Tense\nFor lab reports we use past, present, and future tenses in specific context and you should make sure you are being consistent in how you do this. You should be using the past tense when you are referring to events in the past, including results obtained in the past, and tasks you completed (methods section). You should use the present tense in the Introduction and Discussion section when you are presenting established knowledge and the implications of your results. You would only use future tense when you are talking about future steps you might take."
  },
  {
    "objectID": "A05_inferent-stats.html#homework",
    "href": "A05_inferent-stats.html#homework",
    "title": "5  Introduction to inferential statistics",
    "section": "5.6 Homework",
    "text": "5.6 Homework\nComplete the questions below to review some of what you have learned about descriptive statistics. Render your document as an html-file and submit that document through Canvas. Double check that you are submitting the html file, not the quarto document.\n\n\n\n\n\n\nConsider this\n\n\n\nSampling error and standard error to important concepts that you should understand. Unfortunately, they have very similar names.\n\nIn your own words, describe what sampling error is and what it tells us about a data set and how it should inform how you interpret your results.\nIn your own words, describe what sampling error is, key factors that can contribute to sampling error being larger or smaller, and how it should inform how you interpret your results.\n\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nA statistical test usually gives us a “probability” or P-value.\n\nIn your own words, describe what the p-value tells you.\nArgue whether you would expect the p-value from a statistical test to be higher or lower when sample sizes are small (Remember to explain why).\nArgue whether you would expect the p-value from a statistical test to be higher or lower when variability is high (Remember to explain why).\n\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\n\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nSome researchers want to know whether or not Chinstrap penguins exhibit sexual dimorphism in their flipper length in column flipper_length_mm.\n\nState your null and alternative hypothesis and predict what the results should look like if each is correct.\nModify the code below to run the test and obtain a p-value.\nInterpret the results to draw conclusion about your research question.\n\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\nState your hypothesis:\nNull hypothesis\n\n\nprediction\n\nAlternative hypothesis\n\n\nprediction\n\nCalculate your test statistic and obtain your p-value. You need to fill in the correct response and explanatory variable (replace columnname with the columns that contain the correct information).\n\n# format data set\nAdelie &lt;- penguins %&gt;%\n  filter(!is.na(sex) & species == \"Adelie\")\n\n# calculate the observed statistic\nobserved_statistic &lt;- Adelie %&gt;%\n  specify(response = columnname,  # specify response/dependent\n          explanatory = columnname) %&gt;%         # specify explanatory/independ\n  calculate(stat = \"diff in means\",      # test statistic\n            order = c(\"male\", \"female\")) # order to subtract the mean values\n\n# print observed value\nobserved_statistic\n\nnull_dist &lt;- Adelie %&gt;%\n  specify(response = columnname,\n          explanatory = columnname) %&gt;%               # specify values we are interested in\n  hypothesize(null = \"independence\") %&gt;%       # null hypothesis: sex and weight are independent\n  generate(reps = 1000,                        # generate 1000 samples for null distribution\n           type = \"permute\") %&gt;%               # randomly assign sex to weight to break association\n  calculate(stat = \"diff in means\",            # statistic is t\n            order = c(\"male\", \"female\"))       # order to subtract the mean values\n\n# calculate p value from null distribution and observed statistic\nnull_dist %&gt;%\n  get_p_value(obs_stat = observed_statistic,\n              direction = \"two-sided\")\n\nInterpret your results:\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nYou want to know if the bill depth (in column bill_depth_mm) differs between Adelie and Chinstrap penguins.\n\nState your null and alternative hypothesis.\nModify the code below to run the test and obtain a p-value.\nInterpret the results to draw conclusion about your research question.\n\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\nState your null and alternative hypothesis and predict the outcome for each:\nNull hypothesis\n\n\nPrediction\n\nAlternative hypothesis\n\n\nPrediction\n\nModify the code to calculate the test statistic and p-value (replace the columnname with the appropriate metrics).\n\n# format data set\ndf &lt;- penguins %&gt;%\n  filter(!is.na(sex) & species %in% c(\"Chinstrap\", \"Adelie\"))\n\n# calculate the observed statistic\nobserved_statistic &lt;- df %&gt;%\n  specify(response = columnname,         # specify response/dependent\n          explanatory = columnname) %&gt;%  # specify explanatory/independ\n  calculate(stat = \"diff in means\",      # test statistic\n            order = c(\"Chinstrap\", \"Adelie\")) # order to subtract the mean values\n\n# print observed value\nobserved_statistic\n\nnull_dist &lt;- df %&gt;%\n  specify(response = columnname,\n          explanatory = columnname) %&gt;%        # specify values we are interested in\n  hypothesize(null = \"independence\") %&gt;%       # null hypothesis: sex and weight are independent\n  generate(reps = 1000,                        # generate 1000 samples for null distribution\n           type = \"permute\") %&gt;%               # randomly assign sex to weight to break association\n  calculate(stat = \"diff in means\",            # statistic is t\n            order = c(\"Chinstrap\", \"Adelie\"))  # order to subtract the mean values\n\n# calculate p value from null distribution and observed statistic\nnull_dist %&gt;%\n  get_p_value(obs_stat = observed_statistic,\n              direction = \"two-sided\")\n\nInterpret your results:"
  },
  {
    "objectID": "A01_intro-exp-design.html#sec-scientific-inquiry",
    "href": "A01_intro-exp-design.html#sec-scientific-inquiry",
    "title": "1  Scientific Method & Experimental Design",
    "section": "1.1 Scientific inquiry",
    "text": "1.1 Scientific inquiry\n“Science” can be define as a systematic, evidence-based approach to understanding the natural world1. It comprises both the body of existing knowledge and is a process of inquiry with the goal of understanding, explaining, and predicting natural phenomena. Science relies on observations, empirical2 evidence, experimentation, and the formulation of hypothesis and theories to describe the underlying principles that govern the natural world. Scientific knowledge is provisional and subject to revision as new data and evidence become available which necessitate modifying or replacing scientific theories.1 We frequently use the term natural sciences to refer to fields of science specifically centered around the physical world and the phenomena and processes that govern it, which typically refers to broadly to life sciences/biology, chemistry, physics, geology, though not everyone is in agreement in what to include. Frankly, depending on the type of research questions somebody is interested in. For example, a psychologist might consider themselves a natural scientist compared to a social scientist.2 Empirical in contrast to theory is based on verifiable observation/experiences.\nAll forms of science share a common goal - to know more and better understand how the world operates. For scientific inquiry we typically employ two types of logical thinking inductive reasoning and deductive reasoning3. For descriptive science our goal is to observe, explore and discover. Here, we typically employ inductive reasoning with the goal of developing a hypothesis or theory. To do this, we compile related observation to draw general conclusions. We make observations and record them4 to build large data sets that allow us to infer generalizations based on an analysis. By contrast, hypothesis-driven science begins with a specific question/problem and a proposed testable or answer/solution. In this case, we are using deductive reasoning to test a theory: We start with a general principle and use that to predict a specific outcome assuming it is true and we design an experiment to determine if our results support the hypothesis. Even though we have just described these two types of reasoning as a binary to understand the difference between the approaches, most scientific studies rely on both approaches.3 Inductive reasoning moves from the particular to the general. Deductive reasoning moves from the general to the particular.4 The data sets can be qualitative or quantitative and are frequently supplemented with drawings, pictures/videos."
  },
  {
    "objectID": "A01_intro-exp-design.html#sec-the-scientific-method",
    "href": "A01_intro-exp-design.html#sec-the-scientific-method",
    "title": "1  Scientific Method & Experimental Design",
    "section": "1.2 The Scientific method",
    "text": "1.2 The Scientific method\nThe scientific method is a key process that guides the process of scientific inquiry with the goal of advancing knowledge. It follows a structured process of formulating hypotheses based on observations, designing repeatable5 experiments, collecting data, and drawing conclusions based on evidence.5 In the natural sciences, the scientific method centered around repeatable experiments is arguably the key process. However, for example in paleontology repeatable experiments are not possible. Still, they follow the same process formulating a hypothesis that may be supported or contradicted by new findings\nGenerally the scientific method starts with an Observation. For example, you might notice that on a specific island birds there is a large number of birds nest on the ground. Observations can also include your own or other scientists previous research. So rather than have visited the island yourself, you may have read about it in the literature.\nThe next step is to ask a Research Question based on these observations. Research questions can be descriptive or causal. A descriptive question quantifies an observation and seeks to identify trends and patterns. In this case we might ask, “What proportion of individuals are building their nests on the ground?” or “Are all species building their nests on the ground?”. By contrast, a causal question focuses on understanding why we observe trends and patterns. In our example, we would likely ask “Why are birds building their nests on the ground?”. Frequently, a descriptive question leads to a causal one because once we have an idea of what the trends and patterns are we want to know why we are observing them. So for example, if we had pursued our initial question we may have determine that “90% of nests are built on the ground” or “Species X always builds their nests on the ground.” and we may formulate a more specific question such as “Why does species X build their nests on the ground and species Y does not?”.\nNext, we need to formulate a Hypothesis as a proposed answer to the research question based on prior knowledge. Rationale for hypothesis should be supported by previous research and understanding of an area of research. In this case, as ecologists, we might have read about other examples of ground nest-building or the prevalence of flightless birds in the absence of predators and so we hypothesize that in this case the island might lack predators for species X but there are predators present for species Y. A hypothesis must be both testable and falsifiable. Testable means that we can generate predictions and use observation and experimentation to determine if they are correct. Falsifiable means that we must be able to design experiments so results can disprove the hypothesis.\nOur next step is to make a Prediction that describes the expected observed outcome assuming our hypothesis is true. Prediction and hypothesis are distinct from each other, even though they are frequently used interchangeable. A prediction is frequently formulated as a If-Then statement. In our example, we might predict that for each species on islands where predators co-occur we observe a small number of birds nests on the ground and on islands where there are no or few predators that number would be larger.\nFinally, we design and execute an experiment or study to test our prediction. In this case, we would need to identify a series of islands inhabited by the bird species that vary in their level of predator prevalence and quantify the proportion of nests built on the ground. Then we would determine if our results are consistent with our prediction or not.\nGenerally, once we have the answer, we find that we have more questions and we iterate where our results and conclusion become the next set of observations that start the process over again."
  },
  {
    "objectID": "A01_intro-exp-design.html#sec-designing-controlled-experiments",
    "href": "A01_intro-exp-design.html#sec-designing-controlled-experiments",
    "title": "1  Scientific Method & Experimental Design",
    "section": "1.3 Designing Controlled Experiments",
    "text": "1.3 Designing Controlled Experiments\nThe scientific method hinges on robustly designed experiments. The goal of an experiment is to systematically test a hypothesis, Therefore, we must pay careful attention to ensure our design enables this. In a controlled experiment, the we manipulates one or more independent variables to observe the effect on a dependent variable while carefully controlling other relevant variables to ensure consistent conditions across treatments. As a result, we are able to establish cause-and-effect relationships, with a high level of certainty based on our results.\nHere are the key steps to Experimental Design:\n\nIdentify the dependent and independent variables you are interested in and how they are related.\nDetermine how you will measure your dependent variable.\nFormulate a testable, falsifiable hypothesis and predict the outcome.\nDesign experimental treatments that manipulate your independent variable.\nIdentify variables that you need to control for and modify your design to accommodate this.\nFinalize aspects like sample size, replicates, and randomization.\n\n\n\n\n\n\n\nNote\n\n\n\nEven though we are presenting these as consecutive steps, the process of designing a experiment is frequently iterative where you need to go back and make adjustments or you may be considering components of the experimental design at the same time before making final decisions to ensure a good design.\n\n\nLet’s think through the nuances of experimental design using another example.\nYou make the observation, that when you plant onions, garlic, leeks, or chives on the perimeter of your garden, you lettuce is less affected by slugs. You initially as a descriptive research question: “Do onions, garlic, leeks, or chives reduce the impact of slugs?”. Next, you formulated the hypothesis “Onions, garlic, leeks, or chives repel slugs.” and test this by collecting data from a range of gardens that did or did not have onions, garlic, leeks, or chives planted on the perimeter and you found that indeed the “slug-affectedness” is significantly lower. Based on this new observation you are ready to pose a causal research question: “Why do onions, garlic, leeks, or chives repell slugs?”. Before your formulate your research hypothesis you do some additional research and learn that these plants are all part of the Allium family which produce allicin which is toxic to a range of organisms, including slugs. This leads you to postulate a research hypothesis: “Allicin is toxic to slugs and as a result allicin-producing plants repel slugs.”.\nNow, you are ready to design an experiment.\nFirst, we need to figure out what variables we are are in play and how they relate to each other.\nGenerally, we categorize variables as either independent, dependent, or controlled variables. The dependent or response variable is the variable you expect to be affected and are therefore measuring in your experiment6. By contrast, the independent or explanatory variable is the variable or phenomena that you think will affect your dependent variable7. Any remaining variables are what we would consider constants or controlled variables. These are factors that we will want to intentionally keep constant so they do not effect the outcome.6 It is not uncommon to measure more than one dependent variable in an experiment. Frequently you will measure the outcome in more than one way.7 Having only one independent variable makes the interpretation of the an experiment straightforward. However, including two or more independent variables allows us to test not only the effect of each explanatory variable but also how they interact, i.e. how they modify each other’s effects.\nLet’s think about our example to identify what variables are involved and what their relationship is. In this case, we think that the outcome will differ depending whether allicin is present or not. That makes “allicin presence” our explanatory or independent variable. By contrast, the outcome that we think will change is “slug presence” or “slug impact”, which makes that our response or dependent variable.\nBefore we move on to the next step, we will want to figure out how we are going to quantify our dependent variable.\nFrequently this requires having a rough idea of what your experimental design will look like. Let’s say that we are going to have a set up with multiple plots and plants that slugs like to eat. We could directly measure whether or not slugs are present, for example by counting the number of slugs in a specific parameter or we could indirectly measure whether allicin repels slugs by determine the level of impact by assessing the damage to the plants. We could quantify this by counting the number of affected plants or by creating a scale of how strongly a plant is impacted e.g. based on the amount that was eaten. Which is our best option? Well, there might not actually be a “best” option because each gives us an different line of evidence, in that case we might decide to include all three dependent variables: Number of observed slugs beyond an allicin barrier, number of plants affected, and level of plants affected.\nNow we are ready to formulate a hypothesis.\nWe should conceptually differ between a Biological, Scientific or Research Hypothesis as an idea or claim about one specific or very narrow set of natural phenomena in contrast to a Statistical Hypothesis, that is specific to our experimental design. We generally design an study comprising of one or multiple experiments to test the research hypothesis we developed based on a set of observations. However, when we plan that experiment we make specific choices about the variables we are using and for our data analysis we need to define statistical hypothesis that allow us to make predictions specifically about our outcome or dependent variable and test those mathematically. This will always have two parts: A Null hypothesis \\(H_{O}\\) that states that there is no relationship or pattern between the independent and the dependent variables and an Alternative hypothesis \\(H_{a}\\) which stats that there is a relationship or pattern between the independent and dependent variable.\nLet’s apply this to our example:\nOur Biological hypothesis is “Allicin repels slugs because it is toxic to them”. Now that we have identified our variables and how we are going to measure them, we can restate this as a statistical hypothesis and make predictions regarding the outcome of our experiment.\nWe can state our Null hypothesis \\(H_{O}\\) as “The presence of allicin does not affect the presence of slugs” and then make a set of predictions assuming this is true:\n\nThe number observed slugs will be the same both when plants are or are not protected by an allicin border.\nThe number of impacted plants will be the same both when plants are or are not protected by allicin border.\nThe level to which plants are impacted will be the same both when plants are or are not protected by allicin border.\n\nWe also need to formulate our Alternative hypothesis \\(H_{a}\\) as “The presence of allicin impacts the presence of slugs.” and then make a series of predictions assuming this true.\n\nThe number observed slugs will be lower when plants are protected by an allicin border.\nThe number of affected plants will be lower when plants are protected by an allicin border.\nThe level to which plants are impacted will be lower when plants are protected by an allicin border.\n\n\n\n\n\n\n\nNote\n\n\n\nThe fact that we were able to state both a null and alternative hypothesis ensures that our hypothesis is falsifiable.\n\n\nOur next step is to design an experiment that tests our statistical hypothesis.\nOur experiment should include at least two groups or treatments8, the control and the experimental groups. The groups should be completely the same, with the exception that an experimental group receives a treatment while the control does not. Comparing these two groups allows us to determine if the treatment has had an effect. You should always include a control group to have a baseline of what the outcome should look like if there is no effect (negative control) or if there is an effect (positive control)9. Depending on the complexity of your hypothesis and the number of dependent and independent variables you are testing, your design may comprise several experimental groups, each with a different treatment.8 We use the term treatment to describe a set of conditions.9 It’s important to keep in mind that positive and negative have nothing to do with “good” or “bad” in this context but rather presence/absence of an effect.\nLet’s think about our example. It’s always important to consider what you are trying to test to figure out how to design the experiment. For example, we would not want a set up where we have a number of slugs and we expose them to Allicin to determine if it kills them or makes them sick - We already know that Allicin is toxic to slugs. We want to know whether the presence of Allicin repels slugs, i.e. if they get close to something that is emiting Allicin (e.g. onions), can they sense that from a “safe distance” and stay clear.\nLet’s assume that we have an outdoor space in which we can plant multiple small plots. Our independent variable is “Allicin presence” so we would need to create treatments that differ in whether or not Allicin is present. We can’t do this by planting onions or leeks - because then we still wouldn’t know if it is the Allicin or something else in the Allium plants that is repeling the slugs. So we need to have isolated Allicin, perhaps diluted in water that would allow us to spray a border either around the entire experimental plot. We would want to do some research to figure out what concentrations would be comparable to onions or similar being present. Then as our control, we would have plots without a Allicin border.\nWith our treatments in place, we need to identify variables that we need to control for to ensure that if we do see a difference between our experimental and control treatments we can ensure that they are due to the independent variable.\nFor our example, if we are outdoors we have a wide range of variables like sunlight/shade, soil conditions, temperature, and other weather conditions to consider. Those are variables that we cannot control, however, ideally we can chose our plot locations so we can reasonably assume that they are consistent across treatments. In addition, we would want to make sure that we are running our experiment during a time of year where it e.g. isn’t too cold for slugs to be out and about. Better yet, we might see if we cannot get access to a greenhouse where we can control these conditions.\nIdeally, we would also control the number of slugs. This might require hiring a bunch of 5-year olds to go slug collecting and then we could “seed” the number of slugs around each plot. Again, this might be easier to do in a greenhouse.\nThe type of plant being grown and potentially snacked on by slugs is also a variable that we probably want to control. The most straightforward set up would be to identify a plant that we know slugs like to eat10 and then have each plot be set up in an identical way.10 This is important because otherwise we don’t know whether there was no slug damage/presence because of that plant itself or the Allicin treatment.\nAs a final step, we need to account for sample size, replicates, randomization to ensure that our results are reliable and representative.\nA key component of experimental design is ensuring the sample size is sufficiently large that we can be confident that our results can be generalized to the population as a whole. There is always potential for naturally occurring variation despite our best efforts to control all the variables that we are not testing for. For example some of our slugs might just be lazy. So we would want to have a sufficiently large samplesize of slugs to ensure that we can account for this. Similarly, we would want to make sure that we include a a consistent number of plants in each plot and ensure that that number is large enough to account for variability.\nWe would also want to decide if we want to have just one control and one experimental treatment or whether it makes more sense to have replicates, i.e. we set up multiple plots for each type of treatment. That way if there was e.g. variability in the soil that meant that the Allicin was absorbed/dispersed differently that variability would have less of an effect when we combine results across plots in the same treatment category.\nLet’s say that we decide to have three control and three experimental treatments, then we would need to consider how we want them laid out in our greenhouse or outdoor space. You could have all of the control plots on one side and all of the experimental plots on the other. But what if the greenhouse is warmer on one side compared to the other? This could affect the activity of the slugs. In this case we would want to randomize the placement of our plots to minimize those impacts. Another aspect of randomization would be that if we had for example collected our slugs from several different locations we would want to randomly assign those to different plots.\nAt this point, we have thought through all the important aspects of our experimental design that we have though through to optimize our experimental design and ensure all of these aspects are accounted for.\n\nDoes my design allow me to systematically and precisely manipulate the independent variable(s)?\nAm I able to precisely measure the dependent variably?\nHave I made sure to control any confounding variables?"
  },
  {
    "objectID": "A01_intro-exp-design.html#sec-conducting-field-and-observational-experiments",
    "href": "A01_intro-exp-design.html#sec-conducting-field-and-observational-experiments",
    "title": "1  Scientific Method & Experimental Design",
    "section": "1.4 Conducting Field and Observational Experiments",
    "text": "1.4 Conducting Field and Observational Experiments\nIn many biological fields, including ecology and evolution, we more commonly use observational experiments or studies compare to the controlled design. Here, we observe and record natural phenomena without manipulating any variables. The studies are still carefully designed and follIow the scientific method. However, in a controlled experiment we are able to effectively isolate the effect of the independent variable from other factors because we are able to include both an experimental group that is exposed to the treatment and a control group that is not, while controlling all other variables. By contrast, in an observational experiment we generally cannot intervene to control or change the conditions to create a control group because it is impractical, impossible or frankly, unethical to manipulate variables directly11. Our first example exploring the relationship of bird nests and predator presence would likely fall into this category. Observational experiments give us important insights into existing patterns and relationships in the natural environment and how phenomena play out in the real world. However, results are frequently a bit messier or more ambiguous and it is more difficult to establish causality because a lack of experimental control means that there are more challenges due to the presence of confounding variables and difficulties isolating just one independent variable to test for.11 Sometimes you will hear somebody describe certain field experiments as a “natural laboratory” because there is a naturally occurring situation that mimics how we would design a controlled experiments. For example, islands or lakes can create scenarios where e.g. predators are only present in one location and so it functions as a “natural control group”. However, even then it is not possible to match all variables e.g. temperature or other abiotic factors the way we can in a controlled setting."
  },
  {
    "objectID": "C_ecological-succession.html",
    "href": "C_ecological-succession.html",
    "title": "Marvelous Microbial World of Milk",
    "section": "",
    "text": "After completing this unit, you should be able to\n\ndescribe the general process of ecological succession and formulate a testable hypothesis about changes in the microbial community of spoiled milk.\ndistinguish between antiseptics and disinfectants and formulate a testable hypothesis about their efficacy on milk microbes.\ndistinguish between quantitative and qualitative data.\napply microscopy techniques to distinguish between bacterial shapes, yeasts and molds.\ninoculate culture pretri dishes and make smear slides by heat fixing and staining bacteria.\ncollect, analyze and interpret qualitative data on microbe community composition and quantitative data on zone inhibition.\n\nFor each of our units we will have a project folder1 with an Rproject, *.qmd-files to complete lab assignments and write your lab report along with sub-directories to hold the data and results that you will generate. At the beginning of the semester, you should have created a directory on your Desktop or Documents folder on your laptop and named it something like bi104 as a home directory for all of our project folders this semester.1 We will use “directory” and “folder” synonymously throughout this lab handbook\nDownload the 03_Succession project folder using the link provided to you on Canvas. Once you have downloaded it, unzip the project directory into your bi104 directory2.2 On a MacOS, your system it will automatically unzip the folder for you and you should be able to directly move the entire unzipped directory to your bi104 folder. On a Windows OS you will need to right click and select extract all, then you should be able to specify which directory you want to unzip your file into.\nOnce you have done this, you can open the Rproj for this module either by double clicking on it which will launch Rstudio or by opening Rstudio and then using File &gt; Open Project or by clicking on the Rproject icon in the top right of your program window and selecting Open Project.\nOnce you have opened a project you should see the project name in the top right corner3.3 Pro tip: If you run into issues where a quarto document won’t render or file paths aren’t working (especially if things were working previously) one of your first steps should be to double check that the correct Rproj is loaded.\nIn this unit we will explore ecological succession in milk as well as designing an experiment to test the effects of disinfectants compared to antiseptics on microbial organisms."
  },
  {
    "objectID": "A04_descript-stats.html#sec-learning-objectives",
    "href": "A04_descript-stats.html#sec-learning-objectives",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.1 Learning Objectives",
    "text": "4.1 Learning Objectives\nAfter completing this activity, you should be able to\n\nDefine descriptive statistics and describe its role in summarizing and describing data sets.\nVisualize counts of categorical data using bar plots and explain how to interpret them.\nVisualize distributions of continuous data using histograms and explain how to interpret them.\nRecognize and explain how to calculate measures of central tendency, including mean, median, and mode, and understand when each is appropriate to use.\nRecognize and explain how to calculate measures of variability, including range, variance, and explain how they describe the spread of data.\nUse R to calculate standard descriptive statistics.\n\nQuantitative research is a core component of the natural sciences, including biology. Here, we collect and analyze numerical data1 to find patterns and averages, test causal relationships, generalize results based on a specific sample to the whole population and be able to make predictions beyond that specific case. Over the last few decades the availability of computational power and the ability to generate increasingly large data sets2 means that arguably all biology has become computational biology and your ability to apply statistics is increasingly essential to pursue a career in most biological fields including genetics, ecology, animal behavior, environmental science etc. This semester as you design different research projects you will learn how analyze data using R.1 data is plural, a single numerical observation would be a datum2 This is why we frequently also call this the “Era of Big Data”.\nAfter you have performed an observational or controlled experiment and compiled your individual observations into a data set, your next step will be to analyze that data and draw conclusions from the data using statistics. The goal of descriptive statistics is to summarize and describe the characteristics of a data set. By contrast, inferential statistics are applied to allow you to make generalizations about the population based on your sample or to test a specific hypothesis."
  },
  {
    "objectID": "A04_descript-stats.html#sec-organizing-and-summarizing-raw-data-using-descriptive-statistics",
    "href": "A04_descript-stats.html#sec-organizing-and-summarizing-raw-data-using-descriptive-statistics",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.2 Organizing and summarizing raw data using descriptive statistics",
    "text": "4.2 Organizing and summarizing raw data using descriptive statistics\nTo learn how to use R to generate descriptive statistics we are going to explore a data set collected from 2007 to 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program which is part of the US Long Term Ecological Research Network (Gorman, Williams, and Fraser 2014). We are going to use a curated version3 of the data set.3 This means that the data wrangling has already been done for us and we can directly access a tidy data set.\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3). https://doi.org/10.1371/journal.pone.0090081.\nExecuting the following code chunk4 to install an R package holding the Palmer Penguins data set.4 You will only have to do this once. Note that the eval option of your code chunk is set to false so that when you render your document it will not execute this code chunk\n\ninstall.packages(\"palmerpenguins\")\n\nNow we can get started by loading several R libraries, including the palmerpenguins library, that contain the functions we will need to explore this data set\n\n\n\nLet’s take a quick look at what variables the data set contains using the function glimpse().\n\n# overview of rows and columns\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nYou can also use the function View() to open the data.frame in the Viewer Pane.\n\n# open dataframe in Viewer\nView(penguins)\n\nWe generally differentiate between two different types of variables:\n\nContinuous variables are numeric variables that can take any value within a given range; they are typically measured on interval or ratio scales5, allowing for meaningful mathematical operations like addition and subtraction.\nCategorical variables represent categories or groups and can take on a limited, distinct set of values; they are either measured on a nominal scale6 or an ordinal scale7.\n\n5 These are both types quantitative scales: An interval scale is a measurement scale where the intervals between consecutive values are equal, but the scale lacks a true zero point. A ratio scale has the same properties but also a true zero point indicating the absence of the measured quantity making the ratios of measurements meaningful.6 Categories are assigned labels with no inherent order or ranking7 There is a meaningful order to the categories.\n\n\n\n\n\nConsider this\n\n\n\nTake a look at the data set in the Viewer Pane and get an overview of what information it contains.\n\nHow many different species were included?\nOn how many islands where observations made?\nWhen were the observations made?\nWhat biological measurements were taken?\nHow many individual observations are there?\n\nIndicate which of these variables are continuous and which are categorical variables.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can see that we observations of three different species of penguins (Adelie, Chinstrap, Gentoo) on three different islands in the Antarctica (Togersen, Dream, Biscoe). These are all categorical variables.\nThe biological measurements include sex (categorical variable) as well as several continuous measurements:\n\nbody mass\nflipper length\nbeak measurements (depth, length)\n\n\n\n\nAs you probably noticed having to scroll through raw data8 on 344 individuals that were observed, it is difficult to get much information on general patterns. You were probably able to scroll through and figure out how many different islands were observed and what species were counted and that both males and females were observed - but would you be able to make a statement about whether some species are more common on some islands compared to others? Whether there are more males or females in the data set? If there are differences in size among species? Among sexes within species? What a typical beak size looks like?8 Unprocessed data comprising all the individual observations.\nGenerally our first step in analyzing a data set would be to apply descriptive statistics to summarize the data and identify the following three characteristics\n\nFrequency Distribution\nMeasures of central tendency\n\nmean\nmedian\nmode\n\nMeasures of variability\n\nrange\nstandard deviation\nvariance\ninterquartile range\n\n\nYou can apply these statistics to assess only one variable at a time (univariate analysis) or to compare two (bivariate analysis) or more (multivariate analysis) variables.\nAfter we have calculated our descriptive statistics we will generally visualize them as either a table or graph. A table allows us to organize values into columns and rows. While we frequently gather and organize our raw data into tables, any tables you are presenting in a report should be summarized data. Generally, we reserve tables for complex data that is not easily visualized or include them in an appendix for anyone who needs very specific details. Instead whenever possible we translate tables into a graph or plot which makes it easier to show the relationship between variables and identify trends and patterns, the most commonly used graphs are scatter plots, line plots, box plots, bar plots, and histograms9.9 Today you will use bar plots and histograms to visualize summary statistics."
  },
  {
    "objectID": "A04_descript-stats.html#sec-frequency-distribution",
    "href": "A04_descript-stats.html#sec-frequency-distribution",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.3 Frequency Distribution",
    "text": "4.3 Frequency Distribution\n\n4.3.1 Simple frequency distribution\nEvery data set is made up of a distribution of values. We can summarize the frequency of every possible value of a variable using either counts or by indicating the proportion of all observations that take on a specific value.\nFor example we might want to know how many individuals of each species where observed in the data set. To do this we would create a frequency distribution table that lists every possible value or measurement (in this case the species) and the how many times each value was observed.\nWe can have R do the work for us using the function count()and specifying which column the number of observations per value should be counted for10.10 You will frequently see that we link lines of codes using %&gt;% which is called a “pipe”. You can interpret this as you telling R “and now do this”. The reason we call this piping is tha the first line of code creates and output and then we are saying “take this output and now make that the input and do this” as if we were creating a giant marble run.\n\n# count number of individual per species\npenguins %&gt;%\n  count(species) %&gt;%\n  kable()\n\n\n\n\nspecies\nn\n\n\n\n\nAdelie\n152\n\n\nChinstrap\n68\n\n\nGentoo\n124\n\n\n\n\n\nFrequently, it is more helpful to compare relative proportions. We can do this by creating a new column using the function mutate(). Then, we give it the column name for the column we want to create and define how the content for each row should be calculated based on existing values. In this case, we divide the content of column n (which contains the counts) by the number of individuals observed by the total number of all penguins in the data set which we can calculate as sum(n).\n\n# create frequency table\npenguins %&gt;%\n  count(species) %&gt;%                 # count number of observations\n  mutate(percent = n/sum(n)*100) %&gt;% # calculate percent\n  kable(digits = 1)                  # print table with 1 digit\n\n\n\n\nspecies\nn\npercent\n\n\n\n\nAdelie\n152\n44.2\n\n\nChinstrap\n68\n19.8\n\n\nGentoo\n124\n36.0\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nDescribe your results and consider what this tells you about penguins in the Antarctica.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis gives us some insight into fundamental patterns of species richness and diversity. We know there are three species observed (richness) and the relative abundance (Adelie penguins are the most commonly at 44% while Chinstrap penguins are the most rare at 19%).\n\n\n\nDetermining the number of observations and relative proportions is an important descriptive statistic for any categorical variable.\n\n\n\n\n\n\nGive it a try\n\n\n\nModify/complete the code below to create a frequency table that shows the counts and proportion of males and females in the data set. Describe your results and consider what this tells you about penguins in the Antarctica.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\npenguins %&gt;%\n  count(sex) %&gt;%\n  mutate(percent = n/sum(n)*100) %&gt;%\n  kable(digits = 1)\n\n\n\n\nsex\nn\npercent\n\n\n\n\nfemale\n165\n48.0\n\n\nmale\n168\n48.8\n\n\nNA\n11\n3.2\n\n\n\n\n\nWe see that we have a small number of individuals in the data set for which sex was not determined (NA). More importantly, we see that overall, the penguin populations comprise approximately the same amount of males and females.\nAnother option would be to count the number of individuals per male and female within each species.\n\npenguins %&gt;%\n  count(species, sex) %&gt;%\n  mutate(percent = n/sum(n)*100) %&gt;%\n  kable(digits = 1)\n\n\n\n\nspecies\nsex\nn\npercent\n\n\n\n\nAdelie\nfemale\n73\n21.2\n\n\nAdelie\nmale\n73\n21.2\n\n\nAdelie\nNA\n6\n1.7\n\n\nChinstrap\nfemale\n34\n9.9\n\n\nChinstrap\nmale\n34\n9.9\n\n\nGentoo\nfemale\n58\n16.9\n\n\nGentoo\nmale\n61\n17.7\n\n\nGentoo\nNA\n5\n1.5\n\n\n\n\n\nThis gives us the additional insight that this pattern of equal number of male and females is consistent across all three species.\nCaveat: We don’t know if the the sampling scheme was designed to be random in respect to sex and thus represent the proportions in the species, or if it was more important to include similar numbers of males/females to be able to compare differences in the other metrics in the data set among sexes.\n\n\n\nSo far, we have been looking at frequencies of occurrences across the entire data set, however, in our case we have observations from three different islands. One question we probably have is whether the distribution of different penguin species is consistent across the different locations.\nOne of the advantages of using R is that we can add just one additional line of code to group our data by island and then count the number of observations per island.\n\npenguins %&gt;%\n  group_by(island) %&gt;%               # group data by island\n  count(species) %&gt;%                 # count number of observations in each subset\n  pivot_wider(names_from = species,  # create one column per species\n              values_from = n) %&gt;%   # add counts per island per island/species\n  kable()\n\n\n\n\nisland\nAdelie\nGentoo\nChinstrap\n\n\n\n\nBiscoe\n44\n124\nNA\n\n\nDream\n56\nNA\n68\n\n\nTorgersen\n52\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nDescribe your results and consider what this tells you about penguins in the Antarctica.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAgain, just by looking at simple descriptive stats comparing distribution of observations across these categories we have gained a lot of insight and we now see that the islands are quite different in terms of the which species are present. Adelie penguins are observed on every island, while Gentoo and Chinstrap penguins never co-occur, they are each observed on one island each, in which case they are the more common species compared to Adelie penguins.\n\n\n\nBoth species and sex are categorical variables, so if we wanted to visualize the distribution we would typically use a bar chart where we plot the different categories of a value on the x axis and then scale the height of each bar to correspond to the number of individuals observed.\nWe can do this using the function ggplot()11.11 We aren’t going to go into detail how to customize figures using ggplot, but you can use the comments to see what modifications you would be able to make to e.g. change color coding etc.\n\n\n\n\n\n\nGive it a try\n\n\n\nAdd a descriptive figure caption and x/y-axis titles12.\n\nggplot(penguins, aes(x = species)) +  # specify data frame and column name to plot on x-axis   \n  geom_bar(stat = \"count\",            # define chart type as bar plot            \n           color = \"black\",           # define color of bar lines            \n           fill = \"darkorange\") +     # define color of bars   \n  labs(x = \"species\",                 # x axis label        \n       y = \"number of individuals\") + # y axis label   \n  theme_classic() \n\n\n\n\nFigure 4.1: Number of individuals observed per species.\n\n\n\n\n\n\n12 By giving this code chunk a label that starts with fig it will automatically number your figures.We already know from looking at the frequency table that the patterns are quite different across the islands. We can similarly add a line of code to plot the counts for each island in a separate bar plot.\n\n\n\n\n\n\nGive it a try\n\n\n\nAdd a descriptive figure caption and x/y-axis titles.\n\n# define colors to use\ncol &lt;- c(\"darkorange\", \"purple\", \"cyan4\")\n\n\nggplot(penguins,                             # define data set to plot\n       aes(x = species,                      # define variables to plot on x axis\n           fill = species)) +                # color code bars by species\n  geom_bar(stat = \"count\",                   # define chart type as bar plot\n           color = \"black\") +         \n  facet_grid(. ~ island) +                   # create separate plots per island\n  scale_fill_manual(values = col) +          # define colors for fill\n  labs(x = \"species\",                        # x axis label\n       y = \"number of individuals\") +        # y axis label\n  theme_classic() +\n  theme(legend.position = \"bottom\")          # place legend underneath figure\n\n\n\n\nFigure 4.2: Number of individuals observed per species on each island.\n\n\n\n\n\n\n\n\n4.3.2 Grouped (binned) frequency distributions {#sec-grouped-(binned-frequency-distributions}\nWhat about the distribution of some of the other values in our data set? For example, we have measured the body mass of each individual13. Therefore, we probably want to know what the distribution of body size is for each of the penguin species.13 For second visualize a scientist with a scale and a bunch of penguins standing in line waiting to be weighed.\nHowever, this data set is different from counting the number of individuals in a specific category like we did above, body weight is a continuous variable. In this case, we would use a histogram to visualize the distribution. We do this by grouping our observations into bins that represent equally sized groups of possible values.\n\n\n\n\n\n\nGive it a try\n\n\n\nFor example, we could create bins of 100g and then count how many individuals weigh between 0-50g, 50-100g etc. Then we can plot those bins on the x-xis and the scale each bar to correspond to the number of individuals whose weight falls into that bin.\nPlot a histogram for each penguin species using the table below, and consider how this helps summarize your data.\n\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 100,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4.3: Distribution of body weight for each species.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nHistograms are really helpful for exploring general patterns in the distribution of your data because you easily determine\n\nAll the values occurring in your data set\nWhich values occur more/less frequently (the “shape”)\nThe center of your values\nThe amount of variability in your data\n\nIn short, assessing the distribution with a histogram gives you an idea of central tendency and variability of the data set as well - descriptive statistics that we’ll dig into in just a bit.\n\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nWe should think a little bit about how important choosing the “right” bin size is - or if there even is a “right” bin size. Manipulate the code below to try 3 different bin sizes. Compare the figures you produce with your lab mates sitting near you and briefly describe what you learn about picking the right bin size.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n# try a small bin size\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 10,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n# try a large bin size\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 100,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n# try a REALLY LARGE bin size\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 1000,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nLet’s spend a little bit of time thinking about how to correctly interpret a histogram.\nImagine you and two of your lab mates are riding your snowmobiles on one of the Antarctic islands in the study. You come across a Chinstrap penguin colony. They are very cute, so as you (carefully) drive through the penguins you randomly grab five of them (let’s assume you have a little snowmobile side car). One of your lab mate comes across an Adelie penguin colony and also grabs 5 random penguins, while your last partner in crime nags 5 Gentoo penguins.\nYou meet back up at your research station - discuss with your lab mates how heavy you think each of your sets of penguins are. Be as specific as possible.\nBonus question - given your data set who do you think found their colony first?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nGiven that your histogram shows values from roughly 2,500g to 5,000g for Adelie penguins, you can probably assumed that your penguins are somewhere in there. But you can be even more exact, if you carefully look at the height of the bars - they higher bar the more common than weight has been observed and this means that there is a higher probability that your penguins are in that weight bin.\nEven though the absolute height of the bars is lower for Chinstrap penguins is smaller (because there are fewer in the data set), the distribution is actually quite similar and those two sets of penguins are probably quite similar in weight.\nThe smallest observed Gentoo penguins are about as large as the smallest Adelie and Chinstrap penguins. They most commonly are observed to be between 4,500 and 5,500g - your penguins are likely around that size.\n\n\n\n\n\n\n\n\n4.3.3 Normal distribution\nThe data set can have a wide range of shapes and the shape itself will give you some insight into the statistical properties which can be useful down the line, as you make decisions about for example what statistical test to apply14.14 Statistical tests make specific assumptions about the data, if those assumptions are violated your conclusions are not well supported even if your test results say they should be\nOne of the most well-understood distribution is called a normal distribution. Because it is bell-shaped it is also frequently referred to as the bell curve.\n\n\n\n\n\n\nGive it a try\n\n\n\nExecute the code chunk below to generate a normally distributed data set and plot it.\nThen take a look at the figure you just plotted and identify these key characteristics of a normal distribution:\n\nThere is a singe peak right around the center line.\nMost of the observations are at the center of the distribution.\nThe distribution is symmetric around the center line.\n\n\n# randomly draw values from a specified normal distribution\ndf &lt;- rnorm(n = 1000,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.5,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n\nFigure 4.4: Distribution of 1,000 observations randomly drawn from a normal distribution with mean = 0 and standard deviation = 1.\n\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nTake another look at your penguin body mass histograms and argue whether or not you think we can assume that body mass is normally distributed.\nA good way to argue this is to take the list of key characteristics from the list above and explain whether or not you see this in your data set.\n\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 100,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4.5: Distribution of body weight for Adelie, Chinstrap, and Gentoo penguins. Weights are binned in 100g increments.\n\n\n\n\n\n\nOne thing you will notice is that your choice of bin width will play a bit of a role. For example, let’s re-plot the same figure with a slightly larger binwidth and see if you change your mind.\n\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 250,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                  # create separate plot per species\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4.6: Distribution of body weight for Adelie, Chinstrap, and Gentoo penguins. Weights are binned in 250g increments.\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nAnother factor that we might need to consider is that our data set contains both males and females. Sexual dimorphism is common in animal species which can include that one sex is larger compared to the other.\nLet’s create separate plots of each species and sex. Then, re-consider whether or not you think the weight data is normally distributed.\n\nggplot(penguins, aes(x = body_mass_g,        # define data set & variable on x-axis\n                     fill = species)) +      # color code by species\n  geom_histogram(binwidth = 150,             # define bin width\n                 color = \"black\") +\n  facet_grid(sex ~ species) +                  # create separate plot per species and sex\n  scale_fill_manual(values = col) +          # define color palette\n  labs(x = \"body mass [g]\",                  # x-axis title\n       y = \"number of individuals\") +        # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 4.7: Distribution of body weight for male and female Adelie, Chinstrap, and Gentoo penguins. Weights are binned in 150g increments.\n\n\n\n\n\n\n\n\n4.3.4 Sample vs Population\nBefore we move on to the next statistic, let’s take a moment to think about the difference between a sample and a population. Our data set is a sample of 344 penguins. That is the number of individuals that were observed. However, unless this was a comprehensive census in which the researchers tracked down every single penguin on those three islands this is only a subset of the actual population, which would be all available observations (in this case pengiuns) at a specific point in time for a specific define geographic region (in this case the three islands).\nThe goal is to take a sample that is representative of the population. The larger our sample, the larger the proportion of the population is included which means that our sample will more closely resemble the statistical attributes including the distribution of the population.\n\n\n\n\n\n\nGive it a try\n\n\n\nManipulate the code below to simulate 4 different normal distributions but for three different sample sizes and compare how this affects the distributions. Take a peak at your lab mate’s plots as well, and see if your plots look the same for the same sample size. Assume that body mass of penguins is normally distributed, how large do you think sample sizes should be to make sure your sample is representative.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n# use a small sample size\ndf &lt;- rnorm(n = 10,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.25,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n# use a medium size sample size\ndf &lt;- rnorm(n = 100,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.25,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n# use a large size sample size\ndf &lt;- rnorm(n = 1000,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.25,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n# use a REALLY large size sample size\ndf &lt;- rnorm(n = 10000,   # set number of observations\n            mean = 0,   # determine mean\n            sd = 1) %&gt;% # set standard deviation\n  as.data.frame() %&gt;%\n  setNames(\"obs\")\n\n# plot histogram\nggplot(df, aes(x = obs)) +\n  geom_histogram(binwidth = 0.25,\n                 color = \"black\", fill = \"darkorange\") +\n  theme_classic()\n\n\n\n\nYou should see that the smaller the sample size the more oddly shaped the distributions. If you regenerate a sample of a normal distribution for the same sample size you will also notice that for small sample sizes those plots jump around quite a bit."
  },
  {
    "objectID": "A04_descript-stats.html#sec-measures-of-central-tendency",
    "href": "A04_descript-stats.html#sec-measures-of-central-tendency",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.4 Measures of central tendency",
    "text": "4.4 Measures of central tendency\nMeasures of central tendency give us an estimate of the center (average) of a data set. This is a complicated (but very specific way) of way it tells us what the “average” is. Typically, when you hear the term average you probably are thinking of the mean, but other important metrics are the median and the mode which give us additional important information about a data set.\n\nThe mean is often used synonymous to average and the metric you are likely most familiar with. You calculate it by adding up all the values and divide by the total number of observations. This is metric is most informative when the data has a normal distribution.\nThe median is the value in the middle of the data set. This means that 50% of values fall below and 50% fall above. For a normal distribution the mean and the mode should be very similar. For skewed data sets this is a more approporiate metric to determine centrality.\nThe mode is most commonly observed value, it’s not typically used in statistics, so we are going to skip it here.\n\nLet’s look at each one in a bit more detail.\n\n4.4.1 The mean\nThe mean is the sum of all the values divided by the sample size (number of observations or values). We can write this as a mathematical formula as follows\n\\[\n\\bar{x} = \\sum x\\_{n} = \\frac{x_{1} + x_{2} + x_{3} + ... x_{n}}{n}\n\\tag{4.1}\\]\nwith \\(\\bar{x}\\) as the sample mean15 , \\(n\\) is the sample size16, and \\(x_{n}\\) being the individual observations.15 It’s important to note that we are almost always determining the sample mean {x} and then use it as an estimate of the true population mean \\(\\mu\\).16 If you are referring to the true size of the population we would generally denote that as \\(N\\).\nThe Equation 4.1 is implemented in the base R function mean() which we can easily group our data by species using group_by() and then calculate the mean body mass using the function summarize(). We need to give it a column name for a summary statistic and then tell it how to calculate the contents for that column. In this case we will specify that we want to apply the function mean() to the column body_mass_g. We will use the argument na.rm to tell R to ignore any missing values.\n\npenguins %&gt;%                                                       # define data frame\n  group_by(species) %&gt;%                                            # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE)) %&gt;%  # calculate mean\n  kable(digits = 1)                                                # print table\n\n\n\n\nspecies\nmean_body_mass\n\n\n\n\nAdelie\n3700.7\n\n\nChinstrap\n3733.1\n\n\nGentoo\n5076.0\n\n\n\n\n\n\n\n4.4.2 The median\nTo determine the median by hand, you would first determine if there is an odd or an even number of observations. Then you need to order all your observations from smallest to largest. If there is an odd number of observations, the median is the middle value17. If you have an even number of observations you would determine the median by finding the values at the positions \\(\\frac{n}{2}\\) and \\(\\frac{n+1}{2}\\) and then the median is the mean of those two values.17 You can determine that position as \\(\\frac{n+1}{2}\\) with \\(n\\) being the sample size.\nFortunately, we again have a base R function (median()) that will allow us to quickly calculate the median for a set of values contained in a single column of a data frame. We can do this using the same functions as above, however, we will add an additional argument to the summarize() function that we would like to add a column called median_body_mass that should contain the values calculated by apply the function median() to the column body_mass_g, again we will specify to ignore any missing values.\n\npenguins %&gt;%                                                           # define data frame\n  group_by(species) %&gt;%                                                # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE),          # calculate mean\n            median_body_mass = median(body_mass_g, na.rm = TRUE)) %&gt;%  # calculate median\n  kable(digits = 1)                                                    # print table\n\n\n\n\nspecies\nmean_body_mass\nmedian_body_mass\n\n\n\n\nAdelie\n3700.7\n3700\n\n\nChinstrap\n3733.1\n3700\n\n\nGentoo\n5076.0\n5000\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nCompare and contrast the mean and median body weight for our three penguin data sets in the table you have just produced.\n\nDiscuss whether these values are surprising after having spent (maybe too much?) time looking at the histograms.\nConsider what this means in the context of what you have learned about the distribution of data sets.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nEven though our Adelie and Chinstrap penguins differ slightly (33g) in their mean body mass, they have the same median weight. Gentoo penguins are on average 2,000g heaver.\nIn all three cases the mean and mode are almost identical, this is indicative of a normal distribution, if a distribution is skewed left (a long tail to the right) the median will be smaller than the mean and if it is skewed right (a long tail to the left) the median will be larger than the mean."
  },
  {
    "objectID": "A04_descript-stats.html#sec-measures-of-variability",
    "href": "A04_descript-stats.html#sec-measures-of-variability",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.5 Measures of variability",
    "text": "4.5 Measures of variability\nThe last category we have to think about is what metrics we can use to describe the variance of the a data set. We use measures of centrality to describe the most typical values we should expect, however most things in nature are not the same. Therefore, we need metrics that tell us how different we should expect values to be from the mean or median to have a sense of how spread out the values are going to be.18.18 Think of this as you now knowing how heave a penguin is on average but you want to know how large you should expect the smallest and the largest penguins to be, how different on average the weight of a penguin is going to be from the mean.\nTypical metrics that are used to describe variability are\n\nthe range tells us how far apart the observed values are; described either as the minimum and maximum observed value or as the difference between the two.\nthe variance is the average of the squared deviations of each value from the mean.\nthe standard deviation is the average amount of variability in the data set and describe how far away and observed value is from the mean on average.\n\n\n4.5.1 The range\nOne of the most straight forward metrics we can determine to describe the variability of the data set is the range. To do this, we simply determine the highest and lowest value for a variable. We can easily add these values to the table we have been expanding by adding additional lines to apply the function min() and max() to the body_weight_g column.\n\npenguins %&gt;%                                                           # define data frame\n  group_by(species) %&gt;%                                                # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE),          # calculate mean\n            median_body_mass = median(body_mass_g, na.rm = TRUE),      # calculate median\n            min_body_mass = min(body_mass_g, na.rm = TRUE),            # determine largest value\n            max_body_mass = max(body_mass_g, na.rm = TRUE)) %&gt;%        # determine largest value\n  kable(digits = 1) \n\n\n\n\n\n\n\n\n\n\n\nspecies\nmean_body_mass\nmedian_body_mass\nmin_body_mass\nmax_body_mass\n\n\n\n\nAdelie\n3700.7\n3700\n2850\n4775\n\n\nChinstrap\n3733.1\n3700\n2700\n4800\n\n\nGentoo\n5076.0\n5000\n3950\n6300\n\n\n\n\n\nEven though the range is easy to determine and can be informative in some cases, it is actually rarely used as a measure of variability because the sample range does not always reflect the range of the actual population. There generally is a pattern where up to a certain point your range will increase with sample size.\n\n\n\n\n\n\nConsider this\n\n\n\nWe are going to plot all of our observations for body mass on the x-axis grouped by species. Use this figure to argue why using the range as a measure of variability can be misleading in some cases because it ignores important contextual information.\n\nggplot(penguins, aes(x = body_mass_g, y = species, fill = species)) +\n  geom_jitter(size = 3, shape = 21, alpha = .4, height = .1) +\n  scale_fill_manual(values = col) +\n  labs(x = \"body mass [g]\", y = \"species\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 4.8: Observed body weight for Adelie, Chinstrap, and Gentoo pengiuns.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can see that Chinstrap penguins have a wider range compared to the Adelie penguins but most of their points actually cluster more tightly compared to the other species. We don’t know if this is just because their sample size is smaller and so we are more likely not to observe the full range of body weight exhibited by Gentoo populations or if these are true outliers.\n\n\n\n\n\n4.5.2 Variance\nOne way we can avoid the issues that can be introduced by using the range to describe variability is to determine how far away each point is from the mean value (deviation from the mean).\nLet’s visualize this by plotting all the weights for the Chinstrap penguins on the x-axis and adding a line between each point and the mean.\n\n# create data frame with only chinstrap penguins\nchinstrap &lt;- penguins %&gt;%\n  filter(species == \"Chinstrap\") %&gt;%\n  rownames_to_column(\"indv\")\n\n# plot all observed weights\nggplot(chinstrap, aes(x = body_mass_g, y = indv)) +\n  geom_vline(aes(xintercept = mean(body_mass_g, na.rm = TRUE)), \n             color = \"darkred\", size = 1) +\n  geom_segment(aes(x = body_mass_g, xend = mean(body_mass_g),\n                   yend = indv)) +\n  geom_point(size = 3, shape = 21, fill = \"cyan4\") +\n  labs(x = \"body mass [g]\") +\n  theme_classic() +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\nFigure 4.9: Body weights for Chinstrap penguins. Red line indicates mean body mass. Black lines denote the deviation from the mean for each observation.\n\n\n\n\nAt first glance, just summing up all the values seems like a simple solution to get the “total amount of deviation” in the data set. However, some of our values are positive while others are negative so that would be problematic because they will cancel each other out.\nInstead, we need to square each deviation and then we can sum them up19. The greater the variability in our data, the greater the sum of the squares. We generally refer to the sum of the squared deviations as the variance \\(s^{2}\\). To calculate the sample variance, you need to subtract each value from the sample mean, square each value, add up all the values, and then divide this sum by the number of samples minus 120.19 Squaring the mean means that our values are always positive. It also means that values that are at extreme distances from the mean will have a greated effect on the variability.20 We do this to correct for the fact that we are calculting the sample variance and not the variance of the true population.\n\\[\ns^{2}=\\frac{\\sum (x_{i}-\\bar{x})^2}{n-1}\n\\tag{4.2}\\]\nWith \\(x_{i}\\) being in individual observation, \\(\\bar{x}\\) the sample mean, and \\(n\\) the sample size.\nAgain, it is important that we differ between the sample variance that you can calculate using your data set and provides an estimate of the population variance.\nYou may have assumed that we can calculate this in R using a function called variance() - close, it is actually called var(), but you are correct that we can add the variance in body mass measurements for each penguin species to our summary stats table using the same syntax we have been applying.\n\npenguins %&gt;%                                                           # define data frame\n  group_by(species) %&gt;%                                                # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE),          # calculate mean\n            median_body_mass = median(body_mass_g, na.rm = TRUE),      # calculate median\n            min_body_mass = min(body_mass_g, na.rm = TRUE),            # determine largest value\n            max_body_mass = max(body_mass_g, na.rm = TRUE),            # determine largest value\n            var_body_mass = var(body_mass_g, na.rm = TRUE)) %&gt;%         # calculate variance\n  kable(digits = 1) \n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nmean_body_mass\nmedian_body_mass\nmin_body_mass\nmax_body_mass\nvar_body_mass\n\n\n\n\nAdelie\n3700.7\n3700\n2850\n4775\n210282.9\n\n\nChinstrap\n3733.1\n3700\n2700\n4800\n147713.5\n\n\nGentoo\n5076.0\n5000\n3950\n6300\n254133.2\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nCompare and contrast the range and the variance for body mass in across the different species and argue which metric you think is more informative.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAs indicate before, Adelie have a smaller range of body mass compared to Chinstrap penguins, however, we can see that the variance of Chinstrap is quite a bit smaller. Recall the visualization that showed that Chinstrap body weights points cluster closer together compared to Adelie pengiuns despite those extreme values that are creating the misleading greater range.\n\n\n\n\n\n4.5.3 Standard deviation\nOne of the drawbacks of using the variance to describe variability is that because we have squared the deviations our unites are no longer meaningful21, we can compare variance by magnitude but it is not helpful to say that the variance in Adelie penguin body weight is about 62,000 squared grams larger compared to Chinstrap penguins.21 The we of course remain open to good ideas on how to interpret “square grams”.\nTo get around this issue, we generally calculate the standard deviation \\(s\\) by drawing the square root of the variance.\n\\[\ns = \\sqrt{s^{2}}\n\\tag{4.3}\\]\nWe can add this to our summary stats table using the function sd().\n\npenguins %&gt;%                                                           # define data frame\n  group_by(species) %&gt;%                                                # create subsets by species\n  summarize(mean_body_mass = mean(body_mass_g, na.rm = TRUE),          # calculate mean\n            median_body_mass = median(body_mass_g, na.rm = TRUE),      # calculate median\n            min_body_mass = min(body_mass_g, na.rm = TRUE),            # determine largest value\n            max_body_mass = max(body_mass_g, na.rm = TRUE),            # determine largest value\n            var_body_mass = var(body_mass_g, na.rm = TRUE),            # calculate variance\n            std_body_mass = sd(body_mass_g, na.rm = TRUE)) %&gt;%         # calculate standard deviation\n  kable(digits = 1) \n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nmean_body_mass\nmedian_body_mass\nmin_body_mass\nmax_body_mass\nvar_body_mass\nstd_body_mass\n\n\n\n\nAdelie\n3700.7\n3700\n2850\n4775\n210282.9\n458.6\n\n\nChinstrap\n3733.1\n3700\n2700\n4800\n147713.5\n384.3\n\n\nGentoo\n5076.0\n5000\n3950\n6300\n254133.2\n504.1\n\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nWe frequently report means as mean +/- the standard deviation. Describe your results comparing the body mass of these three species using this format.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nGentoo penguins were observed to be the largest species with a mean body mass of 5,076g +/- 504g. Adelie and Chinstrap penguins are smaller with observed means of 3,700 +/- 458g and 3,733g +/- 384g, respectively, indicating that though on average they are similar in size, Adelie penguins exhibit a larger variability."
  },
  {
    "objectID": "A04_descript-stats.html#sec-homework",
    "href": "A04_descript-stats.html#sec-homework",
    "title": "4  Introduction to descriptive statistics",
    "section": "4.6 Homework",
    "text": "4.6 Homework\nComplete the questions below to review some of what you have learned about descriptive statistics. Render your document as an html-file and submit that document through Canvas. Double check that you are submitting the html file, not the quarto document.\nOur data set contains information on the flipper length for each individual in the data set.\n\n\n\n\n\n\nConsider this\n\n\n\nDescribe the individual steps you would complete to create a figure summarizing the distribution of the data.\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\n[Outline your steps here.]\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nModify and Execute the code chunk below to plot a histogram of flipper length by for each species.\n\nAdd a descriptive figure caption consisting of a concise title and a description that explains what is being displayed\nadd x and y-axis titles.\nAdjust the binwidth (currently at 25) to a width you find appropriate and explain why you chose that number\nArgue whether or not you can assume a normal distribution for flipper length.\n\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\nModify the figure by defining which data to plot on the x-axis and finding an appropriate binwidth:\n\nggplot(penguins, aes(x = columname, # define data set & variable on x-axis\n                     fill = species)) +     # color code by species\n  geom_histogram(binwidth = 25,             # define bin width\n                 color = \"black\") +\n  facet_grid(. ~ species) +                # create separate plot per species\n  scale_fill_manual(values = col) +        # define color palette\n  labs(x = \"title\",          # x-axis title\n       y = \"title\") +      # y-axis title\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n[Argue whether normal distribution].\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nExecute the code chunk below to calculate a series of descriptive statistics summarizing the flipper length measurements.\n\nExplain the difference between measures of central tendency and variability and for metric in the table determine which category it falls under (Protip: A good way to set up an answer for a question like this is to first define a category, then pick a variable and state “this variable tells me …, which falls into this category because …”.\nConsider which summary statistics you think are most informative and argue which two you would report if you were limited to reporting only two. (Protip: Since we have two categories you probably want one in each category. Then a good way to argue is to pick a metric, explain the information it conveys is important and then also explain why it does this better than other variables in that category).\nDescribe your results based on the metrics you chose.\n\n\npenguins %&gt;%                                                       # define data frame\n  group_by(species) %&gt;%                                            # create subsets by species\n  summarize(mean = mean(flipper_length_mm, na.rm = TRUE),          # calculate mean\n            median = median(flipper_length_mm, na.rm = TRUE),      # calculate median\n            min = min(flipper_length_mm, na.rm = TRUE),            # determine largest value\n            max = max(flipper_length_mm, na.rm = TRUE),            # determine largest value\n            var = var(flipper_length_mm, na.rm = TRUE),            # calculate variance\n            sd = sd(flipper_length_mm, na.rm = TRUE)) %&gt;%          # calculate sd\n  kable(digits = 1) \n\n\n?(caption)\n\n\n\n\n\n\nspecies\nmean\nmedian\nmin\nmax\nvar\nsd\n\n\n\n\nAdelie\n190.0\n190\n172\n210\n42.8\n6.5\n\n\nChinstrap\n195.8\n196\n178\n212\n50.9\n7.1\n\n\nGentoo\n217.2\n216\n203\n231\n42.1\n6.5"
  },
  {
    "objectID": "A06_writing-report.html#sec-learning-objectives",
    "href": "A06_writing-report.html#sec-learning-objectives",
    "title": "6  Writing a scientific report",
    "section": "6.1 Learning Objectives",
    "text": "6.1 Learning Objectives\nAfter completing this activity, you should be able to\n\nList the four key sections of a research paper and what their central purpose is.\nOutline the structure of a lab report.\nDescribe key components for each section of a lab report.\n\nA scientific research paper is structured into four sections:\n\nAn introduction of relevant background information providing context for your central question/hypothesis question and the objectives of your study.\nA clear description and justification of your experimental design, including the methods you used both to collect and analyze your data.\nA summary of your results.\nA clear interpretation and discussion of your results leading to an explicit (set of) conclusions.\n\nIn addition, you will have a “fifth” component, the Abstract which summarizes the Introduction, Methods, Results, and Discussion/Conclusion in a single paragraph. Generally, scientific research papers have an hourglass shape, where they start off very broad, become increasingly narrow as you describe your very specific study and then become more broad as you interpret and discuss the results in the broad context of the original research question you posed.\nYou will structure your lab reports in this format - don’t worry, we will give you guidance, though as you progress through the semester and (hopefully) feel more confident you will increasingly become more independent and require less scaffolding."
  },
  {
    "objectID": "A06_writing-report.html#sec-lab-report-components",
    "href": "A06_writing-report.html#sec-lab-report-components",
    "title": "6  Writing a scientific report",
    "section": "6.2 Lab report components",
    "text": "6.2 Lab report components\n\n6.2.1 Title\nYour title should be informative and clearly state the main focus or purpose of your study. It does not need to be especially clever or thought-provoking, you focus should be on clear communication. There are typically two ways to do this, either by a stating what was done (“Effect of Plant X on Radish Seed Growth.”) or what was found (“Plant X inhibits radish seed growth.”). You can consider how broad to make your title (“Allelopathic effects of Plant X”).\n\n\n6.2.2 Introduction\nYour introduction should establish the context the reader needs to understand the topic.\nThink of your introduction as the funnel or top piece of an hourglass that starts off broad and become increasingly focused on the specific focus of your study.\n\nIntroduce the broad topic/general research question you are asking, including why this is an important question to be asking. Give a brief overview of the current state of understanding of the research question. Make sure you introduce any key terms/concepts to set up the context of a large question/scientific field or overarching theme.\nIncreasingly narrow your scope down to the specific focus of your study. Describe why your question is relevant and how it furthers the understanding of this question: Even though you cannot fully answer the big question you laid out, you can contribute to part of the answer by filling a knowledge gap or clarifying existing conflicting results.\nClearly state the purpose of your research, you should state what your hypothesis is, what you expect to find and why, and how you are testing this1.\n\n1 No details about your methods/results yet at this point, just brief “how”.You should always have at least two paragraphs. The first, sets up the broad context2.The last paragraph summarizes what the specific study at hand is investigating and how that will be done. It forms a “bridge” between the introduction that sets up relevant background information (why is my question important?) and the methods section which is a very detailed account of how data was acquired (experimental design), processed, and analyzed.2 For a lab report you can likely do this in a single paragraph, in a longer research paper you will likely then take several following paragraphs to flesh out relevant background information\nYou can use a “formula” to make sure that your introduction ends with a clear statement of the purpose of your study that summarizes your introduction.\n\nIn this study, we investigated [CENTRAL QUESTION OR HYPOTHESIS]. To do this we used [DESCRIPTION OF HOW DATA WAS GENERATED] to [METRICS THAT WERE CALCULATED].\n\nYou should always be able to make a 2-3 sentence statement summarizing what you are investigating and how you did it, it’s a good self-check that you know what you’re trying to accomplish.\nChecklist for a good introduction\n\nAm I giving just enough context for my reader to understand my research question and rationale for my hypothesis or is there a lot of unrelated information?\nDo I explain how my study fits into the current understanding of the field and how it contributes to the overarching research question?\nAm I clearly defining all key terms?\nDid I include components that should actually be in my methods, results, or discussion?\n\nYour introduction should comprise about 15-20% of your lab report.\n\n\n6.2.3 Methods\nYour method section should describe your study design, how you collected, processed, and analyzed the data.\nYour “Introduction Funnel” has now landed you in the most specific part of your report, where you need to describe how you conducted your study, specifically how you gathered and analyzed your data. It should be sufficiently detailed so your study could be repeated by somebody else and reproduce the same results. Make sure you strike the balance of providing necessary details and why you made specific decisions without getting lost in the weeds3. It can be helpful to structure your method section with subheadings to include important components of your study, e.g.3 For example, we need to know that seedling length was measured in millimeters but we don’t need to know that each seedling was removed from the petri dish, laid out on a paper towel, measured by lab partner A etc\n\nStudy area: Describe geographic location, habitat of field experiment, you may want to include a map as a figure.\nExperimental design: Concisely describe how you set up your experiment in terms of sample size, replicates, what measurements where taken to produce the data set that was then analyzed.\nData Analysis: include how you manipulated data, specific calculations, descriptive statistics to summarize data (what metrics you produced), statistical tests.\n\nRemember, that your methods section should be written in paragraph form, not as a list of individual steps. Justify specific decisions you made about design where needed. You should not include a list of materials, rather you would include that information in the paragraph at the appropriate time, including specialized equipment, concentrations of chemical solutions etc.\nChecklist for a good methods section\n\nDid I provide enough details to allow study to be reproduced?\nDid I structure my methods in easy-to-follow, logical paragraphs as opposed to a bullet list or recipe-style set of instructions?\n\nYour methods should comprise about 10-15% of your report.\n\n\n6.2.4 Results\nThis section should present your results based the descriptive and inferential statistics you applied to analyze your data set.\nYou results section should include a written description along with data visualizations in the form of tables and figures that present your results in a way that makes it easy for the reader to identify key findings4. Focus on describing trends and patterns and report which findings are significant. Highlight notable results, when you mention anomalous and unexpected results make sure you pick up on those in the discussion. Remember that you should not be interpreting your data by comparing them to results from other studies, or interpreting them in the context of your research hypothesis.4 You should not include any raw data, it should always be summarized/analyzed.\nGenerally, you should only present your results in only one format (text, table, figure). So for example, if you have a table with the mean +/- standard deviation of your seedling lengths you would describe your results as “Seedlings exposed to plant xx where xx mm shorter (see Table 1)”. In you text you are describing the pattern, you are being specific and quantitative but you are not repeating the information in the Table which has the exact mean values. Using the written description allows you to give context to your results (“increased”, “wider”, “smaller”) and guide the reader through your results that you have spent a lot of time thinking about but are new to them.\nWhile your written description alone should provide all the needed information without the figures and tables, your figures and tables should complement the text of your report. You do want to make sure that your figures and tables stand alone5 by giving them a descriptive figure caption comprising of a descriptive title along with any other information that is needed to understand the information being presented. Figures and Tables should be numbered, by convention captions are placed below figures and above tables. Any figure or table that you include should be referenced in your text.5 This means they should be understood without reading the entire paper\nChecklist for a good results section\n\nHave I described all my summarized data and statistical results?\nDid I point out all the trends and patterns?\nDid I highlight notable/anomalous results?\nDid I report which results are significant?\nDid I make sure not to include an interpretation or explanation of my results?\n\nYour results should comprise about 10-15% of your lab report, figures/tables not included.\n\n\n6.2.5 Discussion\nA good discussion evaluates context, acknowledges constraints, and justifies conclusions.\nNow it is time to take your specific results and broaden the scope of your report as you interpret and discuss your results. Your discussion should mirror your introduction in structure and scope, as now you start with your specific study and build your discussion back out to the broad context you set up initially.\nTransition from your results section by making an explicit statement whether your results support your research question or not6. A good way to do this is by reiterating the original (broad) question you asked with short description of how you specifically investigated it, your key results7, and then interpreting/discussing those. Similar to the last paragraph of your introduction section, you can essentially follow a fill-in-the-blank-formula, which as you become more comfortable communicating your research will sound a little less formulaic and a little bit more you8:6 Remember that we generally use the terms “support” and “reject” when referring to a hypothesis not “prove” and “disprove”7 Make sure that you are making a very concise summary, not restating all of your results in detail.8 Notice how this statement is quite similar to how you ended your introduction, except now you have the results/an answer.\n\nIn this study, I investigated [broad question asked/hypothesis tested]. To achieve this, I [specific data set + analysis used]. I found that [key results] which [statement about how this does or does not support your hypothesis].\n\nBuild on this statement to write an effective discussion that reiterates the key results and interprets them, acknowledges constraints of the study design and other possible caveats limiting clear interpretation, states clear conclusions/take-home messages, and points to results (figures or tables) as evidence thereof. In addition to ending with a clear set of conclusions you should include these four components:\n\nInterpret your results and compare them to your expectations and other studies9:\n\nDid your results match your predictions?\nDo your results support your hypothesis? How do you interpret them in that context?\nWhy do you think you ended up with these results?\nAre your results consistent with similar studies/your classmates results? Different? Why?\n\nDiscuss constraints/limitations, inconsistencies, other possible explanations10\n\nAcknowledge and discuss constraints, point out shortcomings and any caveats for how your data should be interpreted.\nExplain unexpected results.\nIf results are inconsistent/ambiguous why are you confident you are interpreting them way you are?\nAre the other possible explanations? Other reasons of why you may have ended up with the results you did?\nAre there any errors, assumptions, ambiguities etc that could have impacted your results? Why you are still confident interpreting you data as you are?\nHow might you redesign the experiment for more clear results?\n\nDiscuss implications of your results11.\n\nHow do your results contribute to answering your overarching research question?\nWhy is your study important? What do we know now that we didn’t before?\n\nFuture steps\n\nWhat questions where generated?\nWhat is the next step to even better understand your overarching research question?\n\n\n9 In this semester we will generally keep the “compare to other studies very small, don’t worry, you won’t have to go on an extended literature search.10 This is still really specific to your study.11 Now you are broadening your scope to the broader context in which you asked your questionChecklist for a good discussion\n\nDoes your discussion state whether it supports your hypothesis?\nDid you discuss any issues with your study?\nDid you explain how your results fit into and contribute to the broader research question?\n\nYour discussion should comprise about 40-45% of your Lab report12.12 The more practice you get, the more you will find that this is the case."
  },
  {
    "objectID": "A06_writing-report.html#sec-general-notes-on-scientific-reports",
    "href": "A06_writing-report.html#sec-general-notes-on-scientific-reports",
    "title": "6  Writing a scientific report",
    "section": "6.3 General Notes on scientific reports",
    "text": "6.3 General Notes on scientific reports\nThe way you structure and format your report plays a critical role in how the content is delivered. Your goal should be to produce a report that is well-organized, coherent, and the presentation of the information is clear, engaging, and enhances the understanding. This means, that you want to lower the cognitive burden of your reader, i.e. help them think by guiding them through your thought process - you want them to come to the same conclusion as you. Structure your information to help them do this and make that structuring explicit by using headers and sub-headers to make the hierarchy of information and connections obvious to them.\nHere are a few things you can incorporate in terms of structure to help keep your reader engaged and following the story you are telling.\n\n6.3.1 Think of every paragraph as a container of a single thought\nParagraphs are your fundamental building block. A paragraphs says one thing: Your first sentence13 summarizes the overall message or main points of a paragraph and functions as the logical transition from the previous paragraph. The rest of the paragraph supports, elaborates, and defends it. Effective paragraphs comprise about 100 - 200 words, much longer and you usually have more ideas creeping in, much shorter and you’re likely missing a key point. A good way to proof read your paragraphs is to read it out loud to help you spot sentence fragments, run-on sentences, places where the subject and verb are mismatched.13 This is generally referred to as a topic sentence\n\n\n6.3.2 Structure your thinking and then your writing\nRemember to embrace the hour glass structure: Your introduction should start broad, then become more specific as you narrow in on your specific question. After describing your study design and completing your analysis and describing your results (this is the most specific piece of content) you start narrow with your results and then become increasingly broad in your discussion points as you put your results back into the original context of the question you asked and end with your conclusions.\nBefore you start writing paragraphs, create an outline to clarify your thinking and then your writing. Start with the key components, then for each section add your main points and the supporting details14. At this bare bones stage you are just looking at the skeleton and so it is easier to spot anything in that paragraph that does not belong in there or needs to be in another place. Flesh out each “main point” into a topic sentence and gradually build out your supporting points into full sentences.14 You may find that some of your supporting details are actually main points and vice versa it is a lot easier to move those around at this point.\n\n\n6.3.3 Don’t write your report linearly:\nStart with the last statement in your introduction15:15 Think of this as a trailer previewing the remainder of a season “This season on XXX”\n\nIn this study, we investigated [CENTRAL QUESTION OR HYPOTHESIS]. To do this we used [DESCRIPTION OF HOW DATA WAS GENERATED] to [METRICS THAT WERE CALCULATED].\n\nNext, move onto your methods. Then, take the subheading of your methods and fill in the results for each section. Now that you have your results you can write the first section of your discussion16:16 Think of this as the quick summary before a tv episode “Previously, on XXX\n\nIn this study, I investigated [broad question asked/hypothesis tested]. To achieve this, I [specific data set + analysis used]. I found that [key results] which [statement about how this does or does not support your hypothesis].\n\nBuild out your discussion, by starting with the interpretation of your results and then your discussion. Finally, widen the scope to discuss how your study fits into the broad research context. Think about your introduction and discussion as bookends framing your specific study: There is an element of them being mirror images where the key points are quite similar with the difference being that in the discussion you are revisiting key statements and questions you raised in the introduction with the specific information you have just gleaned.\nNow, that you know what that broad research context is you know what themes you need to set up in your introduction. And then as a final touch, come up with a descriptive title for your lab report.\n\n\n6.3.4 Use parallel structures where possible:\nThe hour glass structure, Introduction/Discussion as bookend with matching last intro/first discussion paragraphs are parallels that help your reader because it is easy to follow your line of thought. Additionally you can add explicit parallels to lower the cognitive burden by intentionally structuring steps/points consistently in the same way. For example, if you list three major parts in your last introduction paragraph list them in the same logical sequence you will complete them in your data analysis section, in your data analysis section use sub-headers to indicate which component you are currently completing and then summarize your results after each section (this becomes even more important when you separate methods and results into separate sections). In your first paragraph of the discussion list those key take-aways in the same sequence and then in your discussion, have the paragraphs discussing each key point in the same sequence as well.\n\n\n6.3.5 Don’t just give the what, make sure you include the why:\nDon’t just list what you are doing, always explicitly state why you are taking this step, e.g. don’t say “I made a scatterplot” say “I am exploring the relationship of xx and xx using a scatterplot”. An easy formulaic way to write this out is “I am going to do xxx to determine xxx”. Similarly, where this is appropriate justify why this is an appropriate choice.\n\n\n6.3.6 Be precise and accurate\nYou need to make sure that your reader is understanding the exact meaning that you want to convey.\n\nUse formal, written English\nMake sure you are using correct terminology. For example, only use “significant” when you are talking about statistical results.\nAvoid being jargony17.\nBe specific and concrete. For example, use “for 48 hours”, not “a period of time”.\n\n17 Make sure people know the words you are using, for example, by defining them the first time they are used.\n\n6.3.7 Use active voice\nFor this course, use the active voice when possible as this will allow you to write in a more direct and clear fashion.\nWhile passive voice is not grammatically incorrect, it can lead to awkward syntax and be less clear. There is some debate whether scientific reports should be written in passive voice18 or in passive voice (“People do science”). Passive voice has a sentence construction of Object-Verb-Subject which makes the object the focus of the sentence and can obscure who is acting. By contrast, active voice places an emphasis on who is acting by making the subject the focus of the sentence (Subject-Verb-Object).18 “Science happens”; this emphasizes the objectivity\n\n\n6.3.8 Use the correct Verb Tense\nFor lab reports we use past, present, and future tenses in specific context and you should make sure you are being consistent in how you do this. You should be using the past tense when you are referring to events in the past, including results obtained in the past, and tasks you completed (methods section). You should use the present tense in the Introduction and Discussion section when you are presenting established knowledge and the implications of your results. You would only use future tense when you are talking about future steps you might take."
  },
  {
    "objectID": "B02_allelopathy-methods.html#bioassay-set-up",
    "href": "B02_allelopathy-methods.html#bioassay-set-up",
    "title": "8  Allelopathy Methods",
    "section": "8.1 Bioassay set up",
    "text": "8.1 Bioassay set up\nEven through you are working as part of a group, each of you will set up a control and a treatment assay, essentially these become replicates as part of your experimental design and you will combine your data across replicates to get a larger sample size and minimize effects of uncontrolled variables/confounding factors. Make sure that you read through the instructions before you get started and discuss with your group how you will implement them. Make sure that you keep communicating with your lab partners as you set up your experiment to make sure that you are being as consistent as possible.\n\nObtain two complete petri dishes. Double check that you have a matching top and bottom; when resting on a surface, the open rim of the bottom part of the dish should rest against the underside of the overlapping lid of the dish. If this is not the case, swap tops and bottoms or get a new set of dishes.\nCut four pieces of paper towel that will fit into the bottom of a petri dish. Then place two pieces of paper towel into the bottom of each of your two petri dishes. \nLabel the tops of both dishes with your name, section, and treatment (“Exp” or “Ctrl”). \nFinely chop/mince your plant matter on a cutting board. \nUse weigh boats to weigh out 0.8gof plant material.\nUse the tin foil to make a small container for the plant material, then place the aluminum container with the plant material into the experimental dish so the plant material will never contact the water in the dish, but is free to release volatile chemicals into the dish atmosphere. Make sure the you are using the same amount of tin foil and configuration in your experimental and control dish and your group members are doing the same. \nWash your hands really well to remove any chemicals picked up on your fingertips from the chopped up plant matter. Then obtain 40 radish seeds using clean small weigh boat. Place 20 of the seeds on the paper towel (not in tin foil!) in your experimental dish and 20 seeds on the paper in your control dish. Spread the seeds out with the tip of a pen or pencil or scissors. Communicate with your group mates to standardize your set up.\nOpen the dishes, wet the paper in each Petri dish with ~10 ml of distilled water using a small beaker. Make sure you are being consistent in the amount of water you and your group mates are using.\nPlace your dishes on a tray.\nCarefully wash knives, presses, cutting boards, weighing boats and small petri dishes and set out to dry on paper towels next to sinks. Make sure everything is rinsed well."
  },
  {
    "objectID": "B02_allelopathy-methods.html#determine-germination-rate",
    "href": "B02_allelopathy-methods.html#determine-germination-rate",
    "title": "8  Allelopathy Methods",
    "section": "8.2 Determine germination rate",
    "text": "8.2 Determine germination rate\nYou will need to come into the lab approx. 48 hours after you start the experiment. Your data should be collected no later than 60 hours after your initial set up. Ideally, each of you will count their own seeds, if there is an issue coming in to count seeds due to work/class schedule etc. please make sure you communicate this to your lab group and somebody else is able to count your seeds. You are responsible to make sure you remind them. If your germination rate for the entire group is not determined at approx. the same time you will not be able to analyze your data set properly.\n\nOpen the GoogleSheet to record your data and and navigate to the tab for your lab group. Familiarize yourself with the data sheet and how you will enter your data.\n\nColumn A: enter the name of your plant you are testing; this will be the same for all of your group members1.\nColumn B: Recall that the petri dishes set up by each group member are replicates that help us control for confounding variables and increase our sample size. Enter your lastname.\nColumn C: Here you will record your treatment, choose either “ctrl” or “exp” from the dropdown menu as you enter information from your Control and Experiment petri dish respectively.\nColumn D: We are going to arbitrarily number your seeds 1 - xx to distinguish them in the data set. You do not have to be able to associate the numbers back to your seeds.\nColumn E: As you check each seed chose either “germinated” or “no germination” from the menu.\nColumn F: Record the date you set up your assays. This will be the same for all of your data points.\nColumn G: Record the time you set up your assays. This doesn’t have to be exact to the minute, within an hour is fine.\nColumn H: Record the date you came in to record the germination status.\nColumn I: Record the time you finished recording your data2.\n\nRecover and open your first petri dish. Inspect it to see if there are any concerns about your assays, e.g., excessive drying, leaky boats, etc.; if so take a picture for your records and notify your instructor. Record whether you are checking seeds in a Control or Experiment dish in your datasheet.\nCheck each seed and record it as having germinated if it has a visible radicle3. You may need to move seeds around a bit4 to see if they truly have germinated. Do NOT measure the length of seeds at this time.\nRepeat the process with your second dish.\nDouble check your data entry to make sure you have included all the information (see point 1).\n\n1 This might seem unnecessary but is an example of building good data management habits that can save a lot of work and headaches down the line. Including it means that you could combine data with other groups at a later point and that way the data set itself includes this information if that information should be lost down the line.2 The reason why we are including this information on when the assays were set up and when you recorded your observations is that if your results are unusual or inconsistent you would be able to go. Frequently, scientists are working on multiple projects or you might have several people collaborating and data might not be analyzed right after it is recorded. Consider “future you” as your most important collaborator and keeping good records is being a good collaborator.3 This is the “primary” or “seed” root.4 A pencil or similar is helpful."
  },
  {
    "objectID": "B02_allelopathy-methods.html#determine-seedling-lengths",
    "href": "B02_allelopathy-methods.html#determine-seedling-lengths",
    "title": "8  Allelopathy Methods",
    "section": "8.3 Determine seedling lengths",
    "text": "8.3 Determine seedling lengths\nWe will determine seedling length for all seeds that germinated one week after set-up during our normal lab period.\n\nOpen the GoogleSheet to record your data and and navigate to the tab for your lab group. Familiarize yourself with the data sheet and how you will enter your data.\n\nColumn A: Enter the name of your plant you are testing.\nColumn B: Enter your last name to distinguish your replicates from those of your group members.\nColumn C: Choose either “ctrl” or “exp” from the dropdown menu as you enter information from your Control and Experiment petri dish respectively.\nColumn D: We are going to arbitrarily number your seedlings 1 - xx to distinguish them in the data set. You don’t have to be able to associate the numbers back to your seedlings.\nColumn E: We will record the length for all germinated seeds.\n\nRetrieve your petri dishes with your seeds. Inspect them to see if there are any concerns about your assays, e.g., excessive drying, leaky boats, etc. If this is the case call your instructor over to inspect. If there are any issues make sure to take notes that you can refer to when you discuss your results.\nRetrieve a small ruler to measure the length of your seedlings. Start with your control dish. Gently separate your seedlings and lay them out on a piece of paper. Straighten each seedling as much as you can and then measure the length. You may have to break or cut some seedlings to straighten them. Even if only a small amount of growth has occurred still measure and include those values. Record lengths in the Google Sheet in millimeters. In some cases, only a small bit of growth may have occurred (only a few mm of growth may be have occurred)5.\nRepeat Step 2 for your experimental dish.\nWhen you are finished, double check that you measured all your seedlings and recorded all your data. Then dump your paper towel and seedlings in the trash along with your tin foil boats and plant material. Rinse your petri dishes and set them up to dry.\n\n\n\n5 It is possible that some seeds that had not germinated when you first checked them have now germinated. Do not change your original germination data. We want to know how many seeds germinated within about 48-60 hours!"
  },
  {
    "objectID": "B03_allelopathy-analysis.html#germination-frequency",
    "href": "B03_allelopathy-analysis.html#germination-frequency",
    "title": "9  Allelopathy Data Analysis",
    "section": "9.1 Germination frequency",
    "text": "9.1 Germination frequency\n\n9.1.1 Get the data\nGo to the GoogleSheet that contains the germination counts for your lab and select the tab for your group. Double check that the data has been entered correctly. Then go to File -&gt; Download -&gt; Tab Separated Values (.tsv) down load your data as a tab-delimited file. Now, use your File Explorer if you are on a Windows computer and Finder of you are on a Mac and navigate to the Downloads folder. You should see the file you just downloaded there. Rename it to gemination_plant.tsv depending on the plant you are testing and place it in the data folder of your 02_Allelopathy project folder.\nOnce you go back to your Rstudio window with your Rproj loaded you should see the file in your data folder in the Files pane. Double check that it is there, if not, as you instructor to help you get set up.\nNow we are ready to read in our germination data as a data.frame that we can compute on in R.\n\n# define your filename\nfile &lt;- \"germination_demo-1.tsv\" # CHANGE THIS TO YOUR EXACT FILENAME, remember R is case sensitive\n\n# read in data\ngermination &lt;- read_delim(here(\"data\", file), delim = \"\\t\") %&gt;%\n  clean_names()\n\nRows: 120 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): test-plant, replicate, treatment, germination, time-recorded\ndbl  (1): seedling\nlgl  (1): notes\ndate (2): date-setup, date-recorded\ntime (1): time-setup\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# take a look at first few lines\nhead(germination)\n\n# A tibble: 6 × 10\n  test_plant replicate  treatment seedling germination    date_setup time_setup\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;          &lt;date&gt;     &lt;time&gt;    \n1 plant      lastname-1 ctrl             1 germinated     2023-01-12 09:00     \n2 plant      lastname-1 ctrl             2 germinated     2023-01-12 09:00     \n3 plant      lastname-1 ctrl             3 germinated     2023-01-12 09:00     \n4 plant      lastname-1 ctrl             4 germinated     2023-01-12 09:00     \n5 plant      lastname-1 ctrl             5 germinated     2023-01-12 09:00     \n6 plant      lastname-1 ctrl             6 no germination 2023-01-12 09:00     \n# ℹ 3 more variables: date_recorded &lt;date&gt;, time_recorded &lt;chr&gt;, notes &lt;lgl&gt;\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nTake a look at the data.frame object holding your data by clicking on it in the Environment pane (bottom left) or using View(germination).\nDescribe how your data is organized:\n\nwhat information is in each row?\nwhat information is in each column?\n\nAssess what type of data you have\n\nis it continuous or categorical?\nwhat groups are you comparing?\n\nThen determine which descriptive statistics are appropriate to summarize this data set and how you would visualize your data.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nEach row represents one seed. And each column contains information for one variable.\nYour data is categorical with two possible values “germinated” and “not germinated” and you have two groups (treatments) you are comparing “Control” and “Experiment”.\nWe want to know the total number of seeds that germinated and did not germinate across all replicates, and what proportion of the total number of seeds germinated.\nBecause we have categorical data and want to visualize how the data is distributed across our categories, we will want to use a bar plot to visualize our data.\n\n\n\n\n\n9.1.2 Summarize your results\n\n\n\n\n\n\nConsider this\n\n\n\nYour data.frame contains the raw data.\nSuggest 2-3 ways that you can summarize your data in a meaningful way to describe your results in the context of the hypothesis you are exploring with this experiment. Explain why you think these would be appropriate data analysis methods.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe are going to calculate the total number and relative proportion of seeds that germinated for each treatment.\n\n\n\nExecute the code below to calculate some summary statistics for your germination data. Use the #| tbl-cap code chunk option to create a table caption. Note that we are using the #| label the code chunk option to give the code chunk a name starting with tbl-. By doing this your tables will be automatically numbered.\n\ngermination %&gt;%\n  group_by(treatment) %&gt;%             # group data by treatment\n  count(germination) %&gt;%              # count number seeds germinated\n  mutate(percent = n/sum(n)*100) %&gt;%\n  kable(digits = 2)\n\n\n\n\n\n\ntreatment\ngermination\nn\npercent\n\n\n\n\nctrl\ngerminated\n39\n65.00\n\n\nctrl\nno germination\n21\n35.00\n\n\nexp\ngerminated\n26\n43.33\n\n\nexp\nno germination\n34\n56.67\n\n\n\nTable 9.1: Summary of germination success after 48 hrs. Total number (n) and percent of seeds that germinated.\n\n\nThis table gives you two summary statistics, but the table might not be the best way to present your data for easy comparison in a report.\nData visualizations are frequently more helpful present your results compared to tables with numbers, so let’s create a grouped bar chart visualizing our results.\n\n# pick two colors for your control and experimental treatment\ncolors &lt;- c(\"darkorange\", \"cyan4\")\n\n\n# create bar plot with counts\nggplot(germination, aes(x = treatment,          # specify categories on x axis\n                        fill = germination)) +  # specify how to group data\n  geom_bar(stat = \"count\",                      # length of bar indicates number of observations\n           width = .75,                         # space between groups\n           position = position_dodge(.8),       # space between bars in group\n           color = \"black\") +                   # line color\n  scale_fill_manual(values = colors) +          # use custom colors\n  labs(x = \"Treatment\",                         # x-axis title\n       y = \"Number of Seeds\") +                 # y-axis title\n  theme_classic() +                             # specify theme\n  theme(legend.position = \"bottom\")             # place legend under figure\n\n\n\n\nFigure 9.1: Germination success of radish seeds exposed to odiferous plant material. Number of seeds that germinated (orange) and did not germinate (green) after 48 hours for control (ctrl; no exposure) and experimental treatment (exp).\n\n\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nUsually for your report you would chose to either include a table or a visualization that summarizes your data. In either case you would include a descriptive figure or table title/caption that describes how to interpret the figure2.\nThen you describe your results referencing your figure/table, however, your reader should be able to have the full picture of your results even without seeing the actual figure/table. A good template to use is to start with the broad pattern and then highlight important details. A good description of results contextualizes them for the reader3. You have already done all the thinking, guide your reader through what you’ve found.\nDescribe your results4. Remember, at this point you should not be interpreting them.\n\n\n4 Consider this thinking out loud in a way where you have a set of notes that you can then edit for your report.3 For example, instead of just listing out values provide context and help your reader see the patterns by using terms like “higher” “lower”, “similar” or highlighting certain results as “notably, …”, “except for…”.2 As a rule of thumb, you should have enough information in the caption that the reader can understand the figure even without reading the methods/results\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour description could look something like this:\nBroad pattern:\n\nRadish seeds exposed to plant had a 22% lower germination success compared to the control.\n\nDetails\n\nOverall, a total of 39 seeds (65%) germinated in the control, while only 26 (43%) germinated in the experimental treatment (Figure xx).\n\nIt’s important that we add this context because it is quite different if 100% germinated in the control and 78% in the experimental treatment or something more similar to what we observe here.\n\n\n\nDescriptive statistics are the important first step to summarizing our findings. However, as you know, at this point we don’t know if our observed differences in germination success are just due to sampling error or if they actually represent reality.\n\n\n\n\n\n\nConsider this\n\n\n\nArgue which type of statistical test we should apply and outline the key steps you will need to take to perform the test.\n\n\n\n\n9.1.3 Test for statistical significance\nWe need to apply a \\(𝛘^2\\) to evaluate the difference between frequencies.\n\n\n\n\n\n\nGive it a try\n\n\n\nState your null and alternative hypothesis and predict the outcome if the claim is true.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour null hypothesis is a claim that there is no effect:\n\n\\(H_{0}\\): Presence of plant X does not have an effect on the germination rate of radish seeds.\nPrediction: The number of seeds that germinate are not different between the experimental treatment and the control.\n\nThe alternative hypothesis is a claim that there is an observed effect.\n\n\\(H_{0}\\): Presence of plant X has a negative effect on the germination rate of radish seeds.\nPrediction: The number of seeds that germinate for seeds exposed to plant X will be lower compared to the control.\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nWe will specify our significance threshold \\(\\alpha\\) as 0.05.\nExplain what this means and why it is important for us to specify this.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis means that if our observed results have a less than 5% probability occurring according to our null distribution we will reject our null hypothesis.\n\n\n\nOur next step is to calculate our observed test statistic:\n\n# calculate observed statistic\nobserved_statistic &lt;- germination %&gt;%        # define data to use\n  specify(response = germination,            # specify response (dependent) variable\n          success = \"germinated\",            # specify response that is success\n          explanatory = treatment) %&gt;%       # specify explanatory (independent) variable\n  hypothesize(null = \"independence\") %&gt;%     # define null hypothesis\n  calculate(stat = \"Chisq\",                  # define test statistic to use\n            order = c(\"ctrl\", \"exp\"))        # order to subtract explanatory variables\n\n# print test statistic\nobserved_statistic\n\nResponse: germination (factor)\nExplanatory: treatment (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  4.83\n\n\nNow, let’s simulate our null distribution and compare how our observed test statistic compares.\n\n# create null distribution\nnull_dist &lt;- germination %&gt;%\n  specify(response = germination,            # specify response (dependent) variable\n          success = \"germinated\",            # specify response that is success\n          explanatory = treatment) %&gt;%       # specify explanatory (independent) variable\n  hypothesize(null = \"independence\") %&gt;%     # define null hypothesis\n  generate(reps = 1000,                      # number of samples to generate null distribution\n           type = \"permute\") %&gt;%             # shuffle to break association\n  calculate(stat = \"Chisq\",                  # define test statistic to use\n            order = c(\"ctrl\", \"exp\"))        # order to subtract explanatory variables\n\n\n# visualize null distribution test statistic\nnull_dist %&gt;%\n  visualize(method = \"both\") +\n  shade_p_value(observed_statistic,\n                direction = \"greater\")\n\nWarning: Check to make sure the conditions have been met for the theoretical\nmethod. {infer} currently does not check these for you.\n\n\n\n\n\nFigure 9.2: Null distribution of chi-square test statistic assuming presence of plant material has no effect on radish seed growth. Bars indicates permuted null distribution, black line indicates theoretical chi-square distribution. The calculated chi-square test statistic is indicated by the red line. Values observed in the null distribution that are more extreme that the t-statistic calculated for the empirical data set are shaded in red.\n\n\n\n\nAnd for our last step, we need to get our p-value5:5 Note: for your report, you do not need to visualize your null distribution we are doing this to help us get a better feel for how statistical tests worth. You can combine content from code chunks to go straight to getting your p-value.\n\n# obtain p-value\nnull_dist %&gt;%\n  get_p_value(obs_stat = observed_statistic,\n              direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.033\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nCompare your p-value to the threshold of significance we defined above to determine how to interpret the results of your chi^{2}$-test.\nThen update the results you formulated earlier to add this information into your description of the results.\nFinally, write a few sentences as a draft of how you would include this in the discussion of your results, i.e. how you will interpret your results to draw a conclusion in the context of our broad research hypothesis that we designed this experiment to explore.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn this case our p-value is 0.023 which means that is a less than 2.3% probability that we would observe a test statistic at least this extreme give our null hypothesis.\nThis value is smaller than our specified threshold of significance and therefore we will reject our null hypothesis and conclude that the difference in the observed germination rates is statistically significant and consistent with our hypothesis that plant X has an effect on the germination rate.\nWe can augment our results to include this information:\n\nRadish seeds exposed to plant had a 22% lower germination success compared to the control. Overall, a total of 39 seeds (65%) germinated in the control, while only 26 (43%) germinated in the experimental treatment (Figure xx). This difference is statistically significant (p = 0.02).\n\nIn the discussion we would interpret our results:\n\nThe germination rate of radish seeds was significantly lower when they were exposed to plant X (Figure X). This is consistent with our hypothesis that plant X is releasing an allelopathic chemical compound which impacts radish seed germination.\n\n\n\n\nRemember, if your results are not statistically significant, you still need to write a set of conclusions! Even though we seem to think that “negative results” are “bad”, “negative” just means that we did not observe an effect - but that is also a result because you can now exclude one possible explanation for your observation and move on to test the next one."
  },
  {
    "objectID": "B03_allelopathy-analysis.html#seedling-growth",
    "href": "B03_allelopathy-analysis.html#seedling-growth",
    "title": "9  Allelopathy Data Analysis",
    "section": "9.2 Seedling growth",
    "text": "9.2 Seedling growth\n\n9.2.1 Get your data\nGo to the GoogleSheet that contains the seedling lengths for your lab and select the tab for your group. Double check that the data has been entered correctly. Then go to File -&gt; Download -&gt; Tab Separated Values (.tsv) down load your data as a tab-delimited file. Now, use your File Explorer if you are on a Windows computer and Finder of you are on a Mac and navigate to the Downloads folder. You should see the file you just downloaded there. Rename it to seedling-length_plant.tsv depending on the plant you are testing and place it in the data folder of your 02_Allelopathy project folder.\nOnce you go back to your Rstudio window with your Rproj loaded you should see the file in your data folder in the Files pane. Double check that it is there, if not, as you instructor to help you get set up.\nNow we are ready to read in our germination data as a data.frame that we can compute on in R.\n\n# define your filename\nfile &lt;- \"seedling-length_demo-1.tsv\" # CHANGE THIS TO YOUR EXACT FILENAME, remember R is case sensitive\n\n# read in data\nlength &lt;- read_delim(here(\"data\", file), delim = \"\\t\") %&gt;%\n  clean_names()\n\nRows: 40 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): test-plant, replicate, treatment\ndbl (2): individual, length_mm\nlgl (1): notes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# take a look at first few lines\nhead(length)\n\n# A tibble: 6 × 6\n  test_plant replicate individual treatment length_mm notes\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;lgl&gt;\n1 plant-1    test-1             1 ctrl              9 NA   \n2 plant-1    test-1             2 ctrl             10 NA   \n3 plant-1    test-1             3 ctrl              5 NA   \n4 plant-1    test-1             4 ctrl              8 NA   \n5 plant-1    test-1             5 ctrl              8 NA   \n6 plant-1    test-1             6 ctrl              7 NA   \n\n\n\n\n9.2.2 Summarize your results\n\n\n\n\n\n\nConsider this\n\n\n\nTake a look at the data.frame object holding your data by clicking on it in the Environment pane (bottom left) or using View(length).\nDescribe how your data is organized:\n\nwhat information is in each row?\nwhat information is in each column?\n\nAssess what type of data you have\n\nis it continuous or categorical?\nwhat groups are you comparing?\n\nThen determine which descriptive statistics are appropriate to summarize this data set and how you would visualize your data.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nEach row represents one seedling. And each column contains information for one variable.\nYour data is continuous; you have a length measurement in millimeters. You have two groups (treatments) you are comparing “Control” and “Experiment” which is categorical.\nYou will want to determine the mean and standard deviation of the seedling length for both treatments. It could also be helpful to generate additional summary statistics including median, minimum and maximum values to give yourself an overview of the data.\nWe can explore to visualizations for your data, the first is that you can visualize the distributions using a histogram, the second is that you can visualize the mean and standard deviation using a bar chart or similar.\n\n\n\nWe will need to summarize our results to evaluate our findings.\n\n\n\n\n\n\nConsider this\n\n\n\nSuggest 3-4 metrics to summarize your data in a meaningful way that will allow you to describe your results in the context of the hypothesis you are exploring with this experiment.\nExplain why you think these would be appropriate data analysis methods.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAs we have continuous data, we can use the descriptive statistics we have discussed previously that allow us to assess the distribution (histogram), central tendency (mean, median, mode) and variability (variance, standard deviation) of our data.\n\n\n\nLet’s start by putting together a table with summary statistics.\n\nlength %&gt;%                                # specify data\n  group_by(treatment) %&gt;%                 # group by treatment\n  summarize(mean = mean(length_mm),       # calculate mean\n            sd = sd(length_mm),           # calculate standard deviation\n            median = median(length_mm),   # determine median value\n            min = min(length_mm),         # determine minimum value\n            max = max(length_mm))         # determine maximum value\n\n\n?(caption)\n\n\n\n# A tibble: 2 × 6\n  treatment  mean    sd median   min   max\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ctrl        8.4  1.47      9     5    11\n2 exp         6.2  1.70      6     3    10\n\n\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nDescribe your results6. Make sure that you are being specific and adding context to make it easy for your reader to identify the key patterns.\nAs rule of thumb, you should describe your results so that your reader has all the key information they need even without seeing the data tables.\nRemember, at this point you should not be interpreting your results!\n\n\n6 Consider this thinking out loud in a way where you have a set of notes that you can then edit for your report.\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour description could look something like this:\nBroad pattern:\n\nOverall, seedling growth was lower for radish seeds exposed to plant X compared to the control.\n\nDetails\n\nMean seedling growth after 1 week was 6.2 +/- 1.7mm for plants exposed to plant X compared to a mean length of 8.4 +/- 1.47mm in the control. Similarly, the variability in seedling length was higher in exposed radish seeds (range = 3 - 10) compared to the control (range = 5 - 11).\n\nIn this case, you would not need to include table because you’ve included all the values, rather you might chose to include figure that effectively summarizes your data that you would then reference here.\n\n\n\nAgain, data visualizations are an important tool to summarize and present your data. Let’s start by visualizing our data with a histogram which emphasize the distribution of our values.\n\nggplot(length, aes(x = length_mm, fill = treatment)) +\n  geom_histogram(binwidth = 1,\n                 color = \"black\") +\n  facet_grid(treatment ~ .) +\n  scale_fill_manual(values = colors) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 9.3: Seedling length distribution after 1 week for radish seeds explosed to plant X (green) and not (orange).\n\n\n\n\nAlternatively, we could create a bar plot where the height is scaled relative to the mean. This also allows us to add error bars that are scaled to the length of the standard deviation7.7 This is a classic plot for experiments with multiple treatments\n\n# create dataframe with mean and sd\nsummary &lt;- length %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(mean = mean(length_mm),\n            sd = sd(length_mm))\n\nggplot(summary) +\n  geom_bar(aes(x = treatment, y = mean,\n               fill = treatment),\n           stat = \"identity\",\n           color = \"black\") +\n  geom_errorbar(aes(x = treatment,\n                    ymin = mean-sd,\n                    ymax = mean+sd),\n                width = 0.25,\n                size = 1.2) +\n  scale_fill_manual(values = colors) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 9.4: Mean seedling length and standard deviation after 1 week for radish seeds explosed to plant X (green) and not (orange).\n\n\n\n\nBar plots take up a lot of “ink” and so an alternative is to create a plot using a point range. In that case the mean is represented by a point and the standard deviation by lines extending out from that point.\n\n# create dataframe with mean and sd\nsummary &lt;- length %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(mean = mean(length_mm),\n            sd = sd(length_mm))\n\n# create dataframe with mean and sd\nsummary &lt;- length %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(mean = mean(length_mm),\n            sd = sd(length_mm))\n\nggplot(summary) +\n  geom_pointrange(aes(x = treatment,\n                    y = mean,\n                    ymin = mean-sd,\n                    ymax = mean+sd,\n                    color = treatment),\n                size = 1.2) +\n  scale_color_manual(values = colors) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 9.5: Mean seedling length and standard deviation after 1 week for radish seeds explosed to plant X (green) and not (orange).\n\n\n\n\nAgain, our set of summary statistics and data visualization enable us to present the patterns in our data set. However, at this point we cannot make statement whether our observed differences in seedling lengths are within the range of variability in growth we would expect to see even if the plant material does not have an effect on growth.\n\n\n\n\n\n\nConsider this\n\n\n\nArgue which type of statistical test we should apply and outline the key steps you will need to take to perform the test.\n\n\n\n\n9.2.3 Test for significance\nWe need to apply a \\(t\\)-test to evaluate the difference between means.\n\n\n\n\n\n\nGive it a try\n\n\n\nState your null and alternative hypothesis and predict the outcome if each claim is true.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour null hypothesis is a claim that there is no effect:\n\n\\(H_{0}\\): Presence of plant X does not have an effect on the growth rate of radish seeds.\nPrediction: The seedling length does not differ between the experimental treatment and the control.\n\nThe alternative hypothesis is a claim that there is an observed effect.\n\n\\(H_{0}\\): Presence of plant X has a negative effect on the growth rate of radish seeds.\nPrediction: The seedling length for seeds exposed to plant X will be lower compared to the control.\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nWe will specify our significance threshold \\(\\alpha\\) as 0.05.\nExplain what this means and why it is important for us to specify this.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis means that if our observed results have a less than 5% probability occurring according to our null distribution we will reject our null hypothesis.\n\n\n\nOur next step is to calculate our observed test statistic:\n\n# calculate observed statistic\nobserved_statistic &lt;- length %&gt;%             # define data to use\n  specify(response = length_mm,              # specify response (dependent) variable\n          explanatory = treatment) %&gt;%       # specify explanatory (independent) variable\n  calculate(stat = \"diff in means\",          # define test statistic to use\n            order = c(\"ctrl\", \"exp\"))        # order to subtract explanatory variables\n\n# print test statistic\nobserved_statistic\n\nResponse: length_mm (numeric)\nExplanatory: treatment (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1   2.2\n\n\nNow, let’s simulate our null distribution and compare how our observed test statistic compares.\n\n# create null distribution\nnull_dist &lt;- length %&gt;%\n  specify(response = length_mm,              # specify response (dependent) variable\n          explanatory = treatment) %&gt;%       # specify explanatory (independent) variable\n  hypothesize(null = \"independence\") %&gt;%     # define null hypothesis\n  generate(reps = 1000,                      # number of samples to generate null distribution\n           type = \"permute\") %&gt;%             # shuffle to break association\n  calculate(stat = \"diff in means\",          # define test statistic to use\n            order = c(\"ctrl\", \"exp\"))        # order to subtract explanatory variables\n\n\n# visualize null distribution test statistic\nnull_dist %&gt;%\n  visualize() +\n  shade_p_value(observed_statistic,\n                direction = \"greater\")\n\nWarning in min(diff(unique_loc)): no non-missing arguments to min; returning\nInf\n\n\n\n\n\nFigure 9.6: Null distribution of t test statistic assuming presence of plant material has no effect on radish seed growth. The calculated t-square test statistic is indicated by the red line. Values observed in the null distribution that are more extreme that the t-statistic calculated for the empirical data set are shaded in red.\n\n\n\n\nAnd for our last step, we need to determine our p-value.\n\n# obtain p-value\nnull_dist %&gt;%\n  get_p_value(obs_stat = observed_statistic,\n              direction = \"greater\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nCompare your p-value to the threshold of significance we defined above to determine how to interpret the results of your chi^{2}$-test.\nThen update the results you formulated earlier to add this information into your description of the results.\nFinally, write a few sentences as a draft of how you would include this in the discussion of your results, i.e. how you will interpret your results to draw a conclusion in the context of our broad research hypothesis that we designed this experiment to explore.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn this case our p-value is 0.001 which means that is a less than 0.1% probability that we would observe a test statistic at least this extreme give our null hypothesis.\nThis value is smaller than our specified threshold of significance and therefore we will reject our null hypothesis and conclude that the difference in the observed germination rates is statistically significant and consistent with our hypothesis that plant X has an effect on the germination rate.\nWe can augment our results to include this information:\n\nOverall, seedling growth was significantly lower for radish seeds exposed to plant X compared to the control (p = 0.001). Mean seedling growth after 1 week was 6.2 +/- 1.7mm for plants exposed to plant X compared to a mean length of 8.4 +/- 1.47mm in the control. Similarly, the variability in seedling length was higher in exposed radish seeds (range = 3 - 10) compared to the control (range = 5 - 11).\n\nIn the discussion we would interpret our results:\n\nThe growth rate of radish seeds was significantly lower when they were exposed to plant X (Figure X). This is consistent with our hypothesis that plant X is releasing an allelopathic chemical compound which impacts radish seed growth.\n\n\n\n\nRemember, if your results are not statistically significant, you still need to write a set of conclusions! Even though we seem to think that “negative results” are “bad”, “negative” just means that we did not observe an effect - but that is also a result because you can now exclude one possible explanation for your observation and move on to test the next one."
  },
  {
    "objectID": "C01_ecol-succession.html#background-information",
    "href": "C01_ecol-succession.html#background-information",
    "title": "10  Ecological Succession",
    "section": "10.1 Background information",
    "text": "10.1 Background information\n\n10.1.1 Ecological Succession\nSuccession is a well-documented ecological phenomenon recently covered in the Community Ecology chapter in lecture. As a reminder, ecological succession is defined as a transition in species composition within a community following a disturbance or during the establishment of a community in an uninhabited environment. This sequential change in species composition in successional communities is in response to ongoing physical changes in the environment. These physical environmental changes are often brought about by the very species that composed earlier successional stages. As one community grows, it modifies the environment, and a different community develops as a result.\n\n\n10.1.2 Milk Nutrients and Spoilage\nMilk is a highly nutritious food containing important macromolecules: carbohydrates (lactose, a milk sugar), proteins (casein, commonly referred to as curd), and lipids (butterfat). This high level of nutrition makes milk an excellent medium for the growth of bacteria and other microbes. To minimize the risk of spoilage, milk is pasteurized. Pasteurization is a process by which milk is heated to a specific temperature for a set period of time to kill harmful bacteria. Pasteurizing milk does not sterilize it (sterilizing kills all bacteria) but merely destroys the pathogenic bacteria, leaving many bacteria that will multiply very slowly at refrigerator temperatures. However, if left at room temperature, the surviving bacteria will quickly grow and bring about milk spoilage."
  },
  {
    "objectID": "C01_ecol-succession.html#milk-microbes",
    "href": "C01_ecol-succession.html#milk-microbes",
    "title": "10  Ecological Succession",
    "section": "10.2 Milk Microbes",
    "text": "10.2 Milk Microbes\nOur next few weeks of lab experiments will examine microbial succession in milk, so let’s first define what we mean by microbe (aka microorganism). Simply stated, microbes are organisms that are too small to be seen without using a microscope. For the purposes of this lab, microbes include three broad groups of biological taxa: bacteria (type of prokaryote), yeasts (unicellular fungi), and molds (multicellular fungi). These three types of microbes can be distinguished by their size, cell type (prokaryotic v. eukaryotic), and whether they are unicellular or multicellular.\n\n\n\nFigure 10.1: Yeast and bacteria cells at 400x magnification.\n\n\nThe first comparison to make is between bacteria and yeast (?fig-bact-yeast). Superficially, they appear similar in that they are both microscopic and unicellular. However, there are a couple important differences. Recall from your first semester of General Biology that Bacteria are prokaryotes, which means that they have the smallest cells and lack organelles. Yeasts are single-celled fungi, which are eukaryotes, and like other eukaryotes they have generally larger cells that contain organelles, such as a nucleus and mitochondria. Comparing ?fig-bact-yeast and Figure 10.2 shows just how much of a size difference there is between the two at 400x magnification.\nThe other important comparison to make is between colonies of bacteria or yeast versus mold (Figure 10.2). As a multicellular fungus, mold is easy to distinguish from the unicellular bacteria or yeast under a microscope. Even without magnification, molds are easily identified by their “fuzzy” appearance, which is due to their body morphology of long, filamentous strands growing over top each other.\n\n\n\nFigure 10.2: Mold and yeast in a petri dish."
  },
  {
    "objectID": "C01_ecol-succession.html#introduction",
    "href": "C01_ecol-succession.html#introduction",
    "title": "10  Ecological Succession",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\n\n10.1.1 Ecological Succession\nSuccession is a well-documented ecological phenomenon recently covered in the Community Ecology chapter in lecture. As a reminder, ecological succession is defined as a transition in species composition within a community following a disturbance or during the establishment of a community in an uninhabited environment. This sequential change in species composition in successional communities is in response to ongoing physical changes in the environment. These physical environmental changes are often brought about by the very species that composed earlier successional stages. As one community grows, it modifies the environment, and a different community develops as a result.\n\n\n10.1.2 Milk Nutrients and Spoilage\nMilk is a highly nutritious food containing important macromolecules: carbohydrates (lactose, a milk sugar), proteins (casein, commonly referred to as curd), and lipids (butterfat). This high level of nutrition makes milk an excellent medium for the growth of bacteria and other microbes. To minimize the risk of spoilage, milk is pasteurized. Pasteurization is a process by which milk is heated to a specific temperature for a set period of time to kill harmful bacteria. Pasteurizing milk does not sterilize it (sterilizing kills all bacteria) but merely destroys the pathogenic bacteria, leaving many bacteria that will multiply very slowly at refrigerator temperatures. However, if left at room temperature, the surviving bacteria will quickly grow and bring about milk spoilage.\n\n\n10.1.3 Milk Microbes\nOur next few weeks of lab experiments will examine microbial succession in milk, so let’s first define what we mean by microbe (aka microorganism). Simply stated, microbes are organisms that are too small to be seen without using a microscope. For the purposes of this lab, microbes include three broad groups of biological taxa: bacteria (type of prokaryote), yeasts (unicellular fungi), and molds (multicellular fungi). These three types of microbes can be distinguished by their size, cell type (prokaryotic v. eukaryotic), and whether they are unicellular or multicellular.\n\n\n\nFigure 10.1: Yeast and bacteria cells at 400x magnification.\n\n\nThe first comparison to make is between bacteria and yeast (Figure 10.1). Superficially, they appear similar in that they are both microscopic and unicellular. However, there are a couple important differences. Recall from your first semester of General Biology that Bacteria are prokaryotes, which means that they have the smallest cells and lack organelles. Yeasts are single-celled fungi, which are eukaryotes, and like other eukaryotes they have generally larger cells that contain organelles, such as a nucleus and mitochondria. Comparing Figure 10.1 and Figure 10.2 shows just how much of a size difference there is between the two at 400x magnification.\nThe other important comparison to make is between colonies of bacteria or yeast versus mold (Figure 10.2). As a multicellular fungus, mold is easy to distinguish from the unicellular bacteria or yeast under a microscope. Even without magnification, molds are easily identified by their “fuzzy” appearance, which is due to their body morphology of long, filamentous strands growing over top each other.\n\n\n\nFigure 10.2: Mold and yeast in a petri dish.\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nLet’s take a closer look at the type of microbes you will encounter during this lab. Get yourself set up with a microscope. Then, for each of the groups of microbes indicated obtain examine the indicated specimen create a set of sketches labeling the key features. You will use these as a reference to identify microbes present in your spoiled milk later on, so make sure that you are familiar with the key features. Place digital copies of your sketches in your images folder and insert them into this document as part of your introduction. When you use the insert image button in the visual editor of Rstudio you should also insert a descriptive title including the magnification used using the caption field.\n\n\n\n10.1.3.1 Fungal Mold\n\nWithout magnification, visually examine the mold colony of Penicillium camemberti.1\nUnder low magnification using a dissection microscope sketch a portion of the mold colony and label fungal hyphae (thin filamentous body)\n\n1 Species Note: We won’t see P. camemberti in our spoiled milk experiment, but the species is a good example of fungal mold that grows on dairy. This species creates the flavorful “rind” on soft-ripened cheeses such as Camembert and Brie.[Insert your sketch here - don’t forget to add a title/caption!]\n\n\n10.1.3.2 Bacteria shapes: bacilli (rods), cocci (spheres), and spirilla (spirals)\n\nMake sure you have had a demo on proper use and clean-up of oil immersion.\nWith a compound microscope, examine prepared slide #380 of bacterial shapes using the oil immersion technique for highest magnification.\nSketch examples of the different bacterial forms, labelling each.\n\n[Insert your sketches here - don’t forget to add a title/caption!]\n\n\n10.1.3.3 Yeast cells\n\nUsing a compound microscope, examine prepared slide #378 of yeast Candida albicans2.\nSketch several yeast cells and\nList features that distinguish yeast cells from bacterial cells.\n\n2 Species Note: We also won’t see C. albicans in our spoiled milk experiment, but the species is a good example of a common yeast. The species lives naturally in small amounts in all human bodies. When the yeast overgrows it causes infections, such as thrush (oral bumps) or vaginal yeast infections.[Insert your sketches here - don’t forget to add a title/caption!]\n[Contrast key features of bacteria and yeast cells for easy reference to distinguish them]\n\n\n\n10.1.4 Succession in Milk\nBiologists have discovered that, as pasteurized milk ages, changing conditions in the milk bring about a predictable, orderly succession of microbial communities. As anyone who has opened a bottle of spoiled milk will know, there are a couple distinctive physical attributes of spoiled milk - a strong sour odor is produced and the liquid coagulates into semisolid chunks (a process called curdling) - all in all, a very unpleasant experience. These physical changes are caused by various microbes in the milk. These microbes have been well-studied and the following facts will be useful as you write your hypothesis statement during lab class as to the expected changes in the microbe community as milk ages.\n\nRefrigerated, unspoiled milk has a pH of ~6.5\nStreptococcus (round-shaped cocci) and Lactobacillus (rod-shaped bacilli) bacteria naturally occur in milk and survive pasteurization\nStreptococcus and Lactobacillus both ferment lactose sugar into lactic acid and acetic acid\nStreptococcus bacteria thrive in milk with pH &gt; 5.5\nLactobacillus bacteria grow in milk with pH &gt; 4.5\nTwo common bacteria that always manage to get into milk from the environment, Pseudomonas and Achromobacter (both rod-shaped bacilli), digest butterfat and produce a putrid smell\nAn acidic environment causes casein to solidify, or curd\nYeasts and molds grow well in acidic environments"
  },
  {
    "objectID": "C01_ecol-succession.html#formulate-a-hypothesis",
    "href": "C01_ecol-succession.html#formulate-a-hypothesis",
    "title": "10  Ecological Succession",
    "section": "10.2 Formulate a Hypothesis",
    "text": "10.2 Formulate a Hypothesis\n\n\n\n\n\n\nConsider this\n\n\n\nWrite a clearly worded and testable hypothesis as to the expected changes in the microbe community as milk ages, and the expected differences between the plain and chocolate milk treatments.\nConsider your observations of the physical characteristics of the different milk treatments (pH, odor, consistency), as well as the 8 facts listed in the Succession in Milk paragraph above to inform your hypothesis writing to explain your reasoning.\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\nHypothesis 1: Ecological Succession in milk over time.\nExplain your reasoning behind your hypothesis:\nHypothesis 2: Ecological succession in plain vs chocolate milk\nExplain your reasoning behind your hypothesis:"
  },
  {
    "objectID": "C01_ecol-succession.html#methods",
    "href": "C01_ecol-succession.html#methods",
    "title": "10  Ecological Succession",
    "section": "10.3 Methods",
    "text": "10.3 Methods\nRecord Table 10.1"
  },
  {
    "objectID": "C01_ecol-succession.html#results",
    "href": "C01_ecol-succession.html#results",
    "title": "10  Ecological Succession",
    "section": "10.4 Results",
    "text": "10.4 Results\n\n\n\n\n\nMilk Type\nMilk Age\npH\nodor\nconsistency\norganisms present\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 10.1: Add Caption\n\n\n\n\n\n\nConsider this\n\n\n\nUse Table 10.1 to write a short parapgraph describing the changing sequence of organisms and corresponding environmental changes during the succession of plain milk.\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\n[Remember, at this point you are only describing your results, do not interpret them yet.]\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nUse Table 10.1 to write a short parapgraph describing the changing sequence of organisms and corresponding environmental changes during the succession of chocolate milk.\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\n[Remember, at this point you are only describing your results, do not interpret them yet.]"
  },
  {
    "objectID": "C01_ecol-succession.html#sec-methods",
    "href": "C01_ecol-succession.html#sec-methods",
    "title": "10  Ecological Succession",
    "section": "10.3 Methods",
    "text": "10.3 Methods\n\n10.3.1 Experimental Design\nWe will be comparing the ecological succession of microbes at different ages for plain and chocolate milk.\n\n\n\n\n\n\nGive it a try\n\n\n\nUse the table below ( Table 10.1) to describe the age, type, and physical characteristics for each milk treatment using the following categories and values:\n\nMilk Type: plain, chocolate\nMilk age\npH (use a test strip to determine)\nodor: No, moderate, strong sour smell\nconsistency: liquid, coagulation slight, moderate, chunky\n\n\n\nSince we are interested in microbes that grow in milk specifically, we will be using a culture medium designed specifically for testing milk.  The medium is called TGY agar, a name that is a shorthand for its constituent parts: Tryptone casein (protein), Glucose (carbohydrate), Yeast extract (source of vitamins), and agar (a solidifying agent).  TGY is a jelly-like nutrient that is designed specifically to grow milk microbes as it includes all the major macromolecules discussed earlier that are found in milk.\n\n\n\n\n\n\nPay Attention\n\n\n\nTogether with your lab partner you will be assigned to culture microbes from one of 8 different milk treatments. Be sure to check with your lab instructor for which treatment has been assigned to your group.\nMake sure you have watch your lab instructor demo the lawn technique to create a microbial lawn on your plate before you get started.\n\n\nInoculate TGY agar plate cultures of each milk type via lawn technique:\n\nUse a sterile swab to transfer several drops of milk culture to one edge of a sterile TGY plate\n(be sure to transfer from both the liquid and solid portions of the milk culture)\nUsing the same swab, spread the milk to create a microbial lawn over about one-third of the plate\nAgain using the same swab, pull down several wavy streaks of the microbes from this lawn\nDispose of the used swab in a red autoclave bag\nSeal culture plates with parafilm, note your group’s initials and milk treatment with a sharpie\n\nThe plates will be incubated by the instructor at room temperature for several days, then moved to refrigeration until next week’s lab time.\n\n\n10.3.2 Data Collection\n\n\n\n\n\n\nHeads up\n\n\n\nFor this lab we will be collecting quantitative data. This means that we will determine presence/absence of microbes in the different milk treatments3. Follow the procedure below to create slide smear and heat fix the bacteria to identify and record the microbes present in your milk treatment.\nEvery lab group will take a close look at their milk treatment and identify the presence of microbial groups. Remember to share the data with the other lab groups (and take a look at their slides, too) so that you have a complete data set of observed organisms in each milk treatment to analyze.\n\n\n3 By contrast, for quantitative data we would be counting the number of individuals or determining the density to allow us to compare relative abundance of the different microbe types.Prepare microscope slide of colony microbes\n\nCreate a slide smear\n\nOn a clean slide, add a small drop of water (this must later air dry, so keep the drop small).\nUsing a fresh toothpick, scrape a small portion of bacteria from your culture plate and mix the scraping in the drop of water on the slide.\nUse the toothpick to spread the bacteria-water mixture into a thin film, dab up extra moisture, then allow slide to air dry. Do not begin heat fixation until slide is dry or the water will boil and damage bacteria.\n\nHeat “fix” the bacteria: Hold the dried slide with a clothespin and pass it over a propane torch flame 3 times at a 45 degree angle. This should warm the slide, but not cook the bacteria; slide should never be too hot to touch\nStain the bacteria\n\nPlace slide on the support in staining tub and apply a few drops of safranin stain to the smear\nLeave the stain to set for 2 minutes\nWash the stain off with a gentle stream of water from a squirt bottle so that the stain goes into the staining tub\nBlot the stained slide gently with a paper towel (do not rub hard or bacteria will be removed)\n\nObserve and record microbes present in each milk type by examining your culture plate and bacterial smear slide to record observations in the table below (Table 10.1).\n\nUnder compound microscope, examine bacterial smears for presence of different shapes/types: bacilli, cocci, or yeasts (use oil immersion technique for higher magnification, if needed)\nUnder dissecting scope, examine culture plates for bacterial colony types, yeasts, or molds"
  },
  {
    "objectID": "C01_ecol-succession.html#discussion",
    "href": "C01_ecol-succession.html#discussion",
    "title": "10  Ecological Succession",
    "section": "10.5 Discussion",
    "text": "10.5 Discussion\n\n\n\n\n\n\nConsider this\n\n\n\nDiscuss your results in light of your hypothesis. Write a paragraph that states the broad research question and specific question experiment you executed during this lab and answers the following questions:\n\nDo the changes you described in your results section match your hypothesis?\nHow does the succession in plain and chocolate milk compare? What are potential reasons for similarities and differences you observed?\n\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\nIn this study, I investigated [broad question asked/hypothesis tested]. To achieve this, I [specific data set + analysis used]. I found that [key results] which [statement about how this does or does not support your hypothesis]."
  },
  {
    "objectID": "C02_antiseptic-disinfectant.html#introduction",
    "href": "C02_antiseptic-disinfectant.html#introduction",
    "title": "11  Disinfectants and Antiseptics",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\n\n11.1.1 Mutualistic and Pathogenic Bacteria\nIn this second part of this unit we will test the efficacy of antimicrobial agents on controlling milk microbe growth. Bacteria are found almost everywhere on Earth, and most species are directly or indirectly beneficial to other organisms. These mutualistic bacteria are necessary to maintain optimum environments in animal and plant bodies, and in environmental systems. In fact, humans are so dependent upon mutualistic bacteria that numerically there are more bacterial cells in a healthy human body than human cells. Given that bacteria cells are so much smaller than human cells, by volume and mass we are still mostly composed of human cells1.1 Don’t worry, you are still mostly you.\nHowever, even beneficial species, if they are reproducing at an uncontrolled rate, are potentially harmful or destructive to their environment. Furthermore, not all species of bacteria are beneficial to other organisms. Several species of bacteria and fungi are known to be pathogenic, that is, to cause disease in animals and plants. The growth of these pathogenic microbes must be controlled. Chemical agents have been developed that control bacterial and fungal growth. In today’s lab your will initiate a new experiment that investigates the efficacy of two of these growth-controlling agents: antiseptics and disinfectants.\n\n\n11.1.2 Antiseptics versus Disinfectants\nThere are three major categories of chemical agents used to control bacterial growth: antibiotics, antiseptics, and disinfectants. Antibiotics are chemicals produced by species of bacteria or fungi that inhibit the growth of other bacteria or fungi. These chemicals are packaged into the familiar pills that doctors prescribe to treat infections. For this week’s lab we will be experimenting with the other two categories of chemicals: antiseptics and disinfectants.\n\nAntiseptics are chemical agents that are used to control bacteria on living tissue, such as skin. Familiar examples include: Bactericidal soap, Baking soda, Hydrogen peroxide, Listerine, and Rubbing alcohol.\nDisinfectants are chemical agents used on inanimate objects, such as countertops. Familiar examples of these include: Ammonia, Clorox, Lysol, and Vinegar."
  },
  {
    "objectID": "C02_antiseptic-disinfectant.html#methods",
    "href": "C02_antiseptic-disinfectant.html#methods",
    "title": "11  Disinfectants and Antiseptics",
    "section": "11.2 Methods",
    "text": "11.2 Methods\n\n\n\n\n\n\nConsider this\n\n\n\nIn this experiment, you will choose one antiseptic to compare to one disinfectant with regard to their relative efficacy at controlling microbial growth in a culture of the 4-day old plain milk treatment. For this experiment you will be working with a lab partner.\nBefore you start, discuss your experimental set up with your lab partner.\n\nDiscuss your pre-lab research into the effectiveness of various disinfectants and antiseptics.\nRead the instructions for how to set up your assays below and then choose which treatments to use for your experiment; you should agree on 1 antiseptic and 1 disinfectant.\n\nTake some notes below that you will be able to refer to when you write your lab report for your experiment; they will also serve as good starting point for your homework assignment. For your introduction note the initial observation(s), your broad research question/hypothesis and the specific hypothesis you are testing in your experiment. We have given you a protocol on how to set up your treatments, but make sure that you read through it and record what your dependent and independent variables are, what variables you are controlling for etc. Remember, collecting and analyzing your results is part of your methods so make sure you can describe that process for your lab report.\n\n\n\n\n\n\n\n\nYour Answer Here\n\n\n\nInitial Observation\n[Remember, your research into the topic falls into the category of your “initial observation” because it is what has motivated you to formulate a research question/hypothesis that you want to pursue.]\nBroad research question hypothesis\n[Remember, this should be the big question, you are going to investigate one specific aspect of this to contribute to the broad understanding of this question.]\nSpecific research question you are investigating in your experiment\n[Be as specific as possible as two what you are testing!]\nExperimental Design\n\nIndependent (predictor or explanatory) variable:\nDependent (response) variable:\nWhat data will you be collecting to quantify your dependent variable? How will you analyze it?\nVariables you are controlling for:\n\n\n\n\n\n\n\n11.2.1 Experimental Set-up\n\n\n\n\n\n\nHeads up\n\n\n\nA bacterial lawn is like a lawn of grass - a uniform, even layer of organisms covering an entire surface. The success of your experiment hinges on the uniformity of your bacterial lawn. Before you get started, make sure that you have seen your instructor demo the technique!\nFor this experiment, all the lab groups on your section will be swabbing from the same culture experiment so that you can combine your data for the analysis.\n\n\n\nOn a fresh TGY media plate, use a sterile swab to prepare a bacterial lawn with bacteria from the previous 4 day plain milk culture plate.\n\nLift the agar plate lid to about a 45 degree angle, add a couple small drops of water, and swab the entire surface of the agar, taking care to swab the bacteria to the edges of the dish.\nRotate the plate 90 degrees and swab the agar again at right angles to the first swab.\n\nUsing the provided forceps soaking in alcohol, place 5 pre-soaked antiseptic disks and 5 pre-soaked disinfectant disks on opposite sides of the plate.\n\nBe sure to shake out any extra antiseptic or disinfectant solution from each disk before placing on the gel\nPlan ahead to ensure disks on each half of the plate are evenly spread out.\n\nKeeping careful track of which treatment is which, cover and seal the petri plate with parafilm, then with a sharpie draw a line delineating between the two sides; note your group’s initials and indicate which side has antiseptic disks and which side has disinfectant disks\\\nCulture plates will be incubated at room temperature for several days and then refrigerated.\n\n\n\n11.2.2 Collect zone of inhibition results for antimicrobial activity\n\n\n\n\n\n\nHeads up\n\n\n\nWe will be using a test that is widely used by microbiologists to quantify the effectiveness of antimicrobial agents such as antibiotics, antiseptics, or disinfectants on microbial growth. OUr experimental design follows the Kirby-Bauer or Zone of Inhibition test\n\nA bacterial or fungal strain of interest is grown in culture (we used 4-day old plain milk for all treatments)\nUsing a sterile swab, a suspension of the culture is spread evenly over a sterile agar plate\nThe antimicrobial agent is applied to the agar plate (we did this with soaked paper disks)\nThe agar plate is incubated for a time and at a temperature suitable for the test microorganism\n\nThe way the test works is that while the culture incubates, the antimicrobial agent leaches into the agar and then exerts a growth-inhibiting effect, which appears as a clear zone (the zone of inhibition) in the culture. The size of the zone of inhibition is usually related to the level of antimicrobial activity present in the sample or product – a larger zone of inhibition usually means that the antimicrobial is more potent.\nThis means we can quantify the effectiveness of the different antiseptic and disinfectant treatments by comparing the average zone of inhibition around the two treatments.\nBefore you get started, confirm the final version of your group’s hypothesis and experimental design.\n\nHave you composed a clearly written, testable hypothesis statement for your antiseptic vs disinfectant experiment?\nHave you identified your predictor/explanatory variable and your response variable?\nWhat components of the experiment are control variables? Why was it important that every group in your section used bacteria from the same culture plate?\n\n\n\n\nOpen the GoogleSheet to record your data and use the tab to select your section.\nIn Column A add your group number.\nType in the chemical name in Column B and categorize it as either a disinfectant or antiseptic using the dropdown menu for Column C.\nUse Column D to distinguish between your disks (number them 1-5) and add the zone of inhibition that you measured around that disk in column E.\nOnce all the groups in your lab section have added their data download the spreadsheet as a tab-delimited file using File -&gt; Download -&gt; Tab-separated file.\nUse Windows Explorer or Finder to navigate to your download folder and move the tab-delimited file you just downloaded into the data folder for your 03_Succession project folder. Make sure the file is named zone-of-inhibition.tsv2.\n\n2 It will automatically have *.tsv as the file ending; but you will likely have to change the filename to make sure there are no spaces.\n\n11.2.3 Data analysis\n\n\n\n\n\n\nHeads up\n\n\n\nWe are going to explore your results by applying the key concepts you have learned about descriptive and inferential stats. After completing this component you will use the provided template to write a lab report.\nFirst, you will analyze your groups results to assess the hypothesis you decided to test with this experiment. Those are the results that you will report in your lab report.\nFor your discussion, you will want to compare how effective your antiseptic and disinfectant are to others. Conveniently, your classmates have just generated data that will provide context for how your results compare to other disinfectants and antiseptics! We will analyze them here and describe the results and the you will be able to use that information for your discussion.\nRemember, nothing is worse than a blank piece of paper, so remember to take notes that you can use as a starting point for your lab report.\n\n\nLet’s start by loading our libraries:\n\n11.2.3.1 Get the data\nLet’s start by reading in our data set as a data.frame that we can compute on in R.\n\n# define your filename\nfile &lt;- \"zone-of-inhibition_demo.tsv\" # CHANGE THIS TO YOUR EXACT FILENAME, remember R is case sensitive\n\n# read in data\ninhibition &lt;- read_delim(here(\"data\", file), delim = \"\\t\") %&gt;%\n  clean_names()\n\nRows: 30 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): group, chemical, type\ndbl (2): disk, distance_mm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# take a look at columns in the data set\nglimpse(inhibition)\n\nRows: 30\nColumns: 5\n$ group       &lt;chr&gt; \"grp-1\", \"grp-1\", \"grp-1\", \"grp-1\", \"grp-1\", \"grp-1\", \"grp…\n$ chemical    &lt;chr&gt; \"disinfectant-1\", \"disinfectant-1\", \"disinfectant-1\", \"dis…\n$ type        &lt;chr&gt; \"disinfectant\", \"disinfectant\", \"disinfectant\", \"disinfect…\n$ disk        &lt;dbl&gt; 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5…\n$ distance_mm &lt;dbl&gt; 31, 34, 36, 33, 32, 25, 24, 23, 25, 26, 33, 36, 37, 35, 37…\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nTake a look at the data.frame object holding your data by clicking on it in the Environment pane (bottom left) or using View(inhibition).\nDescribe how your data is organized:\n\nwhat information is in each row?\nwhat information is in each column?\n\nAssess what type of data you have\n\nis it continuous or categorical?\nwhat groups are you comparing?\nwhat column contains your dependent variable?\nwhat column contains you independent variable?\n\nThen determine which descriptive statistics are appropriate to summarize this data set and how you would visualize your data.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nEach row represents one disc that was soaked in either an antiseptic or disinfectant.\nOur response or dependent variable is continuous, we have a length measurement in mm.\nThe independent or explanatory/predictor variable is a categorical variable, the chemicals that each group tested to see how they differ in terms of their impact on microbial group. We classified each as either a disinfectant or antiseptic (categorical variable).\nYou will want to determine the mean and standard deviation of the zone of inhibition for both treatments. It could also be helpful to generate additional summary statistics including median, minimum and maximum values to get an overview of the data.\nIn this case, we do not have a lot of measurements so a histogram is probably not very helpful, but a bar chart or similar comparing the mean and standard deviation is probably a helpful visualization to communicate our results.\n\n\n\n\n\n11.2.3.2 Summarize the data\nWe will need to summarize our results to evaluate our findings.\n\n\n\n\n\n\nConsider this\n\n\n\nSuggest 3-4 metrics to summarize your data in a meaningful way that will allow you to describe your results in the context of the hypothesis you are exploring with this experiment.\nExplain why you think these would be appropriate data analysis methods.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAs we have continuous data, we can use the descriptive statistics we have discussed previously that allow us to assess the central tendency (mean, median, mode) and variability (variance, standard deviation) of our data.\n\n\n\nLet’s start by putting together a table with summary statistics. Currently, the data set still contains everyone’s data, so we will start by creating a subset of just your data.\n\n# create vector with your group name (change this to YOUR group)\ngrp &lt;- \"grp-1\"\n\ninhibition %&gt;%                            # specify data\n  filter(group == grp) %&gt;%              # retain only rows for your group \n  group_by(chemical) %&gt;%                 # group by treatment\n  summarize(mean = mean(distance_mm),       # calculate mean\n            sd = sd(distance_mm),           # calculate standard deviation\n            median = median(distance_mm),   # determine median value\n            min = min(distance_mm),         # determine minimum value\n            max = max(distance_mm))         # determine maximum value\n\n\n?(caption)\n\n\n\n# A tibble: 2 × 6\n  chemical        mean    sd median   min   max\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 antiseptic-1    24.6  1.14     25    23    26\n2 disinfectant-1  33.2  1.92     33    31    36\n\n\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nDecide which descriptive statistics give the best summary of your results and write up a brief description3. Make sure that you are being specific and adding context to make it easy for your reader to identify the key patterns.\nAs rule of thumb, you should describe your results so that your reader has all the key information they need even without seeing the data tables; this means you should include specific values.\nRemember, at this point you should not be interpreting your results!\n\n\n3 Consider this thinking out loud in a way where you have a set of notes that you can then edit for your report.\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour description could look something like this:\nBroad pattern:\n\nOverall, the zone of inhibition was smaller for discs soaked in the antiseptic X.\n\nDetails\n\nThe mean zone of inhibition was 24.6 +/- 1.14mm for discs soaked in X compared to chemical Y, which is a disinfectant.\n\n\n\n\nSince you have included the values in your written description, you would not need to include a table. Figures are important tools for communicating data. an important tool to summarize and present your data. Let’s start by visualizing our data with a histogram which emphasize the distribution of our values.\nWe have previously plotted comparisons of means using a point range where the mean is represented by a point and the standard deviation by lines extending out from that point. This should help us compare our data for this experiment as well:\n\n# pick colors\ncolors &lt;- c(\"orange\", \"purple\")\n\n# create dataframe with mean and sd\nsummary &lt;- inhibition %&gt;%\n  filter(group == grp) %&gt;%\n  group_by(chemical) %&gt;%\n  summarise(mean = mean(distance_mm),\n            sd = sd(distance_mm))\n\n# plot data\nggplot(summary) +\n  geom_pointrange(aes(x = chemical,\n                    y = mean,\n                    ymin = mean-sd,\n                    ymax = mean+sd,\n                    color = chemical),\n                size = 1.2) +\n  scale_color_manual(values = colors) +\n  labs(x = \"chemical tested\", \n       y = \"zone of inhibition [mm]\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 11.1: Mean zone of inhibition around discs soaked in disinfectant Y (blue) and antiseptic X (purple).\n\n\n\n\nAs we know, our set of summary statistics and data visualization enable us to present the patterns in our data set. However, at this point we cannot make statement whether our observed differences in the zone of inhibition are within the range of variability in inhibition we would expect to see even if the two chemicals we tested does not have differential effect on microbial growth.\n\n\n\n\n\n\nConsider this\n\n\n\nArgue which type of statistical test we should apply and outline the key steps you will need to take to perform the test.\n\n\n\n\n11.2.3.3 Test for significance\nWe need to apply a \\(t\\)-test to evaluate the difference between means.\n\n\n\n\n\n\nGive it a try\n\n\n\nState your null and alternative hypothesis and predict the outcome if each claim is true.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour null hypothesis is a claim that there is no effect:\n\n\\(H_{0}\\): There is no difference in the effect of chemical X (antiseptic) and chemical Y (disinfectant) on microbial growth.\nPrediction: There will be no difference in the zone of inhibition observed around the discs regardless of which chemical they were soaked in.\n\nThe alternative hypothesis is a claim that there is an observed effect.\n\n\\(H_{0}\\): Chemical Y (disinfectant) has a stronger effect on microbial growth compared to chemical X (antiseptic).\nPrediction: The observed zone of inhibition will be larger around the discs soaked in chemical Y compared to chemical X.\n\n\n\n\n\n\n\n\n\n\nConsider this\n\n\n\nWe will specify our significance threshold \\(\\alpha\\) as 0.05.\nExplain what this means and why it is important for us to specify this.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis means that if our observed results have a less than 5% probability occurring according to our null distribution we will reject our null hypothesis.\n\n\n\nOur next step is to calculate our observed test statistic:\n\n# create subset with your data\ndf &lt;- inhibition %&gt;%\n  filter(group == grp)\n\n# calculate observed statistic\nobserved_statistic &lt;- df %&gt;%             # define data to use\n  specify(response = distance_mm,              # specify response (dependent) variable\n          explanatory = chemical) %&gt;%       # specify explanatory (independent) variable\n  calculate(stat = \"diff in means\"          # define test statistic to use\n           )        # order to subtract explanatory variables\n\nWarning: The statistic is based on a difference or ratio; by default, for\ndifference-based statistics, the explanatory variable is subtracted in the\norder \"antiseptic-1\" - \"disinfectant-1\", or divided in the order \"antiseptic-1\"\n/ \"disinfectant-1\" for ratio-based statistics. To specify this order yourself,\nsupply `order = c(\"antiseptic-1\", \"disinfectant-1\")` to the calculate()\nfunction.\n\n# print test statistic\nobserved_statistic\n\nResponse: distance_mm (numeric)\nExplanatory: chemical (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  -8.6\n\n\nNow, let’s simulate our null distribution and compare how our observed test statistic compares.\n\n# create null distribution\nnull_dist &lt;- df %&gt;%\n  specify(response = distance_mm,              # specify response (dependent) variable\n          explanatory = chemical) %&gt;%       # specify explanatory (independent) variable\n  hypothesize(null = \"independence\") %&gt;%     # define null hypothesis\n  generate(reps = 1000,                      # number of samples to generate null distribution\n           type = \"permute\") %&gt;%             # shuffle to break association\n  calculate(stat = \"diff in means\"\n            # define test statistic to use\n          )        # order to subtract explanatory variables\n\nWarning: The statistic is based on a difference or ratio; by default, for\ndifference-based statistics, the explanatory variable is subtracted in the\norder \"antiseptic-1\" - \"disinfectant-1\", or divided in the order \"antiseptic-1\"\n/ \"disinfectant-1\" for ratio-based statistics. To specify this order yourself,\nsupply `order = c(\"antiseptic-1\", \"disinfectant-1\")` to the calculate()\nfunction.\n\n# visualize null distribution test statistic\nnull_dist %&gt;%\n  visualize() +\n  shade_p_value(observed_statistic,\n                direction = \"greater\")\n\n\n\n\nFigure 11.2: Null distribution of t test statistic assuming presence of plant material has no effect on radish seed growth. The calculated t-square test statistic is indicated by the red line. Values observed in the null distribution that are more extreme that the t-statistic calculated for the empirical data set are shaded in red.\n\n\n\n\nAnd for our last step, we need to determine our p-value.\n\n# obtain p-value\nnull_dist %&gt;%\n  get_p_value(obs_stat = observed_statistic,\n              direction = \"two sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.014\n\n\n\n\n\n\n\n\nGive it a try\n\n\n\nCompare your p-value to the threshold of significance we defined above to determine how to interpret the results of your t-test.\nThen update the results you formulated earlier to add this information into your description of the results.\nFinally, write a few sentences as a draft of how you would include this in the discussion of your results, i.e. how you will interpret your results to draw a conclusion in the context of our broad research hypothesis that we designed this experiment to explore.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn this case our p-value is 0.004 which means that is a less than 0.4% probability that we would observe a test statistic at least this extreme given our null hypothesis.\nThis value is smaller than our specified threshold of significance and therefore we will reject our null hypothesis and conclude that the difference in the observed germination rates is statistically significant and consistent with our hypothesis that plant X has an effect on the germination rate.\nWe can augment our results to include this information:\n\nOverall, the zone of inhibition was smaller for discs soaked in the antiseptic X. The mean zone of inhibition was 24.6 +/- 1.14mm for discs soaked in X compared to chemical Y at 33.2 +/- 1.92mm which is a disinfectant. The difference of approx 9mm is significant (p &lt; 0.05).\n\nIn the discussion we would interpret our results:\n\nThe zone of inhibition was significant larger for chemical X which is a disinfectant. This is consistent with our hypothesis that disinfectants are more effective in inhibiting microbial growth compared to antiseptics.\n\n\n\n\nRemember, if your results are not statistically significant, you still need to write a set of conclusions! Even though we seem to think that “negative results” are “bad”, “negative” just means that we did not observe an effect - but that is also a result because you can now exclude one possible explanation for your observation and move on to test the next one.\nLet’s take a look at the data overall. We will produce a figure the mean zone of inhibition across all disinfectants and all antiseptics that were tested in your lab section. In addition, you should make sure to get the results from your lab mates that show whether or not the difference in the combination of chemicals they tested is significant.\n\n# pick colors\ncolors &lt;- c(\"orange\", \"purple\")\n\n# create dataframe with mean and sd\nsummary &lt;- inhibition %&gt;%\n  group_by(type) %&gt;%\n  summarise(mean = mean(distance_mm),\n            sd = sd(distance_mm))\n\n# plot data\nggplot(summary) +\n  geom_pointrange(aes(x = type,\n                    y = mean,\n                    ymin = mean-sd,\n                    ymax = mean+sd,\n                    color = type),\n                size = 1.2) +\n  scale_color_manual(values = colors) +\n  labs(x = \"chemical tested\", \n       y = \"zone of inhibition [mm]\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 11.3: Mean zone of inhibition around discs soaked in disinfectant (blue) and antiseptic (purple).\n\n\n\n\nWe can also compare the individual chemicals tested.\n\n# pick colors\ncolors &lt;- c(\"orange\", \"purple\")\n\n# create dataframe with mean and sd\nsummary &lt;- inhibition %&gt;%\n  group_by(type, chemical) %&gt;%\n  summarise(mean = mean(distance_mm),\n            sd = sd(distance_mm))\n\n`summarise()` has grouped output by 'type'. You can override using the\n`.groups` argument.\n\n# plot data\nggplot(summary) +\n  geom_pointrange(aes(x = chemical,\n                    y = mean,\n                    ymin = mean-sd,\n                    ymax = mean+sd,\n                    color = type),\n                size = 1.2) +\n  scale_color_manual(values = colors) +\n  labs(x = \"chemical tested\", \n       y = \"zone of inhibition [mm]\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 11.4: Mean zone of inhibition around discs soaked in disinfectants (blue) and antiseptics (purple)."
  },
  {
    "objectID": "C02_antiseptic-disinfectant.html#discussion",
    "href": "C02_antiseptic-disinfectant.html#discussion",
    "title": "11  Disinfectants and Antiseptics",
    "section": "11.3 Discussion",
    "text": "11.3 Discussion\n\n\n\n\n\n\nHeads up\n\n\n\nAs part of your lab report, you will need to discuss your results. Prepare for writing up your discussion by thinking through the following questions with your lab partner and the rest of your section. Remember to take notes as a starting point for your lab report (nothing is more intimidating than a blank page!).\n\nDid your results support your hypothesis?\nCompare your results to that of the entire class (this is equivalent to “comparing your results to results in the literature”)\n\nWhich antiseptic would be most effective for cleaning up spoiled milk?\nWhich disinfectant would be most effective for cleaning up spoiled milk?\n\nCome to a conclusion about the efficacy of disinfectants and antiseptics.\n\nIn which situations is it most appropriate to use a disinfectant?\nIn which situations is it most appropriate to use an antiseptic?"
  }
]